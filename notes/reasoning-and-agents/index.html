<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2D - Reasoning and Agents Lecture Notes of Bora M. Alper</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Bora M. Alper">
  <meta name="description" content="2D - Reasoning and Agents Lecture Notes of Bora M. Alper" />

  <link rel="stylesheet" href="main.css">

  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'></script>
  <script type="text/javascript">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['$$','$$']],
      processEscapes: true
    }
  });
  </script>
</head>
<body>
<h1 id="2d---reasoning-and-agents">2D - Reasoning and Agents</h1>
<p><em>Based on <a href="https://www.inf.ed.ac.uk/teaching/courses/inf2d/">Informatics 2D: Reasoning and Agents</a> in <a href="https://www.ed.ac.uk/">The University of Edinburgh</a> lecture slides from 2018-2019.</em></p>

<h2>Table of Contents</h2>
<nav id="TOC">
<ul>
<li><a href="#2d---reasoning-and-agents">2D - Reasoning and Agents</a><ul>
<li><a href="#15-january-2019">15 January 2019</a><ul>
<li><a href="#intelligent-agents">Intelligent Agents</a></li>
<li><a href="#simple-reflex-agents">Simple Reflex Agents</a></li>
<li><a href="#model-based-reflex-agents">Model-Based Reflex Agents</a></li>
<li><a href="#goal-based-agents">Goal-Based Agents</a></li>
<li><a href="#utility-based-agents">Utility-Based Agents</a></li>
<li><a href="#types-of-environment">Types of Environment</a></li>
</ul></li>
<li><a href="#17-january-2019">17 January 2019</a><ul>
<li><a href="#simple-problem-solving-agent">Simple Problem Solving Agent</a></li>
<li><a href="#problem-types">Problem Types</a></li>
<li><a href="#search-trees">Search Trees</a></li>
<li><a href="#summary">Summary</a></li>
</ul></li>
<li><a href="#18-january-2019">18 January 2019</a><ul>
<li><a href="#search-strategy">Search Strategy</a></li>
<li><a href="#graph-search">Graph Search</a></li>
<li><a href="#breadth-first-search">Breadth-First Search</a></li>
<li><a href="#depth-first-search">Depth-First Search</a></li>
<li><a href="#depth-limited-search">Depth-Limited Search</a></li>
<li><a href="#summary-1">Summary</a></li>
</ul></li>
<li><a href="#22-january-2019">22 January 2019</a><ul>
<li><a href="#games">Games</a></li>
<li><a href="#game-trees">Game Trees</a></li>
<li><a href="#normal-vs-adversarial-search">Normal vs Adversarial Search</a></li>
<li><a href="#minimax">Minimax</a></li>
<li><a href="#alpha-beta-pruning">$\alpha-\beta$ Pruning</a></li>
<li><a href="#in-practice">In Practice</a></li>
</ul></li>
<li><a href="#24-january-2019">24 January 2019</a><ul>
<li><a href="#informed-search">Informed Search</a></li>
<li><a href="#best-first-search">Best-First Search</a></li>
<li><a href="#greedy-best-first-search">Greedy Best-First Search</a></li>
<li><a href="#a-search">A* Search</a></li>
<li><a href="#heuristic-functions">Heuristic Functions</a></li>
</ul></li>
<li><a href="#25-january-2019">25 January 2019</a><ul>
<li><a href="#knowledge-based-agents">Knowledge-Based Agents</a></li>
<li><a href="#peas">PEAS</a></li>
<li><a href="#logics">Logics</a></li>
</ul></li>
<li><a href="#29-january-2019">29 January 2019</a><ul>
<li><a href="#effective-propositional-inference">(Effective) Propositional Inference</a></li>
<li><a href="#conjunctive-normal-form-cnf">Conjunctive Normal Form (CNF)</a></li>
<li><a href="#dpll-algorithm">DPLL Algorithm</a></li>
<li><a href="#walksat-algorithm"><code>WalkSAT</code> Algorithm</a></li>
<li><a href="#hard-satisfiability-problems">Hard Satisfiability Problems</a></li>
<li><a href="#expressiveness-limitation-of-propositional-logic">Expressiveness Limitation of Propositional Logic</a></li>
</ul></li>
<li><a href="#31-january-2019">31 January 2019</a><ul>
<li><a href="#constraint-satisfaction-problems-csps">Constraint Satisfaction Problems (CSPs)</a></li>
<li><a href="#constraint-graph">Constraint Graph</a></li>
<li><a href="#varieties-of-csps">Varieties of CSPs</a></li>
<li><a href="#varieties-of-constraints">Varieties of Constraints</a></li>
<li><a href="#real-world-csps">Real-World CSPs</a></li>
<li><a href="#backtracking-search">Backtracking Search</a></li>
</ul></li>
<li><a href="#5-february-2019">5 February 2019</a><ul>
<li><a href="#propositional-logic-pl">Propositional Logic (PL)</a></li>
<li><a href="#first-order-logic-fol">First-Order Logic (FOL)</a></li>
</ul></li>
<li><a href="#7-february-2019">7 February 2019</a><ul>
<li><a href="#universal-instantiation-ui">Universal Instantiation (UI)</a></li>
<li><a href="#existential-instantiation-ei">Existential Instantiation (EI)</a></li>
<li><a href="#reduction-to-pl-by-propositionalisation">Reduction to PL by Propositionalisation</a></li>
<li><a href="#unification">Unification</a></li>
<li><a href="#generalised-modus-ponens-gmp">Generalised Modus Ponens (GMP)</a></li>
</ul></li>
<li><a href="#8-february-2019">8 February 2019</a><ul>
<li><a href="#chaining">Chaining</a></li>
<li><a href="#resolution">Resolution</a></li>
</ul></li>
<li><a href="#12-february-2019">12 February 2019</a><ul>
<li><a href="#limitations-of-generalised-modus-ponens">Limitations of Generalised Modus Ponens</a></li>
<li><a href="#resolution--modus-ponens">Resolution &amp; Modus Ponens</a></li>
<li><a href="#factors-resolvents-and-the-lifting-lemma">Factors, Resolvents, and the Lifting Lemma</a></li>
<li><a href="#efficient-algorithms-for-resolutions">Efficient Algorithms for Resolutions</a></li>
</ul></li>
<li><a href="#14-february-2019">14 February 2019</a><ul>
<li><a href="#using-logic-to-plan">Using Logic to Plan</a></li>
</ul></li>
<li><a href="#26-february-2019">26 February 2019</a><ul>
<li><a href="#planning">Planning</a></li>
<li><a href="#problems-with-search">Problems with Search</a></li>
<li><a href="#logic--deductive-inference">Logic &amp; Deductive Inference</a></li>
<li><a href="#planning-domain-definition-language-pddl">Planning Domain Definition Language (PDDL)</a></li>
</ul></li>
<li><a href="#28-february-2019">28 February 2019</a><ul>
<li><a href="#planning-with-state-space-search">Planning with State-Space Search</a></li>
<li><a href="#partial-order-planning-pop-and-plan-space-search">Partial-Order Planning (POP) and Plan-Space Search</a></li>
</ul></li>
<li><a href="#1-march-2019">1 March 2019</a><ul>
<li><a href="#non-deterministic-domains">Non-Deterministic Domains</a></li>
<li><a href="#sensing-with-percepts">Sensing with Percepts</a></li>
<li><a href="#belief-states">Belief States</a></li>
<li><a href="#sensorless-planning">Sensorless Planning</a></li>
<li><a href="#extending-action-representations">Extending Action Representations</a></li>
<li><a href="#contingent-planning">Contingent Planning</a></li>
</ul></li>
<li><a href="#5-march-2019">5 March 2019</a><ul>
<li><a href="#online-planning">Online Planning</a></li>
<li><a href="#hierarchical-decomposition-in-planning">Hierarchical Decomposition in Planning</a></li>
</ul></li>
<li><a href="#7-march-2019">7 March 2019</a><ul>
<li><a href="#handling-uncertainty">Handling Uncertainty</a></li>
<li><a href="#introduction-to-probability">Introduction to Probability</a></li>
</ul></li>
<li><a href="#12-march-2019">12 March 2019</a><ul>
<li><a href="#inference-with-joint-probability-distributions">Inference with Joint Probability Distributions</a></li>
<li><a href="#marginalisation-conditioning--normalisation">Marginalisation, Conditioning &amp; Normalisation</a></li>
<li><a href="#a-general-inference-procedure">A General Inference Procedure</a></li>
<li><a href="#independence">Independence</a></li>
<li><a href="#bayes-rule">Bayes' Rule</a></li>
<li><a href="#combining-evidence">Combining Evidence</a></li>
</ul></li>
<li><a href="#14-march-2019">14 March 2019</a><ul>
<li><a href="#bayesian-networks">Bayesian Networks</a></li>
</ul></li>
<li><a href="#15-march-2019">15 March 2019</a><ul>
<li><a href="#exact-inference-in-bayesian-networks">Exact Inference in Bayesian Networks</a></li>
</ul></li>
<li><a href="#19-march-2019">19 March 2019</a><ul>
<li><a href="#approximate-inference-in-bayesian-networks">Approximate Inference in Bayesian Networks</a></li>
<li><a href="#the-markov-chain-monte-carlo-mcmc-algorithm">The Markov Chain Monte Carlo (MCMC) Algorithm</a></li>
</ul></li>
<li><a href="#21--22-march-2019">21 &amp; 22 March 2019</a><ul>
<li><a href="#time-and-dynamic-environments">Time and Dynamic Environments</a></li>
<li><a href="#inference-tasks-in-temporal-models">Inference Tasks in Temporal Models</a></li>
<li><a href="#hidden-markov-models-hmm">Hidden Markov Models (HMM)</a></li>
</ul></li>
<li><a href="#26-march-2019">26 March 2019</a><ul>
<li><a href="#dynamic-bayesian-networks-dbn">Dynamic Bayesian Networks (DBN)</a></li>
<li><a href="#bns-hmms-and-dbns">BNs, HMMs, and DBNs</a></li>
</ul></li>
<li><a href="#28-march-2019">28 March 2019</a><ul>
<li><a href="#beliefs-and-desires">Beliefs and Desires</a></li>
<li><a href="#utility-theory--utility-functions">Utility Theory &amp; Utility Functions</a></li>
<li><a href="#constraints-on-rational-preferences">Constraints on Rational Preferences</a></li>
<li><a href="#utility-scales">Utility Scales</a></li>
<li><a href="#decision-networks-dn-influence-diagrams">Decision Networks (DN) (Influence Diagrams)</a></li>
</ul></li>
<li><a href="#29-march-2019">29 March 2019</a><ul>
<li><a href="#sequential-decision-problems">Sequential Decision Problems</a></li>
<li><a href="#markov-decision-process-mdp">Markov Decision Process (MDP)</a></li>
<li><a href="#optimality-in-sequential-decision-problems">Optimality in Sequential Decision Problems</a></li>
<li><a href="#value-iteration">Value Iteration</a></li>
<li><a href="#decision-theoretic-agents">Decision-Theoretic Agents</a></li>
</ul></li>
</ul></li>
</ul>
</nav>


<h2 id="15-january-2019">15 January 2019</h2>
<h3 id="intelligent-agents">Intelligent Agents</h3>
<p>An agent</p>
<ul>
<li>Perceives its <strong>operating environment</strong>
<ul>
<li>Think of environment in terms of
<ul>
<li>things to be sensed through sensors,
<ul>
<li>might be tangible objects (<em>e.g.</em> barcodes on letters), or <em>events</em> (changes in the world)</li>
</ul></li>
<li>things to be acted on via actuators</li>
<li>see "Types of Environment" section for more details.</li>
</ul></li>
</ul></li>
<li>Through its <strong>sensors</strong>
<ul>
<li>Might be level- or edge-triggered.
<ul>
<li>Level-Triggered: sensor <em>continues</em> signalling as long as the stimuli is <em>constant</em></li>
<li>Edge-Triggered: sensor signals <em>once</em> the amount stimuli <em>changes</em></li>
</ul></li>
</ul></li>
<li>To achieve its <strong>goals</strong>
<ul>
<li>This is very closely related to the concept of <strong>performance measure</strong>.</li>
<li><strong>"As a general rule, it is better to design performance measures according to what one actually wants in the environment, rather than according to how one thinks the agent should behave."</strong>
<ul>
<li>An example for the violation of this rule: "We might propose to measure performance by the amount of dirt cleaned up in a single daily-shift. With a rational (vacuum cleaner) agent, of course, what you ask for is what you get: a <em>rational</em> agent will maximise this performance measure by cleaning up the dirt, then dumping it all on the floor, then cleaning it up again, and so on!"</li>
</ul></li>
</ul></li>
<li>By acting on its environment via <strong>actuators</strong>.</li>
</ul>
<p><strong>Examples:</strong></p>
<ul>
<li><em>Mail Sorting Robot</em>
<ul>
<li>Environment: conveyor belt of letters</li>
<li>Goals: route letter into the correct bin</li>
<li>Percepts: barcodes read</li>
<li>Actions: route letter into bin</li>
</ul></li>
<li><em>Intelligent House</em>
<ul>
<li>Environment:
<ul>
<li>occupants enter and leave house</li>
<li>occupants enter and leave rooms</li>
<li>daily variation outside light and temperature</li>
</ul></li>
<li>Goals:
<ul>
<li>keep occupants warm</li>
<li>room lights are on whenever and only when a room is occupied</li>
<li>house energy efficient</li>
</ul></li>
<li>Percepts:
<ul>
<li>signals from temperature sensor</li>
<li>movement sensor</li>
<li>light detector</li>
</ul></li>
<li>Actions:
<ul>
<li>room heaters on/off</li>
<li>lights on/off</li>
</ul></li>
</ul></li>
</ul>
<h3 id="simple-reflex-agents">Simple Reflex Agents</h3>
<pre><code>function Simple-Reflex-Agent(percept) returns action:
    persistent: rules - set of condition-action rules

    # Pay attention to the fact that state is constructed
    # purely out of the current percept.
    state  := Interpret-Input(percept)
    rule   := Rule-Match(state, rules)
    action := rule.ACTION

    return action
</code></pre>
<ul>
<li><strong>Action depend only on immediate percepts.</strong>
<ul>
<li>Note this well!</li>
<li>Because of this property, Simple-Reflex Agents will work <strong>"only if the correct decision can be made on the basis of only the current percept."</strong>
<ul>
<li>This often requires the environment to be fully-observable.</li>
<li>And again often, <strong>infinite loops</strong> are unavoidable for Simple-Reflex agents operating in <strong>partially-observable</strong> environments, due to lack of some percepts and the non-existence of our memory!
<ul>
<li>Randomising the action of Simple-Reflex agents is an elegant way to mitigate this issue.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Implemented by <strong>condition-action rules</strong>.</li>
<li>Example:
<ul>
<li>Agent: mail sorting robot</li>
<li>Environment: conveyor belt of letters</li>
<li>Rule: if city is "Edinburgh" then put the letter into Scotland bag</li>
</ul></li>
</ul>
<h3 id="model-based-reflex-agents">Model-Based Reflex Agents</h3>
<pre><code>function Reflex-Agent-With-State(percept) returns action:
    persistent: state  - description of current world state
                model  - description of how the next state depends
                         on current state and does affected by our
                         actions
                rules  - a set of condition-action rules
                action - the most recent action, initially none

    state  := Update-State(state, action, percept, model)
    rule   := Rule-Match(state, rules)
    action := rule.ACTION

    return action
</code></pre>
<ul>
<li>The next logical step after memory-less Simple-Reflex Agent.</li>
<li><strong>Action may depend on HISTORY (<em>i.e.</em> memory) and unperceived aspects of the world.</strong>
<ul>
<li>This requires a <strong>model</strong> + <strong>state</strong>.</li>
<li>What does <em>unperceived aspects</em> mean?
<ul>
<li>It means that we might <em>deduce</em> certain facts about the future states of the world given its current state.
<ul>
<li>"What the world is like now" + "How the world evolves"</li>
<li>"What the world is like now" + "What (effects) my actions do (have)"</li>
</ul></li>
<li>Realise that this <strong>"model" is some "knowledge about "how the world works"</strong> -- whether implemented in Boolean circuits or by some other means."
<ul>
<li>As evidenced by the existence of <code>Update-State</code> function; how can you <em>update</em> your state if you don't have a (mental) model of the world.</li>
</ul></li>
</ul></li>
<li>"It is seldom possible for the agent to determine the current state of a partially observable environment <em>exactly</em>."</li>
</ul></li>
<li>Also called <em>reflex agents with state</em>.</li>
<li>Example:
<ul>
<li>Agent: robot vacuum cleaner</li>
<li>Environment: dirty room, furniture</li>
<li>Model: maps of each room, keeping track of areas that are already cleaned</li>
</ul></li>
</ul>
<h3 id="goal-based-agents">Goal-Based Agents</h3>
<ul>
<li><p><strong>Agents with variable --but no conflicting-- goals</strong></p></li>
<li><p>Example:</p>
<ul>
<li>Agent: household service robot</li>
<li>Environment: house &amp; people</li>
<li>Goals: clean clothes, tidy rooms, lay table, etc.</li>
</ul></li>
<li><p><strong>The most important distinction is that instead of having to encode our goals as <em>condition-action</em> rules, we now directly "supply" the goal(s) and the agent plans.</strong></p></li>
<li><p>Two important concepts introduced by goal-based agents are:</p>
<ul>
<li><strong>"What it will be like right afterwards a if I take a certain action now?"</strong></li>
<li><strong>"What action should I do now?" (based on my Goals)</strong></li>
</ul></li>
</ul>
<h3 id="utility-based-agents">Utility-Based Agents</h3>
<ul>
<li><p><strong>Agents that optimise "utility" over a range of goals.</strong></p></li>
<li><p>Over Goal-Based Agents, Utility-Based Agents have the offer the following advantages:</p>
<ul>
<li>Agents so far have had a single goal, but they may have to juggle conflicting goals.</li>
<li>Under uncertainty, combining with probability of success with the utility of each outcome yields <strong>expected utility</strong>, which helps us operate in such environments.</li>
</ul></li>
<li><p>Utility is defined as a <em>"measure of goodness"</em> (a real number).</p>
<ul>
<li>"An agent's <strong>utility function</strong> is essentially an internalisation of the performance measure."</li>
<li>It is highly desirable that the agent's utility function and the external performance measure are in agreement!
<ul>
<li>And yet it is a gravely mistake to define external performance measure in terms of the agent's utility function! Performance measure should precede.</li>
</ul></li>
</ul></li>
<li><p>Example:</p>
<ul>
<li>Agent: automatic car</li>
<li>Environment: roads, vehicles, signs, etc.</li>
<li>Goals: stay safe, reach destination, be quick, obey law, save fuel, etc.</li>
</ul></li>
<li><p>The most important concept introduced by utility-based agents is asking, for each possible action:</p>
<ul>
<li><strong>"How happy will I be in such state, if I have taken the action X?"</strong></li>
<li>The action chosen is simply the one that yields most happiness.</li>
</ul></li>
</ul>
<h3 id="types-of-environment">Types of Environment</h3>
<ul>
<li><strong>Fully Observable vs. Partially Observable</strong>
<ul>
<li>Full: agent's sensors describe the environment state fully.</li>
<li>Partial: some parts of the environment not visible, and/or sensors are noisy.</li>
</ul></li>
<li><strong>Deterministic vs. Stochastic (<em>i.e.</em> unpredictable)</strong>
<ul>
<li>Deterministic: next state fully determined by current state and agent's actions (alone).</li>
<li>Stochastic: random changes (can't be predicted exactly).
<ul>
<li>"Our use of the word "stochastic" generally implies that uncertainty about outcomes is quantified in terms of <em>probabilities</em>."</li>
</ul></li>
<li><strong>An environment may appear stochastic if it's only partially observable.</strong></li>
</ul></li>
<li><strong>Episodic vs. Sequential</strong>
<ul>
<li>Episodic: next action does not depend on previous actions, <em>e.g.</em> mail-sorting robot.</li>
<li>Sequential: next action depends on previous actions, <em>e.g.</em> crossword puzzle.</li>
</ul></li>
<li><strong>Static vs. Dynamic</strong>
<ul>
<li>Static: environment unchanged while agent deliberates, <em>e.g.</em> factory robot.</li>
<li>Dynamic: environment changes while agent deliberates, <em>e.g.</em> self-driving car.</li>
</ul></li>
<li><strong>Discrete vs. Continuous</strong>
<ul>
<li>Discrete: percepts (<em>i.e.</em> sensor data), actions, and episodes are discrete.</li>
<li>Continuous: ... are continuous.</li>
</ul></li>
<li><strong>Single-Agent vs. Multi-Agent</strong>
<ul>
<li>Self-explanatory.</li>
<li>Multi-Agent environments also tend to be:
<ul>
<li>Sequential (very likely), since our action would influence the action of other agents, and their action will influence ours.</li>
<li>Dynamic, unless each actor acts "in turn" whilst waiting for each other (<em>e.g.</em> chess).</li>
</ul></li>
<li>Do not think that all multi-agent environments are stochastic if we cannot predict the behaviour of all other agents: "in our definition (of deterministic), <strong>we ignore uncertainty that arises <em>purely</em> from the actions of other agents in a multi-agent environment</strong>;" thus, a chess game is considered to be deterministic.</li>
</ul></li>
</ul>
<h2 id="17-january-2019">17 January 2019</h2>
<h3 id="simple-problem-solving-agent">Simple Problem Solving Agent</h3>
<pre><code>function Simple-Problem-Solving-Agent(percept) returns an action:
    persistent: seq     - an action sequence, initially empty
                state   - some description of the current world state
                goal    - a goal, initially null
                problem - a problem formulation

    state := Update-State(state, percept)
    if seq is empty then
        goal    := Formulate-Goal(state)
        problem := Formulate-Problem(state, goal)
        seq     := Search(problem)
        if seq = failure then
            return null action

    action := First(seq)
    seq    := Rest(seq)

    return action
</code></pre>
<p><strong>Agent has a "Formulate, Search, Execute" design.</strong></p>
<h3 id="problem-types">Problem Types</h3>
<ul>
<li><p><strong>Deterministic, fully observable</strong> (single-state problem)</p>
<ul>
<li>Agent knows exactly which state it will be in.</li>
<li>Solution is a sequence.</li>
</ul></li>
<li><p><strong>Non-observable</strong> (sensorless problem [conformant problem])</p>
<ul>
<li>Agent may have no idea where it is.</li>
<li>Solution is again a sequence.</li>
</ul></li>
<li><p><strong>Non-deterministic and/or partially observable</strong> (contingency problem)</p>
<ul>
<li>Percepts provide <em>new</em> information about current state.</li>
<li>Often <em>interleave search &amp; execution</em>.</li>
</ul></li>
<li><p><strong>Unknown state space</strong> (exploration problem)</p>
<ul>
<li>Do not confuse with <em>non-observable</em> or <em>partially observable</em>!
<ul>
<li>Unknown state space means the <em>state space</em> is unknown --which is far worse than knowing your state space but not knowing the current state.</li>
</ul></li>
</ul></li>
<li><p><strong>Example: Vacuum World</strong></p>
<ul>
<li><img src="2D.assets/1548160209136.png" alt="1548160209136" /></li>
<li><img src="2D.assets/1548160250607.png" alt="1548160250607" /></li>
</ul></li>
<li><p><strong>Example: The 8-Puzzle</strong></p>
<ul>
<li><img src="2D.assets/1548160278151.png" alt="1548160278151" /></li>
</ul></li>
<li><p><strong>Example: Robotic Assembly</strong></p>
<ul>
<li><img src="2D.assets/1548160296041.png" alt="1548160296041" /></li>
</ul></li>
</ul>
<h3 id="search-trees">Search Trees</h3>
<p><img src="2D.assets/1548160539501.png" alt="1548160539501" /></p>
<ul>
<li>We know that a <em>state</em> is a (representation of) a physical configuration.</li>
<li>A <em>tree node</em> is a book-keeping data structure constituting part of a search tree, and includes
<ul>
<li>state</li>
<li>parent node
<ul>
<li>[which is interesting because often we keep track of <em>child nodes</em> in trees]</li>
<li>[this also ensures that there is only a single action given a node!]</li>
</ul></li>
<li>action</li>
<li>path cost (<em>i.e.</em> weight of the edge)</li>
</ul></li>
</ul>
<h4 id="tree-search-function">Tree-Search function</h4>
<pre><code>function Tree-Search(problem) returns a solution or a failure:
    initialise `frontier` using the initial state of `problem`

    loop:
        if frontier is empty:
            return failure

        leaf := frontier.Pop()
        if leaf is goal:
            return a solution

        frontier.Add-All(leaf.expand())
</code></pre>
<ul>
<li>Beware that if the tree is not a DAG (directed acyclic graph), the function is not guaranteed to terminate.</li>
</ul>
<h4 id="todo">TODO</h4>
<ul>
<li>What does being <em>parent</em> imply?</li>
<li>How does tree-search work precisely, like what does it mean to <em>expand</em> if we don’t keep track of child nodes (but just the parent)?</li>
<li>How to initialise frontier with the initial state of problem?</li>
<li>How do we compute the solution? By backtracing or by following the parent?</li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li>Problem formulation usually requires abstracting away real-world details to define a state space that can feasibly be explored.</li>
</ul>
<h2 id="18-january-2019">18 January 2019</h2>
<h3 id="search-strategy">Search Strategy</h3>
<ul>
<li>A search strategy is defined by picking the order of node expansion (taken from the <strong>frontier</strong>).</li>
<li>Strategies are evaluated along the following dimensions:
<ul>
<li><strong>completeness:</strong> does it always find a solution if one exists?</li>
<li><strong>time complexity:</strong> number of computational steps taken, or number of nodes visited</li>
<li><strong>space complexity:</strong> maximum number of nodes in memory</li>
<li><strong>optimality:</strong> does it always find a least-cost solution?</li>
</ul></li>
<li>Time and space complexity can be measured in terms of:
<ul>
<li>$b:$ maximum branching factor of the search tree</li>
<li>$d:​$ depth of the optimum solution</li>
<li>$m:$ maximum depth of the search space (which might be $\infty$).</li>
</ul></li>
</ul>
<h3 id="graph-search">Graph Search</h3>
<ul>
<li>Tree-Search algorithm augmented with a set of <em>explored</em> nodes, so to prevent visiting them again and again (thus avoiding the danger of infinite loops/recursions).
<ul>
<li>Newly expanded nodes already in the explored set are discarded.</li>
</ul></li>
</ul>
<h3 id="breadth-first-search">Breadth-First Search</h3>
<ul>
<li><strong>Keep expanding the shallowest unexpanded node.</strong></li>
<li>Use when completeness and optimality is important.</li>
</ul>
<p><strong>Implementation:</strong></p>
<ul>
<li>Frontier is a FIFO queue.</li>
</ul>
<p><strong>Properties:</strong></p>
<ul>
<li>Complete (if b [maximum branching factor of the search tree] is finite).</li>
<li>Time complexity is $O(b^d)$.</li>
<li>Space complexity is also $O(b^d)$ (keeps every node in memory).</li>
<li>.Optimal (assuming cost is the same between all nodes).</li>
<li><strong>Space is the biggest problem (more than time).</strong></li>
</ul>
<h3 id="depth-first-search">Depth-First Search</h3>
<ul>
<li><strong>Keep expanding the deepest unexpanded node.</strong></li>
<li>Use when low-cost (especially space-wise) is important.</li>
<li>Works okay when the graph is "dense" (<em>i.e.</em> $b$ is large and $m$ is small).</li>
</ul>
<p><strong>Implementation:</strong></p>
<ul>
<li>Frontier is a LIFO queue.</li>
</ul>
<p><strong>Properties:</strong></p>
<ul>
<li>Incomplete, fails in spaces with loops and infinite-depth spaces.
<ul>
<li>Of course it can be modified to avoid repeated states along the path, but that will increase the space complexity.</li>
</ul></li>
<li>Time complexity is $O(b^m)$, which is terrible if $m$ is much larger than $d$.
<ul>
<li>$b^m$ because there are $b^m$ <em>leaves</em> of a tree with depth $m$ (starting at zero) and $b$ branches at every level.</li>
</ul></li>
<li>Space complexity is $O(bm)$, <em>i.e.</em> linear!</li>
<li>Non-optimal.</li>
</ul>
<h3 id="depth-limited-search">Depth-Limited Search</h3>
<ul>
<li>Depth-first search with **depth limit $l$ ** (called <strong>cutoff</strong>).
<ul>
<li>Nodes at depth $l$ are taken to have no successors.</li>
</ul></li>
</ul>
<h4 id="iterative-deepening-search">Iterative Deepening Search</h4>
<ul>
<li>When we increase the depth limit $l$ iteratively.</li>
<li><strong>There is some cost associated with generating upper levels again and again.</strong>
<ul>
<li>Overhead is around 11% for $b = 10, d = 5$.</li>
</ul></li>
</ul>
<p><strong>Properties:</strong></p>
<ul>
<li>Complete.</li>
<li>Time complexity is $O(b^d)$.</li>
<li>Space complexity is $O(bd)$.</li>
<li>Optimal (assuming cost is the same between all nodes).</li>
<li><strong>Seems like best of both worlds!</strong></li>
</ul>
<h3 id="summary-1">Summary</h3>
<p><img src="2D.assets/1548716427150.png" alt="1548716427150" /></p>
<ul>
<li>All these algorithms are considered to be <strong>uninformed</strong>, since there is no other information about the graph then the graph itself!</li>
</ul>
<h2 id="22-january-2019">22 January 2019</h2>
<h3 id="games">Games</h3>
<ul>
<li>We concern ourselves with zero-sum games that are
<ul>
<li>Deterministic</li>
<li>Fully observable</li>
<li>Played in turns (<em>i.e.</em> agents act alternately)</li>
<li>Utilities (<em>i.e.</em> scores) at the end of game are equal and opposite (<em>i.e.</em> zero-sum)</li>
</ul></li>
<li>"Unpredictable" opponent requires us to specify a move for every possible reply of him.</li>
<li>Time limits force us to approximate, since it might be unlikely to find the goal in complex games (like chess, go, ...).</li>
</ul>
<h3 id="game-trees">Game Trees</h3>
<p><img src="2D.assets/1549840840750.png" alt="1549840840750" /></p>
<ul>
<li>Game trees are used for two-player, deterministic games.</li>
<li>Each node is a possible game state, and at each (horizontal) level it's someone else's turn.</li>
</ul>
<h3 id="normal-vs-adversarial-search">Normal vs Adversarial Search</h3>
<ul>
<li><strong>adversarial</strong> /ˌadvəˈsɛːrɪəl/ involved or characterised by conflict or opposition</li>
<li>In <strong>normal search</strong>, the optimal decision is a <em>sequence</em> of actions leading to a goal state</li>
<li>Whereas in <strong>adversarial search</strong>, the optimal decision must be taken against the optimal decision of the opponent, and thus is not a single continuous sequence of actions but it's a series of actions against the series of opponent's actions.</li>
</ul>
<h3 id="minimax">Minimax</h3>
<ul>
<li>Applicable when:
<ul>
<li>it's a deterministic</li>
<li>...zero-sum game</li>
<li>...played in turns</li>
<li>...with two players</li>
</ul></li>
<li>The idea (simply, but still accurately):
<ul>
<li>Imagine the game tree...</li>
<li>Evaluate the leaves of the tree from the point of view of the first player and assign scores.</li>
<li>Starting from the bottom of the tree (<em>i.e.</em> from its leaves), keep record of the minimum and the maximum score of each node's children.</li>
<li>The first player shall always choose* the node with the <strong>highest minimum</strong> score.
<ul>
<li>The first player is also called <strong>max-player</strong> as it tries to <em>maximise its score</em>.</li>
<li>*: "Choosing" means playing such that the next game state is as indicated by that node.</li>
</ul></li>
<li>The second player shall always choose the node with the <strong>lowest maximum</strong> score.
<ul>
<li>The second player is also called <strong>min-player</strong> as it tries to <em>minimise the first-player's score</em>.
<ul>
<li>"Why doesn't it try to maximise it's own score instead?" you might ask; because:
<ul>
<li>It's a zero-sum game, meaning that you won just as much as your opponent looses and vice versa so in the end it makes no difference.</li>
<li>Also because the whole tree, for convenience, built from the first player's point of view so consequently the actions of the second player must be in reference to the first player.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>"Best achievable payoff against best play."</li>
</ul>
<h4 id="properties-of-minimax">Properties of Minimax</h4>
<ul>
<li><p><strong>Complete?</strong> Yes, if the game tree is finite.</p></li>
<li><p><strong>Optimal?</strong> It is, against an optimal opponent though.</p></li>
<li><p><strong>Time complexity?</strong> $O(b^m)$ ($b$ is the number of branches and $m​$ is the depth of the tree)</p>
<ul>
<li>$b^m$ because there are $b^m$ <em>leaves</em> of a tree with depth $m$ (starting at zero) and $b​$ branches at every level.</li>
</ul></li>
<li><p><strong>Space complexity?</strong> $O(bm)$ (depth-first exploration)</p></li>
<li><p>For any non-trivial game, the exact solution is completely infeasible since the game tree is enormously large!</p>
<ul>
<li>You would like to eliminate large parts of game tree as much as possible to speed things up.</li>
</ul></li>
</ul>
<h3 id="alpha-beta-pruning">$\alpha-\beta$ Pruning</h3>
<ul>
<li>Applicable whenever Minimax is applicable.</li>
<li>Desirable -especially- when the game tree is too large to use Minimax algorithm feasibly.</li>
<li>We have described the Minimax algorithm consisting of two stages:
<ul>
<li>Firstly, evaluating the game tree in a bottom-up fashion.</li>
<li>Secondly, maximising and minimising the first (starting) player's score in turns.</li>
<li>Whereas, often the Minimax algorithm is applied in a top-down fashion to the game tree recursively, <em>in real time</em>.
<ul>
<li>Which is what we have been taught in class.</li>
<li>This is infeasible for huge game trees due to practical time restrictions.</li>
</ul></li>
</ul></li>
<li>$\alpha - \beta$ pruning works with top-down Minimax algorithm!</li>
<li>The idea:
<ul>
<li>We will try to <em>prune</em> (<em>i.e.</em> eliminate) recursing into branches that we know to be irrelevant to us, due to minimisation and maximisation.</li>
<li>$\alpha$ is the value of the "best" (<em>i.e.</em> highest-value) choice found so far for MAX.</li>
<li>$\beta$ is the value of the "best" (<em>i.e.</em> lowest-value) choice found so far for MIN.</li>
<li>Best explained through an example.</li>
</ul></li>
</ul>
<h4 id="example">Example</h4>
<ul>
<li>The max player starts first, and thus evaluates the first branch: <img src="2D.assets/1549887498872.png" alt="1549887498872" />
<ul>
<li>Since the max player is trying to maximise it's score all the time, it thinks that "for me to choose, any new branches that I consider must have a score greater than or equal to 3."</li>
<li>The max player knows that his opponent is trying to minimise his score.</li>
</ul></li>
<li>In the second step of evaluation, the max player starts evaluating the second branch: <img src="2D.assets/1549887862158.png" alt="1549887862158" />
<ul>
<li>The first child of the second branch is 2, and the max player now thinks that his opponent, trying to minimise his score all the time, will chose the smallest-scored node so the score of the this branch cannot be greater than two ($\le 2$).
<ul>
<li>The max player also remembers that the previous (first) branch he explored had a score of 3 so he has no reason to explore the other children of the second branch.</li>
<li><strong>This saves him so much time!</strong></li>
</ul></li>
</ul></li>
<li>In the third step of evaluation, the max player starts evaluating the third branch: <img src="2D.assets/1549888047275.png" alt="1549888047275" />
<ul>
<li>The first child of the third branch is 14, and the max player now thinks that his opponent, trying to minimise his score all the time, will choose the smallest-scored node so the score of the this branch cannot be greater than fourteen ($\le 14$).
<ul>
<li>The max player also remembers that the first branch he explored had a score of 3 so he is intrigued to explore the other children of this branch to see if it's score is greater than 3.</li>
</ul></li>
</ul></li>
<li>In the fourth step of evaluation, the max player keeps evaluating the third branch: <img src="2D.assets/1549888203446.png" alt="1549888203446" />
<ul>
<li>The second child of this branch is 5, and the max player now thinks that his opponent, trying to minimise his score all the time, will choose the smallest-scored node so the score of the this branch cannot be greater than five now ($\le 5$).
<ul>
<li>But $5 \ge 3$ so the max player decided to keep evaluating.</li>
</ul></li>
</ul></li>
<li>In the fifth (and the last) step of evaluation, the max player keeps evaluating the third branch: <img src="2D.assets/1549888328014.png" alt="1549888328014" />
<ul>
<li>The last child of this branch is 2, and having explored all the children, the max player calculates the score of this branch as 2 (the minimum of all children since this the min player [his opponent] will try to minimise his score in the next turn).</li>
<li>Also having explored all the branches, the max player chooses the branch with the highest score, which is the first branch with score 3.</li>
</ul></li>
</ul>
<h4 id="properties">Properties</h4>
<ul>
<li><strong>Pruning does not affect the final result.</strong></li>
<li><em>Good move ordering</em> improves the effectiveness of pruning.
<ul>
<li>In the example, imagine if the first child of the third branch was 2 instead of 14:
<ul>
<li>Then we would be able to prune other two children!</li>
</ul></li>
</ul></li>
<li>With "perfect (good move) ordering", its time complexity is $O\left(b^{m/2} = \left(\sqrt b\right) ^m\right)$.
<ul>
<li>$b$ being number of branches, and $m$ being the depth of the tree.</li>
<li>See <a href="http://www.cs.utsa.edu/~bylander/cs5233/a-b-analysis.pdf">http://www.cs.utsa.edu/~bylander/cs5233/a-b-analysis.pdf</a> for more details.</li>
<li><strong>$m/2$ implies that using $\alpha-\beta$ pruning we can explore twice as much depth as we could before in the same duration.</strong></li>
</ul></li>
<li>A simple example of the value of reasoning about <em>"which computations are relevant?"</em> (a form of <em>meta-reasoning</em>).</li>
</ul>
<h3 id="in-practice">In Practice</h3>
<ul>
<li>Due to time limits, there is often a <em>cutoff point:</em> no nodes beyond a certain depth limit will be explored.
<ul>
<li>Another alternative is <em>quiescence search</em>, which tries to search interesting positions to a greater depth than quiet ones.
<ul>
<li>There are well-known positions and strategies in many games, an example being chess.</li>
</ul></li>
</ul></li>
<li>In the presence of a cutoff point, the evaluation function has to <em>estimate</em>!
<ul>
<li>Weighted sum of features is a classic example.
<ul>
<li>This should remind you of <em>utility-based agents</em>!</li>
</ul></li>
</ul></li>
</ul>
<h2 id="24-january-2019">24 January 2019</h2>
<h3 id="informed-search">Informed Search</h3>
<ul>
<li>"Informed" indicated that there is "additional information" about the graph.
<ul>
<li><em>E.g.</em> for a graph where nodes are cities and edges are roads between cities with weights to indicate the distance, additional information might be the Euclidean distance (<em>i.e.</em> straight-line distance [SLD]) between cities.</li>
</ul></li>
<li>Evaluator functions that utilise additional information are called <em>heuristic functions</em> (denoted as $h(n)$).
<ul>
<li><strong>Heuristics are estimations.</strong></li>
</ul></li>
<li>Today we'll see some algorithms that exploit this additional information.</li>
</ul>
<h3 id="best-first-search">Best-First Search</h3>
<ul>
<li>In the most generic sense of the word, best-first means that, well, the nodes (in the <em>frontier</em>) that are deemed to be the <em>best</em> are expanded first.
<ul>
<li>Based on an evaluation function $f(n)$ taking a node $n$.</li>
</ul></li>
<li>It means <em>nothing</em>, <strong>nothing</strong> more than that!</li>
<li>There are two search algorithms which belongs to best-first family:
<ul>
<li>Greedy best-first search</li>
<li>A* search</li>
</ul></li>
</ul>
<h3 id="greedy-best-first-search">Greedy Best-First Search</h3>
<ul>
<li>Evaluation function is simply the heuristic function.
<ul>
<li>Alternatively stated as $f(n) = h(n)$.</li>
</ul></li>
<li>$h(n)$ is the estimated cost (<em>i.e.</em> heuristic) of cheapest path from node $n$ to the goal state.</li>
<li><strong>Greedy Best-First Search expands the node that appears to be closest to the goal at every step.</strong></li>
</ul>
<p><strong>Properties:</strong></p>
<ul>
<li><p>Incomplete, since it can get stuck in loops.</p>
<ul>
<li>Considering we are using this very weird tree visualisation as shown below: <img src="2D.assets/1549926546088.png" alt="1549926546088" />
<ul>
<li>... which is not exactly a tree because the exact *** nodes (<em>i.e.</em> cities) can appear multiple times in the tree, meaning that semantically there are loops in your tree (wat) but formally there aren't any (WAT)...</li>
</ul></li>
<li>Using logical reasonable rational <em>normal</em> <strong>graphs, best-first search is complete in finite space, but not in infinite ones.</strong></li>
</ul></li>
<li><p>Time complexity is $O(b^m)$ for the weird-tree version but a good heuristic can give dramatic improvement.</p>
<ul>
<li>$b$ for number of branches from each node (assuming it's constant), and $m$ for the depth of the tree (considering root at depth 0).</li>
</ul></li>
<li><p>Space complexity is $O(b^m)$ since it keeps all nodes in memory.</p></li>
<li><p><strong>Non-optimal.</strong></p>
<ul>
<li><p>As a rule of thumb, you can say that "greedy" algorithms are all non-optimal.</p>
<blockquote>
<p>Then he said to them, “Watch out! Be on your guard against all kinds of greed [...] Luke 12:15</p>
</blockquote></li>
</ul></li>
</ul>
<h3 id="a-search">A* Search</h3>
<ul>
<li>Similar to Greedy Best-First Search, but considers the cost of getting to the path as well.</li>
<li>Evaluation function $f(n) = g(n) + h(n)$:
<ul>
<li>$g(n)$: cost so far to reach $n$</li>
<li>$h(n)$: estimated cost from $n$ to the goal (heuristic)</li>
<li>$f(n)$: estimated total cost of path through $n$ to the goal</li>
</ul></li>
<li>Realise that unlike Greedy Best-First Search, we know need our edges of the graph to be weighted.
<ul>
<li>Another requirement is that all the weights must be non-negative.</li>
</ul></li>
</ul>
<p><strong>Properties:</strong></p>
<ul>
<li>Both complete and optimal if $h(n)$ is <em>admissible</em> <strong>and</strong> <em>consistent</em>.</li>
<li>Time complexity is <em>exponential</em>.</li>
<li>Space complexity is also <em>exponential</em> as it keeps all the nodes in memory.</li>
</ul>
<h3 id="heuristic-functions">Heuristic Functions</h3>
<ul>
<li>A <em>heuristic</em> is any method that is believed or “practically” (not theoretically!) proven to be useful (in many “practical” cases) for the solution of a given problem..…
<ul>
<li>..…although there is no guarantee that it will always work or lead to an optimal solution.</li>
</ul></li>
<li>Heuristic functions can never change the worst case complexity of an algorithm but can help in the average case.</li>
</ul>
<p><strong>Admissibility:</strong></p>
<ul>
<li>An heuristic $h(n)$ is admissible if for every node $n$, $h(n)$ is less than or equal to the <em>true</em> cost to reach the goal state from $n$.</li>
<li>In other words, <strong>an admissible heuristic never overestimates the cost to reach the goal.</strong> Also called <em>optimistic</em>.
<ul>
<li>Thus, $f(n) = g(n) + h(n)$ <em>never</em> overestimates the true cost of a solution.</li>
</ul></li>
</ul>
<p><strong>Consistency:</strong></p>
<ul>
<li>An heuristic is consistent if it abides to triangle inequality: <img src="2D.assets/1549927757943.png" alt="1549927757943" />
<ul>
<li>In the figure above, $h(n)$ is consistent iff $h(n) \le c(n, a, n') + h(n')$.
<ul>
<li>$h(n)$ is the estimated cost of getting from $n$ to the goal state $G$.</li>
<li>$c(n, a, n')$ cost of getting from $n$ to $n'$ (using branch $a$).</li>
<li>$h(n')$ is the estimated cost of getting from $n'$ to the goal state G.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Dominance:</strong></p>
<ul>
<li>Both admissible, if $h_2(n) \ge h_1(n)$ for all $n$ then
<ul>
<li><strong>$h_2$ is said to <em>dominate</em> $h_1$</strong>, and that</li>
<li>$h_2$ is better for search!</li>
<li>Because if $h_2$ is greater than $h_1$ for all $n$ whilst staying admissible, it means that it must have much more accurate estimations than $h_1$!</li>
</ul></li>
</ul>
<p><strong>Common Heuristics:</strong></p>
<ul>
<li><strong>Euclidean (Straight Line) Distance:</strong>
<ul>
<li>Often used for calculations in two-dimensional spaces with no restriction on the direction of movement.</li>
<li>Due to triangle inequality, it is <em>guaranteed</em> to be admissible.</li>
</ul></li>
<li><strong>Manhattan (Taxicab) Distance:</strong>
<ul>
<li>Often used for calculations in two-dimensional spaces with restriction on the direction of the movement.
<ul>
<li>Where only allowed directions are North-East-South-West.</li>
</ul></li>
<li>Where the restriction on the direction applies, it is admissible.</li>
</ul></li>
</ul>
<h4 id="relaxed-problems">Relaxed Problems</h4>
<ul>
<li>A problem with fewer restrictions on the actions is called a <em>relaxed problem</em>.</li>
<li><strong>The cost of an optimal solution to a relaxed problem is an <em>admissible</em> heuristic for the original problem.</strong>
<ul>
<li><strong>We can use relaxation to automatically generate admissible heuristics.</strong></li>
</ul></li>
</ul>
<h2 id="25-january-2019">25 January 2019</h2>
<h3 id="knowledge-based-agents">Knowledge-Based Agents</h3>
<ul>
<li>Consist of two subsystems:
<ul>
<li><strong>Inference Engine</strong> which contains domain independent algorithms for logical inference</li>
<li><strong>Knowledge Base (KB)</strong> which consists of domain-specific content</li>
</ul></li>
<li>A knowledge base is <em>a set of sentences in a formal language</em>.
<ul>
<li>In other words, we <em>tell</em> the agent what it needs to know.</li>
<li>Then the agent can <em>ask</em> the knowledge base (with the help of the inference engine of course!) what to do.
<ul>
<li>Answers should follow (<em>i.e.</em> should be "inferrable") from the knowledge base!</li>
</ul></li>
</ul></li>
<li>Knowledge bases are <em>declarative</em>, that is to say, the sentences <em>define</em> what things <em>are</em>, not how they became our how they <em>do</em>.</li>
<li>Knowledge base can be part of a single agent or can be accessible to many agents.</li>
<li>Sometimes we need to analyse a knowledge base at different levels:
<ul>
<li><strong>knowledge level:</strong> what a KB knows/contains, regardless of how its implementation.
<ul>
<li>This is the level that we often concern ourselves with.</li>
</ul></li>
<li><strong>implementation level:</strong> thinking of a KB as a combination of data structures and algorithms that manipulate them</li>
</ul></li>
</ul>
<p><strong>Pseudocode:</strong></p>
<pre><code>function KB-Agent(percept) returns an action:
    persistent: KB -  knowledge base
                t  - a counter, initially 0, indicating time (monotonic clock)

    # We tell our KB about the most recent state of the world
    # as we perceive.
    Tell(KB, Make-Percept-Sentence(percept, t))

    # &quot;What should I do now (t)?&quot; we ask our knowledge base.
    action := Ask(KB, Make-Action-Query(t))

    # We tell our KB about the action we are about to take.
    Tell(KB, Make-Action-Sentence(action, t))

    # Increase our monotonic clock
    t := t + 1

    return action
</code></pre>
<p>The agent must be able to:</p>
<ul>
<li>Represent states (implicitly in the KB), percepts (<code>Make-Percept-Sentence()</code>), actions (<code>Make-Action-Sentence()</code>) etc.</li>
<li>Incorporate new percepts (line 7)</li>
<li>Update internal representations of the world (<code>Tell()</code>)</li>
<li>Deduce hidden properties of the world (implicitly through the KB)</li>
<li>Deduce appropriate action (<code>Ask()</code>)</li>
</ul>
<h3 id="peas">PEAS</h3>
<ul>
<li><strong>P</strong>erformance measure <em>i.e.</em> evaluation methods; closely related to "goals!"</li>
<li><strong>E</strong>nvironment read section <em>15 January 2019</em> for a description and properties of environments; <em>beware of oversimplifying while thinking about the environment!</em></li>
<li><strong>A</strong>ctuators <em>actions</em> that can be taken in the world; might affect our environment too.</li>
<li><strong>S</strong>ensors <em>information</em>/percepts that we receive</li>
</ul>
<p><strong>Example: Wumpus World</strong></p>
<p><img src="2D.assets/1549977445526.png" alt="1549977445526" /></p>
<h3 id="logics">Logics</h3>
<ul>
<li><p><strong>Logics</strong> (like <em>mathematics</em> [yes, both plural]) are formal language<strong>s</strong> for representing information such that <em>conclusions can be drawn</em></p></li>
<li><p><strong>Syntax</strong> defines the <em>sentences</em> in the language.</p></li>
<li><p><strong>Semantics</strong> define the <em>meaning</em> of sentences.</p>
<ul>
<li><p>Of course, we humans are the ultimate source of any meaning in any sentence.</p>
<blockquote>
<p>That's all the motorcycle is, a system of concepts worked out in steel. There's no part in it, no shape in it, that is not out of someone's mind --- Robert M. Pirsig, <em>Zen and the Art of Motorcycle Maintenance</em></p>
</blockquote></li>
<li><p>Our sentences (generated from sensory data or whatever) are often assumed to be <em>self-evident</em> enough to not require any further consideration, thus we focus on syntactic manipulation (such as logical inference.)</p>
<ul>
<li>Because semantics is dangerously close to <em>metaphysics</em>.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="entailment">Entailment</h4>
<ul>
<li>Entailment means that <em>one thing follows from another.</em> $$ KB \vDash \alpha$$</li>
</ul>
<p><strong>Semantic Entailment (or just <em>entailment</em>), Syntactic Entailment (or <em>inference</em>), and Material Implication (or just <em>implication</em>):</strong></p>
<ul>
<li><p>Those three are often confused, and the difference are rarely well understood so here is some clarification.</p></li>
<li><p><strong>Semantic Entailment</strong> ($KB \vDash \alpha​$) means that $KB​$ <strong>SHOULD</strong> entail $\alpha​$.</p>
<ul>
<li>As we have said earlier, semantics is dangerously close to <em>metaphysics</em> and so is this concept.</li>
<li>We say that, we as an human being, acknowledge that we can see that $KB$ indeed entails $\alpha$.</li>
<li>In other words, semantic entailment is a <em>relationship</em> between sentences that is based on semantics.</li>
</ul></li>
<li><p><strong>Syntactic Entailment</strong> ($KB \vdash \alpha$) means that our inference engine entails (or can entail when asked) $\alpha$ from $KB$.</p>
<ul>
<li>Realise that this talks about a <em>mechanical process</em>.
<ul>
<li>Sometimes you'll see $KB \vdash_i \alpha$ where $i$ is the (name of the) process.</li>
</ul></li>
<li>This is indeed the case ($KB \vDash \alpha$) if our syntactic entailment algorithm is <strong>sound</strong>, meaning that it won't prove anything that's wrong (<em>i.e.</em> no wrong proofs).</li>
<li>If our syntactic entailment algorithm is <strong>complete</strong>, then we can entail (<em>i.e.</em> prove) any $\alpha​$ can indeed be entailed from $KB​$.</li>
</ul></li>
<li><p><strong>Material Implication</strong> ($p \Rightarrow q$ or alternatively $p \rightarrow q$) is a boolean <em>operator</em> so that an expression such as $p \to q$ can be evaluated to a boolean value.</p>
<ul>
<li>Neither semantic entailment nor syntactic entailment are boolean operators; they do <em>not</em> evaluate to a boolean value!</li>
<li>The difference is not as clear in propositional logic, because given a $KB := {p, q, r, \lnot s}$ (in CNF) and $\alpha = p \land r$, it's clear to us that $KB \vDash \alpha$ and material implication $$ KB \to p \land r\\ {p, q, r, \lnot s} \to p \land r $$ indeed is true, but realise that:
<ul>
<li>Material implication does <em>not</em> generate new propositions from a given KB.</li>
<li>Material implication is an <em>expression</em>, it's <em>declarative</em>. We need a <em>process</em>, an <em>algorithm</em> to use in our agents!
<ul>
<li>This is even more important with first-order logic since even a simple-looking material implication is not as immediately evident!</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Hopefully things will become clearer as you read more on the subject and gain familiarity with them.</p></li>
<li><p>A knowledge base KB entails sentence $\alpha$ if and only if $\alpha$ is true in <em>all worlds</em> <strong>where KB is true</strong>.</p></li>
</ul>
<h4 id="models">Models</h4>
<ul>
<li>Logicians typically think in terms of <strong>models</strong>, which are <em>formally structured worlds</em> with respect to which <em>truth can be evaluated</em>.
<ul>
<li>For instance in propositional logic, each model specifies true/false for each proposition symbol.
<ul>
<li>So we can think of a model as a row of a truth table!</li>
</ul></li>
</ul></li>
<li>We say $m$ is <strong>A</strong> model of sentence $\alpha$, if $\alpha$ is true in $m$.</li>
</ul>
<p><img src="2D.assets/1549980063350.png" alt="1549980063350" /></p>
<ul>
<li><p>$M(\alpha)​$ is the set of all models of $\alpha​$.</p></li>
<li><p>$KB \vDash \alpha \text{ iff } M(KB) \subseteq M(\alpha)$.</p>
<ul>
<li>It might seem counter-intuitive at first so make sure you get it right!</li>
<li>Because: a knowledge base KB entails sentence $\alpha$ if and only if $\alpha$ is true in <em>all worlds</em> <strong>where KB is true</strong>.</li>
</ul></li>
<li><p>The stronger an assertion, the fewer models it has.</p></li>
<li><p>Indeed, our <em>models</em> are often <em>game states</em>. <img src="2D.assets/1549980739475.png" alt="1549980739475" /></p></li>
</ul>
<h4 id="equivalence">Equivalence</h4>
<ul>
<li>$\alpha$ is equivalent to $\beta$: $\alpha \equiv \beta$</li>
<li>Two sentences are logically equivalent iff true in the same models.
<ul>
<li>$\alpha \equiv \beta \text{ iff } \alpha \vDash \beta \land \beta \vDash \alpha$</li>
<li>In other words $\alpha \equiv \beta \text{ iff } M(\alpha) \subseteq M(\beta) \land M(\beta) \subseteq M(\alpha)$</li>
<li>Yani $\alpha \equiv \beta \text{ iff } M(\alpha) = M(\beta)$ which is to say "two sentences are logically equivalent iff true in the same models."</li>
</ul></li>
</ul>
<h4 id="inference-by-enumeration">Inference by Enumeration</h4>
<ul>
<li>Depth-first enumeration of all models is both sound and complete.
<ul>
<li>For $n$ symbols, it's time complexity is $O(2^n)$ and space complexity is $O(n)$.</li>
</ul></li>
<li>It's basically enumerating all the rows of a truth table by assigning each propositional symbol true and false, and at each row, evaluating $KB \to \alpha$.</li>
</ul>
<h4 id="validity-and-satisfiability">Validity and Satisfiability</h4>
<ul>
<li><p>A sentence is <strong>valid</strong> if it's true in <em>all</em> models.</p>
<ul>
<li>There is no such thing as <em>invalid</em> sentence!</li>
</ul></li>
<li><p>Validity is connected to inference via the <em>Deduction Theorem:</em></p>
<ul>
<li>$KB \vDash \alpha \text{ if and only if } KB \to \alpha \text{ is valid}$</li>
</ul></li>
<li><p>A sentence is <strong>satisfiable</strong> if it's true in <em>some</em> model(s).</p>
<ul>
<li>A sentence is <strong>unsatisfiable</strong> if it is true in <em>no</em> models.</li>
</ul></li>
<li><p>Satisfiability is connected to inference via the following:</p>
<ul>
<li>$KB \vDash \text{ if and only if } KB \land \lnot \alpha \text{ is unsatisfiable}$</li>
</ul></li>
</ul>
<h4 id="proof-methods">Proof Methods</h4>
<ul>
<li>Proof methods divide into (roughly) two kinds:
<ul>
<li><strong>Application of Inference Rules</strong>
<ul>
<li>Legitimate (sound) <em>generation of new sentences</em> from old.</li>
<li>A proof in such system would be a sequence of inference rule applications.</li>
<li>Typically require transformation of sentences into a <em>normal form</em>.</li>
<li>Example: resolution.</li>
</ul></li>
<li><strong>Model Checking</strong>
<ul>
<li>Truth table enumeration.
<ul>
<li>Always exponential in $n$</li>
<li>Improved backtracking methods exists, <em>e.g.</em> DPLL (Davis-Putnam-Logemann-Loveland) method.</li>
</ul></li>
<li>Heuristic search methods in the model space also exist.
<ul>
<li>Sound but incomplete.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h2 id="29-january-2019">29 January 2019</h2>
<h3 id="effective-propositional-inference">(Effective) Propositional Inference</h3>
<ul>
<li><em>Two</em> families of efficient algorithms for propositional inference:
<ul>
<li><strong>Complete backtracking search algorithms</strong>
<ul>
<li>DPLL (Davis, Putnam, Logemann, Loveland)</li>
</ul></li>
<li><strong>Incomplete local search algorithms</strong>
<ul>
<li><code>WalkSAT</code> algorithm</li>
</ul></li>
</ul></li>
</ul>
<h3 id="conjunctive-normal-form-cnf">Conjunctive Normal Form (CNF)</h3>
<ul>
<li><p>Both DPLL and <code>WalkSAT</code> manipulate formulae in conjunctive normal form.</p></li>
<li><p><em>Sentence</em> is a formula whose satisfiability is to be determined.</p>
<ul>
<li>In CNF form, a sentence is a conjunction ($C_1 \land \ldots \land C_n$) of clauses.</li>
</ul></li>
<li><p>A <em>clause</em> is a <em>disjunction</em> of literals ($L_1 \lor \ldots \lor L_m$).</p></li>
<li><p>A <em>literal</em> is a proposition or a negated proposition ($L_1$ or $\lnot L_1$).</p></li>
<li><p>Clauses and literals can be separated by comma too:</p>
<ul>
<li>$(A, \lnot B), (B, \lnot C)$ represents $(A \lor \lnot B) \land (B \lor \lnot C)$.</li>
</ul></li>
<li><p>To convert to CNF:</p>
<ol>
<li><p><strong>Eliminate "if-and-only-if"s:</strong> $ P \iff Q $ becomes $P \implies Q \land Q \implies P$</p></li>
<li><p><strong>Eliminate implications:</strong></p>
<p>$P \implies Q$ becomes $\lnot P \lor Q$</p></li>
<li><p><strong>Move "not" inwards using <em>de Morgan's rules</em> such that there are no "not"s in front of any parentheses:</strong> $P \lor \lnot (Q \land R)$ becomes $P \lor \lnot Q \lor \lnot R$</p></li>
<li><p><strong>Distribute OR over AND:</strong> $P \lor (Q \land R)$ becomes $(P \lor Q) \land (P \lor R)$</p></li>
<li><p><strong>Flatten:</strong> $P \land (Q \lor R \lor (S \lor T))$ becomes $P \land (Q \lor R \lor S \lor T)$</p></li>
</ol></li>
<li><p>N.B. that the sentences with a single clause are also in CNF form (even though they might look otherwise)! For instance:</p>
<ul>
<li>$P \lor Q \lor R$ is represented as $(P, Q, R),$ and is a sentence with a single clause!</li>
</ul></li>
<li><p>Did you know that you can use <strong>bolexman</strong> &lt; labs.boramalper.org/boolexman &gt; to convert <em>any</em> propositional expression into CNF?</p>
<pre><code>0001&gt; toCNF (P iff R or Q)

toCNF (P &lt;=&gt; (R v Q))
━━━━━━━━━━━━━━━━━━━━━

1. Transform all if-then-else (ITE) expressions:
  No ITE expressions are found!

After all ITE expressions are transformed:
  (P &lt;=&gt; (R v Q))

2. Transform all if-and-only-if (IFF) expressions:
  • (P &lt;=&gt; (R v Q))
    is transformed into
    !(P + (R v Q))

After all IFF expressions are transformed:
  !(P + (R v Q))

3. Tranform all implications:
  No implications are found!

After all implications are transformed:
  !(P + (R v Q))

4. Tranform all exclusive-or (XOR) expressions:
  • (P + (R v Q))
    is transformed into
    ((P v R v Q) ^ (!P v !(R v Q)))

After all XOR expressions are transformed:
  !((P v R v Q) ^ (!P v !(R v Q)))

5. Distribute NOTs:
  • !(R v Q)
    is transformed into
    (!R ^ !Q)
  • !((P v R v Q) ^ (!P v (!R ^ !Q)))
    is transformed into
    (!(P v R v Q) v !(!P v (!R ^ !Q)))
  • !(P v R v Q)
    is transformed into
    (!P ^ !R ^ !Q)
  • !(!P v (!R ^ !Q))
    is transformed into
    (!!P ^ !(!R ^ !Q))
  • !!P
    is transformed into
    P
  • !(!R ^ !Q)
    is transformed into
    (!!R v !!Q)
  • !!R
    is transformed into
    R
  • !!Q
    is transformed into
    Q

After all NOTs are distributed:
  ((!P ^ !R ^ !Q) v (P ^ (R v Q)))

6. Distribute ORs over ANDs:
  • ((!P ^ !R ^ !Q) v (P ^ (R v Q)))
    is transformed into
    ((!P v R v Q) ^ (!R v P) ^ (!Q v P))

Resultant expression:
  ((!P v R v Q) ^ (!R v P) ^ (!Q v P))
</code></pre></li>
</ul>
<h3 id="dpll-algorithm">DPLL Algorithm</h3>
<ul>
<li>Used to determine if an input propositional logic sentence (in CNF) is <em>satisfiable</em>.</li>
<li>It is much better than truth table enumeration because of:
<ul>
<li>Tautology deletion (optional)</li>
<li>Early termination</li>
<li>Various heuristics
<ul>
<li>Unit clause heuristic</li>
<li>Pure symbol heuristic</li>
<li>The order of heuristics is important too! Not covered here, but a little thinking would help.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="early-termination">Early Termination</h4>
<ul>
<li>We know that:
<ul>
<li>A <em>clause</em> is true if <strong>one</strong> of its literals is true.</li>
<li>A <em>sentence</em> is false if <strong>any</strong> of its clauses is false.</li>
</ul></li>
<li>We can use these facts to avoid unnecessary work.</li>
</ul>
<h4 id="unit-clause-heuristic">Unit Clause Heuristic</h4>
<ul>
<li>A <em>unit clause</em> is a clause that consists of a single <em>literal</em>.
<ul>
<li>...and we know that, since the sentence is in CNF, the literal must be true.</li>
</ul></li>
<li>The idea can also be extended to cases where we know <strong>all but one</strong> literal is not known to be false; for example:
<ul>
<li>Given $(\lnot B), (\lnot C), (A, B, C)$</li>
<li>Apply <em>unit clause heuristic</em> on clauses $(\lnot B)$ and $(\lnot C)$: $\cancel{(\lnot B)}, \cancel{(\lnot C)}, (A, B, C)$</li>
<li>We know that $B$ and $C$ are false, therefore we can apply unit clause heuristic on clause $(A, B, C)$ too: $\cancel{(\lnot B)}, \cancel{(\lnot C)}, \cancel{(A, B, C)}$</li>
<li>Thus we found a satisfying evaluation: $A$ is true. $B$, $C$ are false.</li>
</ul></li>
</ul>
<h4 id="pure-symbol-heuristic">Pure Symbol Heuristic</h4>
<ul>
<li>A <em>pure symbol</em> is a literal that appears always with the same "sign" or "polarity" in <em>all</em> clauses.
<ul>
<li>In more comprehensible words, the literal is either always negated or never negated!</li>
<li>For instance, in $(A, \lnot B), (\lnot B, \lnot C), (C, A)$
<ul>
<li>$A$ and $\lnot B$ are pure.</li>
<li>$C$ is impure.</li>
</ul></li>
</ul></li>
<li>Make <em>pure symbols</em> true.
<ul>
<li>In our example above, we would make $A$ and $\lnot B$ true (thus making $B$ false).
<ul>
<li>This, in combination with early termination, would eliminate all the clauses without us having to consider $C$; neat!</li>
</ul></li>
</ul></li>
</ul>
<h4 id="tautology-deletion">Tautology Deletion</h4>
<ul>
<li>Tautology is when both a proposition and its negation are in a single clause: <em>e.g.</em> $(A, B, \lnot A)$</li>
<li>Tautologies are bound to be true!
<ul>
<li>Therefore they can be deleted at the very beginning (sort of like preprocessing).</li>
</ul></li>
</ul>
<h3 id="walksat-algorithm"><code>WalkSAT</code> Algorithm</h3>
<ul>
<li>Incomplete, local search algorithm.</li>
<li>Algorithm checks for satisfiability by randomly flipping the values of variables.</li>
<li>Evaluation function is the <em>min-conflict</em> <strong>heuristic</strong> that minimises the number of unsatisfied <em>clauses</em>.
<ul>
<li>Aims a balance between greediness and randomness.</li>
</ul></li>
<li>Much more efficient than DPLL: <img src="2D.assets/1550666497104.png" alt="1550666497104" /></li>
</ul>
<h3 id="hard-satisfiability-problems">Hard Satisfiability Problems</h3>
<ul>
<li>Remember than every propositional sentence can be represented as a 3-CNF (CNF with maximum literals per clause).</li>
<li>For example: $$(\lnot D \lor \lnot B \lor C) \land (A \lor E \lor F) \land \ldots$$
<ul>
<li>$m$: number of clauses</li>
<li>$n$: number of symbols/literals</li>
</ul></li>
<li>Hard problems seem to cluster near $m / n = 4.3$ (critical point). <img src="2D.assets/1550666429472.png" alt="1550666429472" /></li>
</ul>
<h3 id="expressiveness-limitation-of-propositional-logic">Expressiveness Limitation of Propositional Logic</h3>
<ul>
<li>While programming agents, our knowledge bases has to contain sentences for each <em>instance</em> to which a phenomena applies.</li>
<li>For instance, we know that the gravity applies to all objects with mass (pardon the physical inaccuracies). We can state this in predicate (first-order) logic as: $\forall x. (has\_mass(x) \implies affected\_by\_gravity(x))$ whereas in propositional logic we must create a proposition for each object with mass in our universe explicitly: $$\begin{align*} G_{Bora} &amp;\equiv affected\_by\_gravity(Bora)\\ G_{Apple} &amp;\equiv affected\_by\_gravity(Apple)\\ &amp;\vdots \end{align*}$$
<ul>
<li><em>Rapid</em> proliferation of clauses, even in smallest cases!</li>
</ul></li>
</ul>
<h2 id="31-january-2019">31 January 2019</h2>
<h3 id="constraint-satisfaction-problems-csps">Constraint Satisfaction Problems (CSPs)</h3>
<ul>
<li><p><strong>Standard Search Problem</strong></p>
<ul>
<li><p><em>State</em> is a "black box", any data structure that supports</p>
<ul>
<li>a <em>successor</em> function</li>
<li>a <em>heuristic</em> function</li>
<li>and a <em>goal test</em></li>
</ul>
<p>would suffice.</p></li>
</ul></li>
<li><p><strong>Constraint Satisfaction Problem</strong></p>
<ul>
<li><em>State</em> is defined by <em>variables</em> $X_i$ with <em>values</em> from <em>domain</em> $D_i$.</li>
<li><em>Goal test</em> is a set of <em>constraints</em> specifying allowable combinations of values for subsets of variables.
<ul>
<li>In a more clear way: constrains are logical expressions over <em>variables</em>.</li>
</ul></li>
<li>This in turn allows useful <em>general-purpose</em> algorithms with more power than standard search algorithms.</li>
<li>A <em>solution</em> is a <em>complete</em> and <em>consistent</em> assignment.</li>
</ul></li>
</ul>
<h4 id="example-1">Example</h4>
<p><img src="2D.assets/1550669892711.png" alt="1550669892711" /></p>
<ul>
<li><strong>Variables:</strong> $WA,\ NT,\ Q,\ NSW,\ V,\ SA,\ T​$</li>
<li><strong>Domains:</strong> $D_i = {\text{red},\text{ green}, \text{ blue}}$</li>
<li><strong>Constraints:</strong> "adjacent regions must have different colours"
<ul>
<li>$WA \neq NT$</li>
<li>$WA \neq SA$</li>
<li>...</li>
</ul></li>
<li><strong>Solution:</strong> <img src="2D.assets/1550670283092.png" alt="1550670283092" /></li>
</ul>
<h3 id="constraint-graph">Constraint Graph</h3>
<ul>
<li><strong>Binary CSP:</strong> each constraint relates exactly two variables.</li>
<li><strong>Constraint graph:</strong>
<ul>
<li>Nodes are variables.</li>
<li>Edges (arcs) represent constraints relating two nodes each.</li>
</ul></li>
<li><strong>Example:</strong> (Australia again) <img src="2D.assets/1550670414778.png" alt="1550670414778" /></li>
</ul>
<h3 id="varieties-of-csps">Varieties of CSPs</h3>
<ul>
<li><strong>Discrete variables:</strong>
<ul>
<li><strong>Finite Domains:</strong>
<ul>
<li>For $n$ variables, domain size $d \implies O(d^n)$</li>
<li><em><strong>enumerable</strong></em>
<ul>
<li>You can basically enumerate all the possible assignments and brute force it!</li>
</ul></li>
<li><em>e.g.</em> Boolean CSPs, incl. Boolean satisfiability (NP-complete)</li>
</ul></li>
<li><strong>Infinite domains:</strong>
<ul>
<li>Integers, strings, <em>etc.</em></li>
<li>For example, job scheduling with variables for start and end time of each job.</li>
<li><em><strong>not enumerable</strong></em>
<ul>
<li>There is such thing as enumerating all the possible assignments!</li>
</ul></li>
<li>Requires a <em>constraint language</em> with computing semantics.
<ul>
<li><em>e.g.</em> $\text{StartJob}_1 + 5 \le \text{StartJob}_3$</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Continuous variables:</strong>
<ul>
<li><em><strong>not enumerable</strong></em> again!</li>
<li>Again, requires a <em>constraint language</em> with computing semantics.</li>
<li>Linear constraints are solvable in polynomial time by linear programming.</li>
</ul></li>
</ul>
<h3 id="varieties-of-constraints">Varieties of Constraints</h3>
<ul>
<li><strong>Unary</strong> constraints involve a single variable:
<ul>
<li><em>e.g.</em> $SA \neq \text{green}$</li>
</ul></li>
<li><strong>Binary</strong> constraints involve pairs of variables:
<ul>
<li><em>e.g.</em> $SA \neq WA$</li>
</ul></li>
<li><strong>Higher-order</strong> constraints involve 3 or more variables.</li>
<li><strong>Global</strong> constraints concern an <em>arbitrary</em> number of variables.
<ul>
<li><em>I.e.</em> Can be unary, binary, or higher-order.</li>
</ul></li>
</ul>
<h3 id="real-world-csps">Real-World CSPs</h3>
<ul>
<li><p>Assignment problems</p>
<ul>
<li><em>e.g.</em> who teaches what class</li>
</ul></li>
<li><p>Timetabling problems (Scheduling)</p>
<ul>
<li><em>e.g.</em> which class is offered when and where</li>
</ul></li>
<li><p><strong>Many real-world problems involve real-valued variables.</strong></p></li>
</ul>
<h3 id="backtracking-search">Backtracking Search</h3>
<ul>
<li><p><em>States</em> are defined by the values assigned so far.</p></li>
<li><p><strong>Initial State:</strong> the empty assignment ${}$.</p></li>
<li><p><strong>Successor Function:</strong> assign a value to an unassigned variable that does not conflict with current assignment.</p>
<ul>
<li>Fail if no legal assignments!</li>
</ul></li>
<li><p><strong>Goal Test:</strong> checks whether the current assignment is complete.</p></li>
<li><p>This is the same for all CSPs.</p></li>
<li><p>Using depth-first search, this is called <em>backtracking</em> search.</p>
<ul>
<li>Basically enumerating all the assignments and brute forcing...</li>
<li><strong>Complete on finite domains, incomplete otherwise!</strong></li>
</ul></li>
</ul>
<h4 id="improving-backtracking-search-efficiency">Improving Backtracking Search Efficiency</h4>
<ul>
<li>Some questions, which are applicable in <em>all</em> cases (<em>i.e.</em> "general-purpose"), are cruical:
<ul>
<li>"Which <em>variable</em> should be assigned next?"</li>
<li>"Then, in what order should its <em>values</em> be tried?"</li>
<li>"What inferences should be performed at each step of the search?"
<ul>
<li>Arc-consistency!</li>
</ul></li>
<li>"Can we detect inevitable failures early?"</li>
</ul></li>
</ul>
<h5 id="most-constrained-variable">Most Constrained Variable</h5>
<p><img src="2D.assets/1550672451779.png" alt="1550672451779" /></p>
<ul>
<li>Choose the variable with the fewest legal values.
<ul>
<li>In other words, prioritise those which are harder to satisfy.</li>
</ul></li>
<li>Also called <em>minimum remaining values (MRV)</em>.</li>
</ul>
<h5 id="most-constraining-variables">Most Constraining Variables</h5>
<p><img src="2D.assets/1550672510412.png" alt="1550672510412" /></p>
<ul>
<li>Used as a <em>tie-breaker</em> among most constrained variables.</li>
<li>Chose the variable with the most constraints on remaining variables, thus reducing branching.</li>
<li>It's a <em>degree heuristic</em>.</li>
</ul>
<h5 id="least-constraining-value">Least Constraining Value</h5>
<p><img src="2D.assets/1550672531565.png" alt="1550672531565" /></p>
<ul>
<li>Given a variable, choose the least constraining value.
<ul>
<li>In other words, choose the value that rules out the <em>fewest</em> values in the remaining variables.</li>
</ul></li>
</ul>
<h4 id="inference-forward-checking">Inference: Forward Checking</h4>
<p>Idea:</p>
<ul>
<li>Keep track of remaining legal values for unassigned variables.</li>
<li>Terminate search when any variable has no legal values.</li>
</ul>
<p><strong>Example:</strong></p>
<p><img src="2D.assets/1550673153836.png" alt="1550673153836" /></p>
<h4 id="constraint-propagation">Constraint Propagation</h4>
<ul>
<li>Forward Checking propagates information from assigned to unassigned variables, but doesn't provide early detection for all failures.
<ul>
<li>For instance: (demonstrating the failure of forward checking) <img src="2D.assets/1550673292094.png" alt="1550673292094" />
<ul>
<li>NT and SA cannot both be blue!</li>
<li>Constraint Propagation repeatedly enforces constraints <em>locally</em>.</li>
</ul></li>
</ul></li>
</ul>
<h5 id="arc-consistency">Arc Consistency</h5>
<ul>
<li><p>Simplest form of propagation makes each arc consistent.</p></li>
<li><p>$X \rightarrow Y​$ is consistent iff</p>
<ul>
<li>for <em>every</em> value of the variable $X$, there is some allowed/legal value $y$ in the domain of variable $Y​$.</li>
<li>(that little arrow $X \rightarrow Y$ is an <em>arc</em> by the way, not implication!)</li>
</ul></li>
<li><p>It's called <strong>Arc</strong> Consistency because you draw an arc from X to Y for visualisation purposes... <img src="2D.assets/1551185172267.png" alt="1551185172267" /></p>
<ul>
<li><strong>Variables:</strong> <em>WA, NT, Q, NSW, V, SA, T</em></li>
<li><strong>Domain:</strong> <em>red, green, blue</em></li>
<li><strong>Arc:</strong> <em>SA -&gt; NSW</em></li>
</ul></li>
<li><p>It is immensely helpful to think Arc Consistency in <strong>Constraint Graphs</strong>:</p>
<ul>
<li>For each node:
<ul>
<li>For every value of the node, there is some some allowed/legal value that its neighbour can take.</li>
</ul></li>
</ul></li>
<li><p>Realise that you do <em>not</em> need to check arc consistency <em>for all</em> nodes/states <em>again and again</em>:</p>
<ul>
<li><strong>You only need to check for states/nodes who lost a value.</strong></li>
<li>It's costly anyway:
<ul>
<li>Time complexity: $O(cd^3)$ where $d$ is the maximum size of each domain and $c$ is the number of binary constraints (arcs).</li>
<li>Space complexity: $O(c)$</li>
</ul></li>
</ul></li>
<li><p>Arc consistency detects failure earlier than forward checking.</p>
<ul>
<li>Indeed, you can think of forward checking as a very limited form of Arc Consistency where only the neighbours of the most recent node/state is checked!</li>
</ul></li>
</ul>
<h2 id="5-february-2019">5 February 2019</h2>
<h3 id="propositional-logic-pl">Propositional Logic (PL)</h3>
<ul>
<li>(+) Propositional Logic (PL) is declarative.</li>
<li>(+) PL allows <em>partial</em>, <em>disjunctive</em>, and <em>negated</em> information.
<ul>
<li><strong>Partial:</strong> Instead of saying "false" (or claiming that "it's not the case"), it allows us to say "we don't know" if we can't conclude an assertion from certain premises.
<ul>
<li>There are also "3-valued logics" (such as Lukasiewicz logic) that has an explicit 3rd value called "I don't know!"</li>
</ul></li>
<li><strong>Disjunctive:</strong> The dominant majority of real-world databases <em>conjunct</em> data, for example: "A person has a name <em>and</em> a date of birth <em>and</em> a gender <em>and</em> ..." whereas we can construct disjunctions in PL (which should be obvious)!
<ul>
<li>Our lovely Haskell of course supports "disjunctive" data types, remember <em>Algebraic Data Types</em>?</li>
</ul></li>
<li><strong>Negated:</strong> Again, the dominant majority of real-world databases record "positive" data: you record the presence of something and again there is often not a way to record something which is not the case. So for instance, while we can record the age of a person, we cannot store the fact that that person is <em>not</em> 21 years old. In contrast, PL allows its "facts" to be negated.</li>
</ul></li>
<li>(+) PL is <em>compositional</em>, that is, the meaning of its sentences can be derived from its elements!</li>
<li>(+) Meaning in PL is context-independent (<em>i.e.</em> unambiguous), unlike natural language where meaning depends on many contexts.</li>
<li>(-) PL has <em>very</em> limited expressive power, unlike natural language.
<ul>
<li>For instance, we cannot say "pits cause breezes in adjacent squares", <strong>except by writing a sentence for <em>each</em> square.</strong></li>
</ul></li>
</ul>
<h3 id="first-order-logic-fol">First-Order Logic (FOL)</h3>
<ul>
<li><p>Whereas PL assumes the world contains <strong>facts</strong>, first-order logic (FOL) --like natural language-- assumes the world contains:</p>
<ul>
<li><strong>Objects:</strong> people, houses, numbers, colours, ...</li>
<li><strong>Relations (predicates):</strong>
<ul>
<li><strong>Unary:</strong> prime, hot, cold, ...</li>
<li><strong>Binary:</strong> "brother of", "bigger than", "part of"</li>
<li><strong>...</strong></li>
</ul></li>
<li><strong>Functions:</strong> "one more than", "plus", "father of"</li>
<li><strong>NOTE WELL:</strong>
<ul>
<li><em>Relations (predicates)</em> are "evaluated" to True or False whereas <em>Functions</em> "return" an object.</li>
<li>Unfortunately both predicates and functions have the same syntax (<em>e.g.</em> function: $\text{FatherOf(Jesus)} = \text{Jesus}$) (<em>e.g.</em> relation: $\text{BiggerThan(Jesus, Temple)} = \bot$), but take care not to confuse them!
<ul>
<li>By John 2:13-17, we know that Jesus can fit inside the temple which also gives us an upper bound on his size.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h4 id="syntax-of-first-order-logic">Syntax of First-Order Logic</h4>
<ul>
<li><strong>Constants (Named Objects):</strong> <em>e.g.</em> KingJohn, 2, UoE, Bora, ...</li>
<li><strong>Predicates (Relations):</strong> <em>e.g.</em> Prime, &gt;, HotterThan, ...</li>
<li><strong>Functions:</strong> <em>e.g.</em> Sqrt, LeftLegOf, ...</li>
<li><strong>Variables:</strong> <em>e.g.</em> x, y, a, b
<ul>
<li>By convention, variables start with lower-case letters (lest they might be confused with constants.)</li>
</ul></li>
<li><strong>Connectives:</strong> <em>e.g.</em> $\lnot, \implies, \land, \lor, \iff,\ \ldots$</li>
<li><strong>Equality:</strong> <em>e.g.</em> $=$
<ul>
<li>Equality compares whether two objects are equal, <em>regardless of their name</em>. So for instance, $\text{Jesus} = \text{Jesus of Nazareth}$.</li>
</ul></li>
<li><strong>Quantifiers:</strong> <em>e.g.</em> $\forall, \exists, \exists!,\ \ldots$</li>
</ul>
<h4 id="atomic-formulas">Atomic Formulas</h4>
<ul>
<li><p>Atomic Formulas are those that do not contain any connectives.</p></li>
<li><p>Either <strong>Atomic Formula:</strong></p>
<ul>
<li>$\text{predicate}(\text{term}_1,\ \ldots,\ \text{term}_n)$</li>
<li>or, $\text{term}_1 = \text{term}_2$</li>
<li><strong>N.B.</strong> these evaluate into <em>True</em> or <em>False</em>!</li>
</ul></li>
<li><p>Or <strong>Term:</strong></p>
<ul>
<li>$\text{function}(\text{term}_1,\ \ldots,\ \text{term}_n)$</li>
<li>or, <em>constant</em></li>
<li>or, <em>variable</em></li>
<li><strong>N.B.</strong> these evaluate into <em>a single Object</em>!</li>
</ul></li>
</ul>
<h4 id="complex-formulas">Complex Formulas</h4>
<ul>
<li>Complex Formulas are made from Atomic Formulas using connectives.</li>
</ul>
<h4 id="semantics-of-first-order-logic">Semantics of First-Order Logic</h4>
<ul>
<li>Formulae are mapped to an interpretation.
<ul>
<li>Interpretations contain objects (<strong>domain elements</strong>) and <em>relations between them</em>.
<ul>
<li><strong>N.B.</strong> "and relations between them." So, an interpretation must specify which relations (between which objects) hold true!</li>
</ul></li>
<li>Mapping specifies referents for
<ul>
<li>Constant Symbols (<em>i.e.</em> names) $\rightarrow$ objects</li>
<li>Predicate Symbols $\rightarrow$ relations</li>
<li>Function Symbols $\rightarrow$ functions</li>
<li>These are all very <em>abstract</em> concepts, almost in the metaphysical sense so don't sweat to much.
<ul>
<li>It's just that we are trying to make a distinction between the name of an object, predicate, or function and the object, predicate, or function itself!</li>
</ul></li>
</ul></li>
</ul></li>
<li>An interpretations is called a "<strong>model</strong> of a set of formulas" when all the formulas in that set are true according to that interpretation.</li>
</ul>
<h4 id="quantifiers">Quantifiers</h4>
<h5 id="universal-quantifier-for-all">Universal Quantifier ("for all")</h5>
<ul>
<li>$\forall &lt;\text{variables}&gt;\ .\ &lt;\text{formula}&gt;​$</li>
<li>$\forall x. P$ is <em>true</em> with respect to an interpretation $m$ if and only if:
<ul>
<li>$P$ is <em>true</em> for all objects in the interpretation m.</li>
</ul></li>
</ul>
<h5 id="existential-quantifier-there-exists">Existential Quantifier ("(there) exists")</h5>
<ul>
<li>$\exists &lt;\text{variables}&gt;\ .\ &lt;\text{formula}&gt;$</li>
<li>$\exists x.P$ is <em>true</em> with respect to an interpretation $m$ if and only if:
<ul>
<li>There exists an object in the interpretation $m$ for which $P$ is true.</li>
</ul></li>
</ul>
<h5 id="duality">Duality</h5>
<ul>
<li>Each quantifier can be expressed using the other:
<ul>
<li>$\forall x. \text{Likes}(x, \text{IceCream}) \equiv \lnot\exists x. \lnot\text{Likes}(x, \text{IceCream})​$</li>
<li>$\exists x. \text{Likes}(x, \text{Broccoli}) \equiv \lnot\forall x. \lnot\text{Likes}(x, \text{Broccoli})$</li>
</ul></li>
</ul>
<h4 id="equality">Equality</h4>
<ul>
<li>$\text{term}_1 = \text{term}_2$ is true under a given interpretation <em>if and only if</em> $\text{term}_1$ and $\text{term}_2$ refer to the same object.</li>
</ul>
<h4 id="substitution">Substitution</h4>
<ul>
<li>Given a sentence $S$ in FOL and a <strong>substitution</strong> $\sigma$, $S\sigma$ denotes the result of "plugging" $\sigma$ into $S$.
<ul>
<li>For example: $$\begin{align*} S &amp;= \text{Smarter}(x, y)\\ \sigma &amp;= {x/\text{Clinton}, y/\text{Trump}}\\ S\sigma &amp;= \text{Smarter}(\text{Clinton}, \text{Trump}) \end{align*}$$</li>
</ul></li>
</ul>
<h4 id="knowledge-bases--fol">Knowledge Bases &amp; FOL</h4>
<ul>
<li><p>This section is about how FOL is utilised in "Knowledge-Based Agents."</p></li>
<li><p><code>Ask(KB, Query)</code> returns some or all $\sigma$ such that $\text{KB} \vDash S\sigma$.</p>
<ul>
<li>So our query (in the wumpus world) would look like: $\text{Ask}(\text{KB}, \exists a. \text{BestAction}(a, \text{CurrentTime}))$</li>
</ul></li>
<li><p>The implications in the KB can be classified into two categories:</p>
<ul>
<li><strong>Perception:</strong> <em>e.g.</em> $\forall t,s,b. \text{Percept}([s, b, \text{Glitter}], t) \implies \text{Glitter}(t)$
<ul>
<li>$t$ for time, $s$ for smell, $b$ for breeze.</li>
<li>Perception implications are responsible for updating our view of the world.</li>
</ul></li>
<li><strong>Reflex:</strong> <em>e.g.</em> $\forall t. \text{Glitter(t)} \implies \text{BestAction(Grab, t)}$
<ul>
<li>Reflex implications are responsible for helping us decide what is the best action to take <em>given an up-to-date view of the world</em>.</li>
</ul></li>
</ul></li>
<li><p>The implications can be categorised under one of the following "rules:"</p>
<ul>
<li><strong>Diagnostic Rule:</strong> inferring cause from effect <em>e.g.</em> $\forall s. \text{Breezy}(s) \implies \exists r. \text{Adjacent}(r, s) \land \text{Pit}(r)$</li>
<li><strong>Casual Rule:</strong> inferring effect from cause <em>e.g.</em> $\forall r. \text{Pit}(r) \implies [\forall s. \text{Adjacent}(r, s) \implies \text{Breezy}(s)]$</li>
</ul></li>
</ul>
<h2 id="7-february-2019">7 February 2019</h2>
<h3 id="universal-instantiation-ui">Universal Instantiation (UI)</h3>
<ul>
<li><p><em>Every</em> instantiation of a universally quantified formula $\alpha$ is entailed by it (the universally quantified formula).</p></li>
<li><p>$ \dfrac{\forall v. \alpha} {\alpha {v / g}} $</p>
<ul>
<li>read as "$\forall v. \alpha$ entails that $\alpha$ must be true when $v$ is substituted by $g$."</li>
<li>...is true for <em>any</em> variable $v$ and <strong>ground term</strong> $g$.
<ul>
<li><em>Any</em>, because it's <em>universal</em> quantifier!</li>
<li>A <strong>ground term</strong>, which we substitute our variable by, contains no variables.
<ul>
<li>Usually the ground term itself is an object in your domain, or something that evaluates into an object like a function call.</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Also called "Universal Elimination."</p></li>
</ul>
<h3 id="existential-instantiation-ei">Existential Instantiation (EI)</h3>
<ul>
<li><em>Some</em> instantiation of an existentially quantified formula $\alpha$ is entailed by it (the existentially quantified formula).</li>
<li>$\dfrac{\exists v. \alpha}{\alpha {v/k}}$
<ul>
<li>...is true for <em>some</em> <strong>constant symbol</strong> $k​$.
<ul>
<li><em>Some</em>, because it's <em>existential</em> quantifier!</li>
<li>We use a **constant symbol **because $k​$ must NOT appear elsewhere in the knowledge base. This restriction is to prevent the name conflicts:
<ul>
<li>If you have $\exists x. \text{Round}(x)$ and $\exists x. \text{Red}(x)$ and if you substitute $x$ with the same constant symbol $c$, you'll get: $ \dfrac{\exists x. \text{Round}(x)\quad\exists x. \text{Red}(x)}{\text{Round}(c)\quad\text{Red}(c)} $ which, wrongly, entails that there is something that is both <em>round</em> and <em>red</em> which cannot be concluded from our premise!</li>
<li>Universal Instantiation does not suffer from this of course, since a universally quantified sentence must hold true <em>for all</em> objects. =)</li>
</ul></li>
<li>The new constant symbol is called a <strong>Skolem constant</strong>.</li>
</ul></li>
</ul></li>
<li>Also called "Existential Elimination."</li>
</ul>
<h3 id="reduction-to-pl-by-propositionalisation">Reduction to PL by Propositionalisation</h3>
<ul>
<li>Suppose a given KB is as following:
<ul>
<li><strong>Sentences:</strong>
<ul>
<li>$\forall x. \text{King}(x) \land \text{Greedy}(x) \implies \text{Evil}(x)​$</li>
<li>$\text{King}(\text{John})$</li>
<li>$\text{Greedy}(\text{John})$</li>
<li>$\text{Brother}(\text{Richard, John})$</li>
</ul></li>
<li><strong>Named Objects:</strong>
<ul>
<li>John</li>
<li>Richard</li>
</ul></li>
</ul></li>
<li>Instantiating the universal sentence in all possible ways results in the following new sentences:
<ul>
<li>$\text{King}(\text{John}) \land \text{Greedy}(\text{John}) \implies \text{Evil}(\text{John})$
<ul>
<li>${x / \text{John}}​$</li>
</ul></li>
<li>$\text{King}(\text{Richard}) \land \text{Greedy}(\text{Richard}) \implies \text{Evil}(\text{Richard})$
<ul>
<li>${x/\text{Richard}}$</li>
</ul></li>
<li>The original universal sentence can be discarded now.</li>
</ul></li>
<li>Resulting KB (with new propositional sentences and previous sentences) is considered <strong>propositionalised</strong>, as there are no "unbound variables."
<ul>
<li>Proposition symbols are:
<ul>
<li>Which were previously in the KB as a fact:
<ul>
<li>$\text{King}(\text{John})$</li>
<li>$\text{Greedy}(\text{John})$</li>
</ul></li>
<li>Which are not in KB as a standalone fact:
<ul>
<li>$\text{Evil}(\text{John})$</li>
<li>$\text{King}(\text{Richard})$</li>
<li>$\text{Greedy}(\text{Richard})$</li>
<li>$\text{Evil}(\text{Richard})$</li>
</ul></li>
</ul></li>
</ul></li>
<li>Every FOL KB can be propositionalised so as to <em>preserve</em> entailment.
<ul>
<li>Meaning that a ground sentence is entailed by the new KB <strong>iff</strong> entailed by the original KB.</li>
</ul></li>
<li>We can then propositionalise the KB and the query, apply DPLL (or some other complete propositional method), and return the result!
<ul>
<li>The problem is that with function symbols there are <strong>infinitely many</strong> ground terms!
<ul>
<li>$\text{Father}(\text{Father}(\text{Father}(\ldots(\text{John}))))$</li>
</ul></li>
<li>Unfortunately propositionalisation seems to generate lots o irrelevant sentences.
<ul>
<li>With $p$ k-ary predicates and $n$ constants/objects there are $p \cdot n^k$ instantiations.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="theorems-regarding-propositionalisation">Theorems Regarding Propositionalisation</h4>
<h5 id="herbrand-1930">Herbrand (1930)</h5>
<ul>
<li>If a sentence $\alpha$ is entailed by a FOL KB, it must be entailed by a finite subset of the propositionalised KB.</li>
<li>The idea is for $n = 0$ to $\infin$, create a propositionalised KB by instantiating with depth-$n$ terms and see if $\alpha$ is entailed by this KB.</li>
<li>The problem is that this returns an answer only if $\alpha$ is entailed, loops forever otherwise!</li>
</ul>
<h5 id="turing-1936-church-1936">Turing (1936), Church (1936)</h5>
<ul>
<li>Entailment for FOL is <strong>semi-decidable</strong>:
<ul>
<li>Algorithms exists that say yes to every entailed sentence but...</li>
<li>No algorithms exists that also says no to every non-entailed sentence.</li>
</ul></li>
</ul>
<h3 id="unification">Unification</h3>
<ul>
<li>Unifying two sentences is to find a <em>substitution</em> $\theta$ such that the sentences become identical.
<ul>
<li>Symbolically, this substitution is produced by the <em>Unify</em> operator: $\text{Unify}(\alpha, \beta) = \theta$ <strong>iff</strong> $\alpha\theta \equiv \beta\theta$
<ul>
<li>Here, $\theta$ is the "<strong>unifier</strong> of $\alpha$ and $\beta$.
<ul>
<li>Yes, it's called unifier. Like an operator.</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>For example:</strong> <img src="2D.assets/1551561333412.png" alt="1551561333412" />
<ul>
<li>It should be clear that we can try to unify a given query with the knowledge base and the unifier $\theta$ would be the answer to our query (<em>i.e.</em> the substitution that satisfies it).</li>
</ul></li>
</ul>
<h4 id="standardising-variables-apart">Standardising Variables Apart</h4>
<ul>
<li>$\text{Knows}(\text{John}, x)​$ and $\text{Knows}(x, \text{Richard})​$ cannot be unified if the two $x​$'s are referring to the same variable (<em>i.e.</em> $x​$ cannot be substituted with <em>John</em> and <em>Richard</em> at the same time!)</li>
<li>If the two $x$ are unrelated and just an (inconsiderate) naming of an unknown variable, then we can <strong>rename</strong> one of them and thus avoid the clash of variables.
<ul>
<li>Guess what this is called.</li>
</ul></li>
<li>For example, if we change $\text{Knows}(\text{John}, x)​$ to $\text{Knows}(\text{John}, z)​$, we can unify it with $\text{Knows}(x, \text{Richard})​$:
<ul>
<li>$\theta = {x/\text{John},\ z/\text{Richard}}$
<ul>
<li><em>I.e.</em> "John knows Richard."</li>
</ul></li>
</ul></li>
</ul>
<h4 id="most-general-unifier">Most General Unifier</h4>
<ul>
<li>There might be more than one unification:
<ul>
<li>$\text{Knows}(\text{John}, x)$ and $\text{Knows}(y, z)$ can be unified as:
<ul>
<li>$\theta = {y/\text{John},\ x/z}​$</li>
<li>$\theta = {y/\text{John},\ x/\text{John},\ z/\text{John}}$</li>
<li><strong>Realise that both unifications are just as equally valid.</strong></li>
<li><strong>But the first unifier is <em>more general</em> than the second.</strong></li>
</ul></li>
</ul></li>
<li><strong>There is a single <em>most general unifier</em> (MGU) that is unique</strong> (irrespective of the variable names of course.)</li>
</ul>
<h5 id="computing-the-mgu">Computing the MGU</h5>
<ul>
<li>Can be broken-down into a series of steps:
<ol>
<li>Decomposition</li>
<li>Conflict</li>
<li>Eliminate</li>
<li>Delete</li>
<li>Switch</li>
<li>Coalesce</li>
<li>Occurs Check</li>
</ol></li>
<li>TODO: How does MGU work with respect to negations?</li>
</ul>
<ul>
<li><strong>Decomposition:</strong>
<ul>
<li>Given $\text{Knows}(\text{John}, x) \equiv \text{Knows}(y, z)$
<ul>
<li>Replace $\text{John} \equiv y$ and $x \equiv z$.</li>
</ul></li>
<li>In general, given $ f(s_1, \ldots, s_n) \equiv f(t_1, \ldots, t_n) $
<ul>
<li>Replace with $s_1 \equiv t_1, \ldots, s_n \equiv t_n$.</li>
</ul></li>
</ul></li>
<li><strong>Conflict:</strong>
<ul>
<li>Given $\text{Knows}(\text{John}, x) \equiv \text{Greedy}(y)$
<ul>
<li>Fail!</li>
</ul></li>
<li>In general, given $f(s_1, \ldots, s_m) \equiv g(t_1, \ldots, t_n),\quad \text{where } f \ne g$
<ul>
<li>Fail!</li>
</ul></li>
</ul></li>
<li><strong>Eliminate:</strong>
<ul>
<li>Given $\text{Knows}(\text{John}, x) \equiv \text{Knows}(y, z)$ and $z \equiv \text{Richard}$
<ul>
<li>Replace with $\text{Knows}(\text{John}, x) \equiv \text{Knows}(y, \text{Richard})$ and $z \equiv \text{Richard}$.</li>
</ul></li>
<li>In general, given $P$ and $x \equiv t$ where $x$ occurs in $P$ †
<ul>
<li>Replace with $P{x/t}$ and $x \equiv t$.
<ul>
<li>$P{x/t}$ denotes the result of substituting $x$ by $t$ in $P$.</li>
</ul></li>
<li><strong>†:</strong> As common sense dictates, neither $x$ should occur in $t$, nor $t$ should be another variable.</li>
</ul></li>
</ul></li>
<li><strong>Delete:</strong>
<ul>
<li>Given $\text{Greedy}(\text{John}) \equiv \text{Greedy}(\text{John})$
<ul>
<li>Remove this equation.</li>
</ul></li>
<li>In general, given $P$ and $s \equiv s$
<ul>
<li>Replace with $P$.</li>
</ul></li>
</ul></li>
<li><strong>Switch:</strong>
<ul>
<li>Given $\text{Knows}(\text{John}, x) \equiv \text{Knows}(y, z)$ and $\text{Richard} \equiv z$
<ul>
<li>Replace with $\text{Knows}(\text{John}, x) \equiv \text{Knows}(y, z)$ and $z \equiv \text{Richard}$.</li>
</ul></li>
<li>In general, given $P$ and $s \equiv x$, where $x$ (but not $s$) is a variable
<ul>
<li>Replace with $P$ and $x \equiv s$.</li>
</ul></li>
</ul></li>
<li><strong>Coalesce:</strong>
<ul>
<li>Given $\text{Knows}(\text{John}, x) \equiv \text{Knows}(y, z)$ and $y \equiv z$
<ul>
<li>Replace with $\text{Knows}(\text{John}, x) \equiv \text{Knows}(z, z)$ and $y \equiv z$.</li>
</ul></li>
<li>In general, given $P$ and $x \equiv y$, where $x$ and $y$ are variables occurring in $P$
<ul>
<li>Replace with $P{x/y}$ and $x \equiv y$.</li>
</ul></li>
</ul></li>
<li><strong>Occurs Check:</strong>
<ul>
<li>Given $x \equiv \text{Father}(x)$
<ul>
<li>Fail!</li>
</ul></li>
<li>In general, given $x \equiv s$, where $s$ is a function and $x$ occurs in $s$
<ul>
<li>Fail!</li>
</ul></li>
</ul></li>
</ul>
<h3 id="generalised-modus-ponens-gmp">Generalised Modus Ponens (GMP)</h3>
<p>$$\begin{align*} \dfrac{p'_1, p'_2, \ldots, p'_n\ (p_1 \land p_2 \land \ldots \land p_n \implies q)}{q\theta}\quad \text{when for all $i$, }\ p'_i\theta \equiv p_i\theta \end{align*}​$$</p>
<ul>
<li>Generalised Modus Ponens (GMP) is used with KB of <strong>definite clauses</strong>.
<ul>
<li>A <em>clause</em> is a disjunction of literals.</li>
<li>A <em>definite clause</em> (also called <em>Horn clause</em>) is a disjunction of literals with <strong>at most one positive literal</strong>.
<ul>
<li>So you have something like $\lnot L_1 \lor \lnot L_2 \lor \ldots \lor \lnot L_n \lor L$</li>
<li>...which is equivalent to $L_1 \land L_2 \land \ldots \land L_n \implies L​$</li>
<li>Thus $(p_1 \land p_2 \land \ldots \land p_n \implies q)​$ up there is the definite clause.</li>
</ul></li>
</ul></li>
<li><strong>All variables are assumed to be universally quantified.</strong></li>
<li>Since GMP works only with definite clauses:
<ul>
<li>TODO: confirm these!</li>
<li>If an implication contains negated literals such as $P \land \lnot Q \implies R$ rename it as $P \land Q' \implies R$ and try applying GMP on it.</li>
<li>If an implication contains a disjunction such as $\dfrac{P, Q_1,\ P \land (Q_1 \lor Q_2) \implies R}{?}$ <em>"expand"</em> the disjunction and apply GMP on each expansion $\dfrac{P, Q_2,\ P \land Q_1 \implies R}{\text{Fail!}}$ and $\dfrac{P, Q_2,\ P \land Q_2 \implies R}{R\theta}$.
<ul>
<li>Each unifier is a possible solution!</li>
<li>Also realise that for $n$ disjunctions with $k$ literals, there are $k^n$ expansions in total so not computationally feasible for non-trivial cases!</li>
</ul></li>
</ul></li>
<li>GMP is <em>sound</em>!</li>
</ul>
<h4 id="example-2">Example</h4>
<p>$$\begin{align*} p'_1&amp;: \text{King}(\text{John})\\ p'_2&amp;: \text{Greedy}(y)\\ p_1&amp;: \text{King}(x)\\ p_2&amp;: \text{Greedy}(x)\\ q&amp;: \text{Evil}(x)\\ \end{align*}​$$</p>
<p>then it follows that</p>
<p>$$\begin{align*} \theta&amp;: {x/\text{John},\ y/\text{John}}\\ q\theta&amp;: \text{Evil}(\text{John}) \end{align*}$$</p>
<p>Alternatively:</p>
<p>$$\dfrac{\text{King}(\text{John}), \text{Greedy}(y),\ (\text{King}(x), \text{Greedy}(x) \implies \text{Evil}(x))}{\text{Evil}(x){x/\text{John},\ y/\text{John}}}​$$</p>
<h2 id="8-february-2019">8 February 2019</h2>
<h3 id="chaining">Chaining</h3>
<ul>
<li>Chaining is the process of concluding new facts from a given set of sentences, using a <em>chain</em> of proofs.</li>
<li>Combination of forward and backward chaining is known as <strong>opportunistic reasoning</strong>.</li>
<li>The two critical questions in choosing the chaining strategy are:
<ul>
<li>What do users expect from the system?</li>
<li>Which direction has the larger <em>branching factor</em>?</li>
<li>The questions will became clearer once you read more about the chaining strategies.</li>
</ul></li>
</ul>
<h4 id="forward-chaining">Forward Chaining</h4>
<ul>
<li>In forward chaining, we (try) building a chain of proofs <strong>starting from our knowledge base</strong> to the sentence we are trying to prove.
<ul>
<li>It is also useful if there are no specific Goals, but the system needs to <em>react</em> to new facts (data driven).</li>
<li>Can make <em>suggestions</em>, for instance.</li>
</ul></li>
<li>Crudely said, consists of pattern matching &amp; unification.</li>
<li><img src="./2D.assets/1554203979512.png" alt="1554203979512" />
<ul>
<li>In the screenshot, the chain is built bottom-up.</li>
<li>The last proof of the chain uses the sentence marked in red.</li>
<li>The edges who belong to the same proof are connected (like angle markers).</li>
</ul></li>
<li>Forward chaining is widely used in <strong>deductive databases</strong>.</li>
</ul>
<h5 id="properties-1">Properties</h5>
<ul>
<li>Forward Chaining is <em>sound</em> and <em>complete</em> for first-order <strong>definite</strong> clauses
<ul>
<li>See "Generalised Modus Ponens (GMP)"" section for definition of <em>definite</em> clause.</li>
</ul></li>
<li>May <strong>not</strong> terminate if $\alpha$ is <em>not</em> entailed.
<ul>
<li>This is unavoidable: entailment with definite clauses is proven to be semi-decidable.</li>
</ul></li>
<li>Datalog is a declarative logic programming language that uses first-order definite clauses with <em>no functions allowed</em>.
<ul>
<li>Thus Forward Chaining terminates in finite number of iterations.</li>
</ul></li>
</ul>
<h5 id="efficiency">Efficiency</h5>
<ul>
<li><strong>Incremental Forward Chaining:</strong> no need to (try) pattern-match a rule on current iteration $k$ if a premise was not added on the previous iteration $k-1$.
<ul>
<li>In other words, match each rule whose premise contains at least one newly added positive literal in the previous iteration.</li>
</ul></li>
<li>Pattern matching itself can be expensive, but luckily <em>database indexing</em> allows $O(1)$ retrieval of known facts.
<ul>
<li><em>E.g.</em> query $\text{Missile}(x)$ retrieves $\text{Missile}(M1)$.</li>
</ul></li>
<li>Finding all possible unifiers can be <strong>really expensive</strong>.
<ul>
<li>For example:
<ul>
<li>$\text{Missile}(x) \land \text{Owns}(\text{Nono}, x) \rightarrow \text{Sells}(\text{West}, x, \text{Nono})$</li>
<li>We can find each object owned by Nono in constant time and then check if it is a missile.</li>
<li>But what if Nono owns many objects but very few missiles?
<ul>
<li>It would then be wiser to find each missile in constant time and then check if Nono owns them!</li>
<li>This is called <strong>Conjunct Ordering</strong>.</li>
<li>Optimal ordering is NP-hard, but luckily heuristics are available.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h4 id="backward-chaining">Backward Chaining</h4>
<ul>
<li>In backward chaining, we (try) building a chain of proofs <strong>starting from the goal/query</strong> to our knowledge base.</li>
<li>Crudely said, it consists of pattern-matching and unification (again).
<ul>
<li>Pattern matching used to fetch rules that might unify with the <strong>goal</strong>.
<ul>
<li>Goal is a very important concept here, understand it well!</li>
</ul></li>
</ul></li>
<li><img src="./2D.assets/1554206460451.png" alt="1554206460451" />
<ul>
<li>In the screenshot, the tree is built top-down.</li>
<li><strong>At each level/height, each node is the goal of the sub-tree underneath.</strong></li>
<li>Backward Chaining is stopped once we reach the <strong>ground facts</strong> in our knowledge base.</li>
</ul></li>
</ul>
<h5 id="properties-2">Properties</h5>
<ul>
<li><strong>Depth-first</strong> recursive proof search: space required is linear in size of proof.</li>
<li><strong>Incomplete due to infinite loops!</strong>
<ul>
<li>Can be mitigated <em>partially</em> by checking the current goal against the other goals on the path from the root of the tree to itself.</li>
</ul></li>
<li><strong>Inefficient</strong> due to repeated subgoals (whether the result is a success or a failure)
<ul>
<li>Can be fixed by caching of previous results (at the cost of increased space usage).</li>
</ul></li>
<li>Widely used in <strong>logic programming</strong>.</li>
</ul>
<h4 id="chaining-and-constraint-satisfaction-problems">Chaining and Constraint Satisfaction Problems</h4>
<ul>
<li><img src="./2D.assets/1554205945312.png" alt="1554205945312" />
<ul>
<li>Definite clause is above (Diff and Diff and ... Diff implies Colourable).</li>
<li>Ground facts are below (Diff, Diff, ...).</li>
</ul></li>
</ul>
<h3 id="resolution">Resolution</h3>
<ul>
<li>You'd be grateful for Michael Fourman. =)</li>
<li>A method for telling
<ul>
<li>whether a <strong>propositional</strong> formula is <em>satisfiable</em>,</li>
<li>and for proving that a <strong>first-order</strong> formula is <em>unsatisfiable</em>.
<ul>
<li>To prove $\alpha​$, apply resolution to $\text{CNF}(\text{KB} \land \lnot\alpha)​$...</li>
<li>...empty clause means $\alpha$ is entailed by KB.</li>
<li>...non-empty clauses yield counter-examples (<em>i.e.</em> unifications where KB is true but $\alpha$ is false or vice versa).</li>
</ul></li>
</ul></li>
<li>Resolution is <strong>sound</strong> and <strong>complete</strong>.</li>
<li>If a set of clauses is unsatisfiable, then once can derive an empty clause from this set.</li>
</ul>
<h4 id="ground-binary-resolution">Ground Binary Resolution</h4>
<p>$$\begin{align}\frac{C \lor P \quad D \lor \lnot P}{C \lor D}\end{align}​$$</p>
<ul>
<li>Take two (hence <em>binary</em>) <em>ground</em> clauses (<em>i.e.</em> ground meaning no unbound/free variables), containing the same variable/proposition with different polarity.</li>
<li>Combine them!</li>
<li><strong>N.B.</strong> If both $C$ and $D$ are empty then the resolution deduces the <strong>empty clause</strong>, <em>i.e.</em> false.</li>
<li><strong>Proof of Soundness:</strong>
<ul>
<li>$C \lor P \iff \lnot C \implies P$</li>
<li>$D \lor \lnot P \iff P \implies D$</li>
<li>Thus $$\lnot C \implies D$$ by chaining, and the result is equivalent to $$C \lor D$$.</li>
</ul></li>
</ul>
<h4 id="non-ground-binary-resolution">Non-Ground Binary Resolution</h4>
<p>$$\begin{align}\frac{C \lor P \quad D \lor \lnot P'}{(C \lor D)\theta}\end{align}$$</p>
<p>where $\theta$ is the MGU (most general unifier) of $P$ and $P'$.</p>
<ul>
<li>The two clauses are assumed to be <em>standardised apart</em> (<em>i.e.</em> all variables uniquely renamed) so that they share no variables.</li>
<li><strong>Example:</strong>
<ul>
<li>$$\begin{align}\frac{\lnot\text{Rich}(x) \lor \text{Unhappy}(x) \quad \text{Rich}(\text{Ken})}{\text{Unhappy}(\text{Ken})}\end{align}$$</li>
<li>with $\theta = {x / \text{Ken}}$</li>
</ul></li>
<li><strong>Proof of Soundness:</strong>
<ul>
<li>Apply unifier $\theta$ to the premises and then appeal to ground binary resolution:</li>
<li>$$\begin{align}\frac{C\theta \lor P\theta \quad D\theta\lor\lnot P\theta}{C\theta \lor D\theta}\end{align}$$</li>
</ul></li>
</ul>
<h4 id="factoring">Factoring</h4>
<p>Okay this one is <em>hairy</em>...</p>
<ul>
<li>If two or more positive/negative literals in clause $S$ are unifiable and $\theta$ is their most general unifier, then $S\theta$ is called a factor of S.</li>
<li>In such a factorisation, duplicates are removed.</li>
<li>Source: <a href="http://mathworld.wolfram.com/ResolutionPrinciple.html">http://mathworld.wolfram.com/ResolutionPrinciple.html</a></li>
</ul>
<p><strong>Example 1</strong></p>
<p>$$\begin{align} S&amp;:\quad P(x) \lor \lnot Q(f(x), b) \lor P(g(y))\\ S\theta&amp;:\quad P(g(y)) \lor \lnot Q(f(g(y)), b) \end{align}$$</p>
<p>for $\theta = {x/g(y)}$.</p>
<p>Now for resolution purposes:</p>
<ul>
<li>Consider $C \lor P_1 \lor \cdots \lor P_m​$</li>
<li>Assume there exists a most general unifier (MGU) $\theta$ for $Pi$.</li>
<li>Let $(C \lor P_1)\theta$ be a factor of $C \lor P_1 \lor \cdots \lor P_m$ <strong>after duplicates are removed</strong>.</li>
<li>Then we can conclude that if $C \lor P_1 \lor \cdots \lor P_m$ was true, $(C \lor P_1)\theta$ must be true too. $$\dfrac{C \lor P_1 \lor \cdots \lor P_m}{(C \lor P_1)\theta}$$</li>
</ul>
<h4 id="full-resolution">Full Resolution</h4>
<p>$$\begin{align}\frac{C \lor P_1 \lor \cdots \lor P_m \qquad D \lor \lnot P'_1 \lor \cdots \lor \lnot P'_n }{(C \lor D)\theta}\end{align}​$$</p>
<p>where $\theta$ is MGU of all $P_i$ and $P'_j$.</p>
<ul>
<li><strong>Proof of Soundness</strong> is omitted, but consists of a combination of <strong>factoring</strong> and <strong>binary resolution</strong>.</li>
<li>TODO: I don't get full resolution really...</li>
</ul>
<h4 id="conversion-to-cnf">Conversion to CNF</h4>
<ol>
<li>Eliminate all biconditionals (if-and-only-if's) and implications.</li>
<li>Push $\lnot$ inwards all the way.</li>
<li>Standardise variables apart; <em>each quantifier should use a different one</em>.</li>
<li><strong>Skolemise:</strong> a more general form of <em>existential instantiation</em>
<ul>
<li><strong>Each</strong> existential variable is replaced by <strong>a</strong> Skolem function of the <strong>enclosing</strong> universally quantified variables.
<ul>
<li>$$\forall x.[\exists y.\text{Animal}(y) \land \lnot\text{Loves}(x,y)] \lor [\exists z.\text{Loves}(z, x)]$$ becomes $$\forall x.[\text{Animal}(F(x)) \land \lnot\text{Loves}(x,F(x))] \lor [\text{Loves}(G(x), x)]$$</li>
<li>Realise that $F()$ is used for $\exists y$ and $G()$ for $\exists z$.</li>
<li>Realise also that both functions take the enclosing universally quantified variable as their arguments.</li>
<li>If there are no enclosing universal quantifier, just replace with <strong>Skolem constant</strong> <em>i.e.</em> a function with no arguments.</li>
</ul></li>
</ul></li>
<li>Drop universal quantifiers.</li>
<li>Distribute $\lor$ over $\land$.</li>
</ol>
<h2 id="12-february-2019">12 February 2019</h2>
<h3 id="limitations-of-generalised-modus-ponens">Limitations of Generalised Modus Ponens</h3>
<p>Due to restriction to <em>definite</em> clauses, in order to (be able to) apply GMP:</p>
<ul>
<li>Premises of all rules contain only non-negated symbols.
<ul>
<li>Possible solution: introduce more variables <em>e.g.</em> $Q \equiv \lnot P$</li>
</ul></li>
<li>Conclusions of all rules is <strong>a (single)</strong> non-negated symbol.
<ul>
<li>We can again introduce more variables <em>e.g.</em> $X \equiv (A \land B) \lor C$</li>
</ul></li>
<li>Facts are non-negated propositions.
<ul>
<li>Think: can we <em>always</em> say that "if we cannot prove $A$, then $\lnot A$ is true"?</li>
<li><em>Only if there is a rule for each variable.</em></li>
</ul></li>
</ul>
<h3 id="resolution--modus-ponens">Resolution &amp; Modus Ponens</h3>
<h4 id="binary-resolution--modus-ponens">Binary Resolution &amp; Modus Ponens</h4>
<p>Consider</p>
<p>$$\dfrac{C \lor P \quad D \lor \lnot P}{C\lor D}$$</p>
<p>and suppose $C$ is False</p>
<p>$\dfrac{P \quad D \lor \lnot P}{D}$</p>
<p><em>i.e.</em> $P$ and $P \implies D$ entails $D$.</p>
<h4 id="resolution-in-implication-form">Resolution in Implication Form</h4>
<p>Consider</p>
<p>$\dfrac{C \lor P \quad D \lor \lnot P}{C\lor D}$</p>
<p>and set $C \equiv \lnot A$</p>
<p>$\dfrac{A \implies P \quad P \implies D}{A \implies D}$</p>
<p>where the conclusion is equivalent to $\lnot A \lor D \equiv C \lor D$ .</p>
<h4 id="full-resolution-and-generalised-modus-ponens">Full Resolution and Generalised Modus Ponens</h4>
<p>TODO: I don't get this!</p>
<p><img src="2D.assets/1554220356463.png" alt="1554220356463" /></p>
<h3 id="factors-resolvents-and-the-lifting-lemma">Factors, Resolvents, and the Lifting Lemma</h3>
<h4 id="factors">Factors</h4>
<p><em>Described again to refresh the memory.</em></p>
<ul>
<li>If two or more positive/negative literals in clause $S​$ are unifiable and $\theta​$ is their most general unifier, then $S\theta​$ is called a factor of S.</li>
<li>In such a factorisation, duplicates are removed.</li>
<li>Source: <a href="http://mathworld.wolfram.com/ResolutionPrinciple.html">http://mathworld.wolfram.com/ResolutionPrinciple.html</a></li>
</ul>
<p><strong>Example 1</strong></p>
<p>$$\begin{align} S&amp;:\quad P(x) \lor \lnot Q(f(x), b) \lor P(g(y))\\ S\theta&amp;:\quad P(g(y)) \lor \lnot Q(f(g(y)), b) \end{align}$$</p>
<p>for $\theta = {x/g(y)}​$.</p>
<h4 id="resolvents">Resolvents</h4>
<ul>
<li>The result of a (often <em>non-ground</em>) resolution operation/step.</li>
</ul>
<p><strong>Example 2</strong></p>
<p>$$\begin{align} C_1&amp;:\quad P(x) \lor P(f(y)) \lor R(g(y))\\ C_2&amp;:\quad \lnot P(f(g(a))) \lor Q(b)\\ \end{align}$$</p>
<p>now let us calculate a factor of $C_1​$ where $\theta = {x/f(y)}​$</p>
<p>$$C'_1: P(f(y)) \lor R(g(y))​$$</p>
<p>now let us calculate a <em>resolvent</em> $C$ of $C'_1$ and $C_2$ where $\theta = {y/g(a)}$</p>
<p>$C = R(g(g(a))) \lor Q(b)​$</p>
<h4 id="the-lifting-lemma">The Lifting Lemma</h4>
<p><img src="2D.assets/1554227017275.png" alt="1554227017275" /></p>
<ul>
<li>The lifting lemma asserts that $C​$ in Example 2 is a resolvent of $C\theta_1​$ and $C_2​$ <strong>and thus</strong> a resolvent of $C_1​$ and $C_2​$.</li>
<li>More formally:
<ul>
<li>If
<ul>
<li>$C_1$ and $C_2$ are two clauses with no shared variable names</li>
<li>and $C'_1$ and $C'_2$ are their respective instances</li>
<li>and $C'$ is a resolvent of $C'_1$ and $C'_2$</li>
</ul></li>
<li>then there exist a clause $C​$ such that both
<ul>
<li>$C$ is a resolvent of $C_1$ and $C_2$</li>
<li>and $C'$ is an instance of $C$.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="efficient-algorithms-for-resolutions">Efficient Algorithms for Resolutions</h3>
<ul>
<li>Lecture slides aren't that great for this section so I used <a href="http://www.cogsci.rpi.edu/~heuveb/teaching/Logic/CompLogic/Web/Presentations/ResolutionDavisPutnam.pdf">http://www.cogsci.rpi.edu/~heuveb/teaching/Logic/CompLogic/Web/Presentations/ResolutionDavisPutnam.pdf</a> instead.</li>
<li>Heuristics folks!
<ul>
<li><strong><em>Unit</em> Preference</strong> Prefer clauses with only one symbol.</li>
<li><strong><em>Pure</em> Clause Elimination</strong> Eliminate pure clauses.
<ul>
<li>A <strong>literal</strong> $L$ is pure with regard to a set of clauses <em>iff</em> $\lnot L$ does not occur in any clause.</li>
<li>A <strong>clause</strong> is pure with regard to a set of clauses <em>iff</em> the clause contains a pure literal.</li>
<li>Any set of clauses is satisfiable <em>iff</em> the set of clauses after pure clauses are removed is satisfiable.
<ul>
<li>The idea is that pure literals are dead easy to satisfy: for each pure literal $L$, assume that it's true!</li>
</ul></li>
</ul></li>
<li><strong>Tautology</strong> You can simply eliminate all clauses containing a symbol together with its negation.</li>
<li><strong>Input Resolution</strong> Intermediately generated clauses can only be combined with original input clauses.
<ul>
<li>I couldn't find a better explanation for this nor it's immediate to me why this would be a good strategy. Take it with a grain of salt.</li>
</ul></li>
<li><strong>Subsumption</strong> If a clause "contains" another one, use only the shorter clause.
<ul>
<li>Obviously, if $C_1​$ (<em>e.g.</em> ${A, B}​$) subsumes $C_2​$ (<em>e.g.</em> ${A, B, \lnot C}​$), then any truth-value assignment that satisfies $C_1​$ will satisfy $C_2​$.</li>
<li>Makes life easier.</li>
</ul></li>
</ul></li>
</ul>
<h2 id="14-february-2019">14 February 2019</h2>
<h3 id="using-logic-to-plan">Using Logic to Plan</h3>
<ul>
<li><p>We have talked about</p>
<ul>
<li>representing the world</li>
<li>representing the goals</li>
</ul>
<p>but not as much about</p>
<ul>
<li>representing how actions <em>change</em> the world
<ul>
<li>After an actions, new things are true, and <em>only some</em> previously true facts are no longer true.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="situation">Situation</h4>
<ul>
<li>Situations extend the concept of a state by <em>time</em>.
<ul>
<li>Time is not the right word, but gives a good intuition. <em>Chronological ordering</em> might be a better choice.</li>
</ul></li>
<li>The world is modelled as having an <strong>initial situation</strong> (usually called $S_0$) and all situations generated by applying an action <em>in</em> a situation.</li>
<li>Can be applied by two methods:
<ul>
<li><strong>Extending Every Predicate</strong> $\text{On}(A, B)$ becomes $\text{On}(A, B, S_0)$</li>
<li><strong>Using <em>Holds</em> Predicate</strong> $\text{Holds}(\text{On}(A, B), S_0)$ denotes that $\text{On}(A, B)$ hold in situation $S_0$
<ul>
<li>Think what $\text{On}(A, B)$ means now. Hint: a category/set of situations in which $A$ is on $B$.</li>
</ul></li>
</ul></li>
<li>Actions are thus
<ul>
<li>performed in a situation</li>
<li>and produce new situations with new facts</li>
</ul></li>
</ul>
<h4 id="actions">Actions</h4>
<ul>
<li>Each action has
<ul>
<li><strong>preconditions</strong> that need to be in place to perform the action</li>
<li><strong>results</strong> of doing the action</li>
</ul></li>
<li>Functions can be used to <em>abbreviate</em> actions.
<ul>
<li><em>E.g.</em> $\text{Move}(A, B)$ denotes the <strong>action type</strong> of moving $A$ onto $B$.</li>
<li><strong>Action type</strong> is the description of a function, and <strong>action</strong> is an instance of an <em>action type</em> applied in specific situation with specific arguments.
<ul>
<li>Like <em>function</em> itself and <em>function call</em> in programming.</li>
</ul></li>
</ul></li>
<li>Let us introduce an actual function $\text{Result}$, designating "the situation resulting from an action"
<ul>
<li><em>E.g.</em> $\text{Result}(\text{Move}(A, B), S_0)​$ means "the situation resulting from doing an action of type $\text{Move}(A, B)​$ in situation $S_0​$"</li>
<li>Note well that the expression $\text{Result}(\text{Move}(A, B), S_0)$ is a situation just as $S_0$ is.
<ul>
<li>Thus we can say $\text{Holds}(\text{On}(A, B), \text{Result}(\text{Move}(A, B), S_0))$</li>
</ul></li>
</ul></li>
</ul>
<h5 id="axiomatising-actions">Axiomatising Actions</h5>
<ul>
<li>Now we have all the necessary tools to describe the <strong>effects</strong> of actions, together with their <strong>preconditions</strong>.</li>
<li><em>E.g.</em> "If nothing is on $x$ and $y$, then one can move $x$ to on top of $y$, in which case $x$ will then be on $y$"
<ul>
<li>$\forall x, y, s.\ \text{Clear}(x, s) \land \text{Clear}(y, s) \implies \text{On}(x, y, \text{Result}(\text{Move}(x, y), s))$</li>
<li>This is called <strong>effect axiom</strong>, and it includes the precondition as well.</li>
</ul></li>
</ul>
<h4 id="situation-calculus">Situation Calculus</h4>
<p><img src="2D.assets/1554285476613.png" alt="1554285476613" /></p>
<ul>
<li>The approach described above is called <strong>Situation Calculus</strong>.</li>
<li>We axiomatise all our actions, and then use a <em>general theorem prover</em> to prove that a situation exists in which our <em>goal</em> is true.
<ul>
<li>And the situation, being a chain of actions from the initial state, would be the plan to execute!</li>
</ul></li>
</ul>
<h4 id="the-frame-problem">The Frame Problem</h4>
<ul>
<li><p>We have failed to express the fact that everything that isn't changed by an action stays the same.</p></li>
<li><p>We can try to fix is by adding <strong>frame axioms</strong>, but there would be lots of these.</p>
<ul>
<li>Imagine having to write frame axioms for every single object and configuration in your universe!</li>
</ul></li>
<li><p>It's a big problem.</p></li>
<li><p>We can have <em>neater</em> formulations, which then can be combined with effect axioms to get <strong>successor-state axioms</strong>:</p>
<p><img src="2D.assets/1554285899062.png" alt="1554285899062" /></p>
<ul>
<li>The first formulation says that "for all $x$, $y$, $s$ for state, and $a$ for action, if $x$ is on $y$ in situation $s$, then it will remain being the case in the resultant state of action $a$ unless the action is to move $x$ to top of some other block than $y$."</li>
<li>You can see how messy this is even for a dead-simple (for all practical planning purposes) example...</li>
<li>This solves the <em>representational</em> part of the frame problem.</li>
<li>We still have to compute (indeed now even more) things!
<ul>
<li>Inefficient (as is general theorem proving [as is most of theoretical computer science]).</li>
<li>Solution: special purpose representations &amp; algorithms, and software that exploits those called <strong>planners</strong>.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="refutation-theorem-proving">Refutation Theorem Proving</h4>
<p><em>Not getting into the details since it seems very much unnecessary.</em></p>
<ol>
<li>Negate the goal.</li>
<li>Skolemise.</li>
<li>Apply resolution on $\text{KB} \land \text{Skolemise}(\lnot\text{Goal})$.</li>
<li>Empty clause indicates that there is a solution.</li>
<li>The last/latest situation, being a chain of actions starting from the initial situation $S_0$ is your <strong>plan</strong>.</li>
</ol>
<h2 id="26-february-2019">26 February 2019</h2>
<p><em>Alex Lascarides' part of the course starts here.</em></p>
<h3 id="planning">Planning</h3>
<ul>
<li>Planning is the task of coming up with a <em>sequence</em> (hence ordered) of actions that will achieve a <em>goal</em>.</li>
<li>We are considering only <strong>classical planning</strong> in which environments are
<ul>
<li>fully observable</li>
<li>deterministic</li>
<li>finite</li>
<li>static (up to agents' actions)</li>
<li>discrete (in actions, states, objects, and events)</li>
</ul></li>
<li>We'll lift some of these restrictions as we proceed as "uncertainty" is a fact of life.</li>
</ul>
<h3 id="problems-with-search">Problems with Search</h3>
<ul>
<li>Planning problems can be quite challenging for search-based problem-solving agents.</li>
<li>Search-based solutions have no <strong>goal-directedness</strong>.</li>
<li>They also lack problem decomposition into <strong>sub-goal</strong> that build on each other.
<ul>
<li>A depth-first search-based agent can perfectly
<ul>
<li>undo past achievements and redo again and again</li>
<li>repeat the same action many times without realising its uselessness</li>
</ul></li>
<li>Simple goal test ("did I reach the goal state?") doesn't allow for the identification of <strong>milestones</strong> (important sub-goals).</li>
</ul></li>
<li>The questions are:
<ul>
<li>How do we find a good <strong>heuristic</strong> function?</li>
<li>How do we model the way humans perceive complex goals?
<ul>
<li>Beware that modelling and benchmarking against human behaviour is a good starter, but insisting on it is a <strong>myopia</strong>.</li>
</ul></li>
<li>How do we model &amp; evaluate the <strong>quality</strong> of a plan?</li>
</ul></li>
</ul>
<h3 id="logic--deductive-inference">Logic &amp; Deductive Inference</h3>
<ul>
<li>Logic &amp; deductive inference can be a better solution than search.</li>
<li>When goals are represented as a conjunction, it allows achievement of sub-goals (if they are independent).
<ul>
<li>Also beware that sometimes we might have to <em>regress</em> so it's not as simple. Imagine a blocks-world where there are blocks A, B, C, and our goal is to stack blocks A, B, C top-down. If the initial state is where A is on B and B &amp; C are on the table, we must first put A on the ground (which would be a regression since the sub-goal "A must be on B" is no longer the case.)</li>
</ul></li>
<li>Allows compact descriptions/formalisations:
<ul>
<li>States can be described by their properties in a compact way (<em>e.g.</em> $\text{On}(\text{A}, \text{B})$ can stand for hundreds of states).</li>
<li>Actions can be described compactly too: $\forall x.\ \text{Object}(x) \implies \text{Can}(\text{Agent}, \text{Grab}(x))$</li>
<li><strong>Most importantly, we use the same language/formalism/concept (first-order logic) for describing and reasoning about many things (from states, to actions).</strong></li>
</ul></li>
<li>Allows for representing a plan hierarchically (<em>e.g.</em> graduated = pass first year <strong>and</strong> pass second year <strong>and</strong> ...) where further decomposition is allowed (<em>e.g.</em> pass second year = pass 2D <strong>and</strong> ...)</li>
<li><strong>Problems:</strong>
<ul>
<li><em>In its general form</em>, it's either awkward (propositional logic) or suffers from tractability problems (first-order logic) [<em>e.g.</em> frame problem].</li>
<li>High-complexity.
<ul>
<li>While describing states and actions.</li>
<li>And especially for planners.</li>
</ul></li>
<li>If $p$ is a sequence that achieves the goal, then so is $[a, a^{-1}\mid p]$.
<ul>
<li>Same problem as in search.</li>
</ul></li>
</ul></li>
<li><strong>Solutions:</strong>
<ul>
<li>We need to <strong>reduce complexity</strong> to allow scaling up.</li>
<li>We need to allow reasoning to be <em>guided</em> by plan <em>quality</em>/efficiency.</li>
</ul></li>
</ul>
<h3 id="planning-domain-definition-language-pddl">Planning Domain Definition Language (PDDL)</h3>
<ul>
<li>We need a language expressive enough to cover interesting problems, and restrictive enough to allow (the use of) efficient algorithms (by the planner).</li>
<li>Welcome <strong>Planning Domain Definition Language (PDDL).</strong></li>
<li>PDDL allows you to express:
<ul>
<li>states</li>
<li>actions (a description of <em>transitions</em> between states)</li>
<li>and goals (as <em>partial</em> description of a state)</li>
</ul></li>
</ul>
<h4 id="representing-states--goals">Representing States &amp; Goals</h4>
<ul>
<li>States are represented as <strong>conjunctions</strong> of propositional or <em>function-free</em> first-order <em>positive</em> <em>literals</em>. Let's take a moment to make sure it sinks in:
<ul>
<li>A state description is a conjunction.</li>
<li>No functions are allowed.</li>
<li>Negations are not allowed, furthermore does not make sense due to <strong>closed-world</strong> assumption: if you have not mentioned something, the planner will assume that it's false.</li>
<li>Only literals &amp; propositions are allowed (<em>i.e.</em> no variables, bound or unbound [so you cannot use quantifiers either]).</li>
<li><em>E.g.</em> $\text{Happy} \land \text{In}(\text{Bora}, \text{Bodrum})$</li>
</ul></li>
<li>Goals are <strong>partial descriptions</strong> of states, and you can use negations, variables, quantifiers, disjunctions too!
<ul>
<li><em>E.g.</em> $\forall x.\ \lnot\text{In}(x, \text{Bodrum}) \lor \text{Happy}​$</li>
</ul></li>
</ul>
<h4 id="actions-in-pddl">Actions in PDDL</h4>
<p><img src="2D.assets/1554292368652.png" alt="1554292368652" /></p>
<ul>
<li>Called <strong>action schemata</strong>, as they may contain variables (like the <em>action type</em> vs <em>action</em> distinction in Vaishak's section.)</li>
<li><strong>Precondition</strong> defines the states in which the action is <em>executable</em>.
<ul>
<li>Same as goal, this is also a <strong>partial description</strong> so lot's of flexibility.</li>
</ul></li>
<li><strong>Effect</strong> defines how literals <strong>in the input state</strong> get changed.
<ul>
<li><strong>Anything not mentioned stays the same.</strong>
<ul>
<li>This is how we solve the frame-problem <em>practically</em>. Gotta love engineers.</li>
</ul></li>
<li><strong>Conjunction</strong> of positive and negative literals.
<ul>
<li>Not sure if it's a must, but negative literals are <em>very strongly advised</em> to occur in positive form in the precondition.</li>
</ul></li>
</ul></li>
<li>Any action is applicable/executable in any state that satisfies the precondition with an <em>appropriate substitution for parameters</em>.</li>
<li><strong>Result</strong> of executing action $a$ in state $s$ is state $s'$ with any positive literal $P$ in $a$'s <strong>EFFECT</strong> <em>added</em> to the state and every negative literal $\lnot P$ removed from it (all under the given substitution).</li>
<li>Hence the <strong>solution</strong> would be the action sequence that leads from the initial state to <strong>any</strong> state that satisfies the goal.</li>
</ul>
<p><strong>Last Remarks:</strong></p>
<ul>
<li>In theory, like in SQL, you'd just write what you want and a PDDL planner should work out the plan for you whereas in practice you might want/need to use customised actions to reduce the search space (<em>e.g.</em> instead of a generic $\text{Move()}$ action, you might have a $\text{MoveToTable}()$ action dedicated specifically to moving blocks to the table.)</li>
</ul>
<h2 id="28-february-2019">28 February 2019</h2>
<h3 id="planning-with-state-space-search">Planning with State-Space Search</h3>
<ul>
<li><p>State-space search is the most straightforward way to think of planning process: search the space of states using action schemata.</p></li>
<li><p>Since action definitions consist of both <em>preconditions</em> and <em>effects</em>, we can search in both directions.</p>
<ul>
<li><p><strong>Forward State-Space Search</strong></p>
<p>Start in initial state, consider action sequences until goal state is reached.</p></li>
<li><p><strong>Backward State-Space Search</strong></p>
<p>Start from goal state, consider action sequences until initial state is reached.</p></li>
</ul></li>
</ul>
<h4 id="forward-state-space-search">Forward State-Space Search</h4>
<ul>
<li><p>Also called <strong>progression</strong> planning.</p></li>
<li><p><strong>Formulation:</strong></p>
<ul>
<li>Initial state of search is initial state of planning problem (which is easy since initial state is a <em>complete state description</em>).</li>
<li>Applicable actions are those whose preconditions are satisfied.</li>
<li>Goal test: checking whether the current state satisfies goal of planning problem.</li>
<li>Step cost is usually one (<em>i.e.</em> each action has a cost of 1), but different costs can be allocated.</li>
</ul></li>
<li><p>Search space is <em>finite</em> in the <strong>absence of functions</strong>.</p></li>
<li><p>Any complete graph search algorithm (like A*) will be a complete graph planning algorithm.</p></li>
<li><p>Suffers from the problem of <em>irrelevant</em> actions, since all actions are considered from each state.</p>
<p><img src="2D.assets/1554295445819.png" alt="1554295445819" /></p></li>
<li><p>Thus its efficiency depends largely on the quality of heuristics.</p></li>
</ul>
<h4 id="backward-state-space-search">Backward State-Space Search</h4>
<ul>
<li>Also called <strong>regression</strong> planning.</li>
<li>Harder to start with since the goal is a <strong>partial description</strong> of the goal <em>states</em>.</li>
<li>Generating (possible) <em>predecessors</em> is also harder since effects are more akin to imperatives (add these literals, remove these literals) than a description of states.</li>
<li>Might be a good choice since exclusion of irrelevant actions decreases branching factor significantly.</li>
<li><strong>General process</strong> of constructing predecessors for backward search given goal description $G$, and a <em>relevant</em> &amp; <em>consistent</em> action $A$:
<ul>
<li>Any positive effects of $A$ that appear in $G$ are deleted.</li>
<li>Each precondition of $A$ is added unless it already appears.</li>
<li><em>Relevant</em> means that precondition of $A$ should not contain goal $G$.</li>
<li><em>Consistent</em> means that the effect of action $A$ should not negate goal $G$.</li>
<li>Substitutions may be required due to first-order logic.</li>
</ul></li>
<li>Any standard graph search algorithm can be used, terminating when predecessor description is satisfied by the initial (planning) state.</li>
</ul>
<h4 id="heuristics">Heuristics</h4>
<ul>
<li>Two main strategies for heuristics:
<ul>
<li><strong>Divide and Conquer</strong> (sub-goal decomposition)</li>
<li><strong>Relaxed Problem</strong></li>
</ul></li>
</ul>
<h5 id="divide-and-conquer">Divide and Conquer</h5>
<ul>
<li>Can perform poorly where regressions are required.
<ul>
<li>Imagine a blocks-world where there are blocks A, B, C, and our goal is to stack blocks A, B, C top-down. If the initial state is where A is on B and B &amp; C are on the table, we must first put A on the ground (which would be a regression since the sub-goal "A must be on B" is no longer the case.)</li>
</ul></li>
<li>Optimistic (<em>admissible</em>) if negative interactions exist (<em>e.g.</em> subplan deletes goal achieved by other subplan.)</li>
<li>Pessimistic (<em>inadmissible</em>) if positive interactions exist (<em>e.g.</em> subplans contain redundant actions.)</li>
</ul>
<h5 id="relaxations">Relaxations</h5>
<ul>
<li>Drop all preconditions (all actions are always applicable.)
<ul>
<li>Combined with subgoal independence, makes prediction even easier.</li>
</ul></li>
<li>Remove all negative effects (and count minimum number of actions so that <em>union</em> satisfies goals.)</li>
</ul>
<h3 id="partial-order-planning-pop-and-plan-space-search">Partial-Order Planning (POP) and Plan-Space Search</h3>
<p><img src="2D.assets/1554296635766.png" alt="1554296635766" /></p>
<ul>
<li>State-space search planning algorithms consider <strong>totally ordered</strong> sequences of actions.</li>
<li>Better not to commit ourselves to complete chronological ordering of tasks (<strong>least commitment</strong> is better).</li>
<li><strong>Basic Idea</strong>
<ul>
<li>Add actions to a plan without specifying which comes first unless necessary.</li>
<li>Combined <em>independent</em> subsequences afterwards.</li>
</ul></li>
<li>Partial-order solution will correspond to one or several <strong>linearisations</strong> of a partial-order plan.
<ul>
<li>Of course different linearisations have different implications for execution!</li>
</ul></li>
<li>Search in <em><strong>plan space</strong></em> rather than <em>state spaces</em> (because your search is over <em>ordering constraints on actions</em>, as well as transitions among states.)
<ul>
<li>Thus <strong>nodes denote actions</strong> and <strong>edges denote preconditions satisfied</strong>. This isn't quite intuitive at first so make sure you get it right.</li>
</ul></li>
</ul>
<h5 id="pop-as-a-search-problem">POP as a Search Problem</h5>
<p><img src="2D.assets/1554304439932.png" alt="1554304439932" /></p>
<p>POP as a search problem consists of:</p>
<ul>
<li><p><strong>Actions</strong></p>
<p>Initial plan consists of <em>dummy</em> actions $\text{Start}$ (no preconditions, effect=initial state) and $\text{Finish}$ (no effects, precondition=goal literals).</p></li>
<li><p><strong>Ordering Constraints</strong> on actions</p>
<p>Such as $A \prec B$ (A precedes B); contradictory constraints are prohibited.</p></li>
<li><p><strong>Casual Links</strong> between actions</p>
<p>$A \xrightarrow{p} B$ expresses $A$ achieves $p$ for $B$ ($p$ precondition of $B$ and effect of $A$); $p$ must remain true between $A$ and $B$ (inserting action $C$ with effect $\lnot p$ such that $A \prec C \prec B$ would lead to conflict [since precondition $p$ is violated as it's removed by $C$]).</p></li>
<li><p><strong>Open Preconditions</strong></p>
<p>Set of conditions not yet achieved by the plan (planners try to make open precondition set empty without introducing contradictions.)</p></li>
<li><p>A <strong>consistent plan</strong> is a plan without cycles in orderings and conflicts with <em>causal links</em>.</p></li>
<li><p>A <strong>solution</strong> is a consistent plan without <em>open preconditions.</em></p></li>
</ul>
<h5 id="the-pop-algorithm">The POP Algorithm</h5>
<ul>
<li>Pick a $p$ from open preconditions <em>on some action $B$</em>, generate a consistent successor plan for every $A$ <em>that achieves $p$</em>.</li>
<li>Goal test: check whether there are any open preconditions left.
<ul>
<li>We don't have to worry about consistency as we ensure it at every step; see below.</li>
</ul></li>
</ul>
<p><strong>Ensuring Consistency:</strong></p>
<ul>
<li>If conflict between $A \xrightarrow{p} B​$ and $C​$, add $B \prec C​$ or $C \prec A​$.
<ul>
<li>In other words, if $C$ reverses an effect of $A$ which also happens to be the precondition of $B$, then it's clear that $C$ must come either before $A$ or after $B$ (and we must make sure that $C$ does not violate the precondition of $A$ or of the action that immediately follows $B$, and goes on...)</li>
</ul></li>
</ul>
<p><strong>Unbound Variable:</strong></p>
<ul>
<li>In first-order case (which is almost always the case...) unbound variables may occur during planning process.</li>
<li><img src="2D.assets/1554305143059.png" alt="1554305143059" /></li>
<li>This also has an effect on links, <em>e.g.</em> in the example above $\text{Move}(A, x, B) \xrightarrow{\text{On}(A, B)} \text{Finish}$ would be added.</li>
<li><strong>Problem:</strong> If another action has effect $\lnot\text{On}(A, z)$, then this leads to a conflict if $z = B$.
<ul>
<li><strong>Solution:</strong> Insert <strong>inequality constraints</strong> (<em>e.g.</em> $z \ne B$) and checks these constraints whenever applying substitutions.</li>
</ul></li>
<li>Makes <em>heuristics</em> even harder than in total-order planning.</li>
</ul>
<h5 id="example-3">Example</h5>
<ul>
<li><p><img src="2D.assets/1554305365454.png" alt="1554305365454" /></p>
<ul>
<li>The EFFECT of <strong>LeaveOvernight</strong> is on the presumption that everything will get stolen if you leave them overnight. =)</li>
</ul></li>
<li><p><img src="2D.assets/1554305378851.png" alt="1554305378851" /></p>
<ul>
<li>Note well that we choose open preconditions in an <strong>orderly manner</strong>. Now, this is not a formal requirement, but can make your life <em>a lot</em> easier in the exam.</li>
</ul></li>
<li><p><img src="2D.assets/1554305392582.png" alt="1554305392582" /></p>
<ul>
<li>It's unfortunately a bit confusing when they ask "Why this is the only solution?" when it's clear that it's not a solution! Keep reading.</li>
</ul></li>
<li><p><img src="2D.assets/1554305407333.png" alt="1554305407333" /></p></li>
<li><p><img src="2D.assets/1554305687054.png" alt="1554305687054" /></p></li>
</ul>
<h2 id="1-march-2019">1 March 2019</h2>
<h3 id="non-deterministic-domains">Non-Deterministic Domains</h3>
<ul>
<li><strong>"Classic"</strong> assumption (fully observable, static, deterministic, actions descriptions are correct &amp; complete) is **unrealistic **in many real-world applications:
<ul>
<li>We don't know everything, and may even hold <em>incorrect information</em>.</li>
<li>Actions can <em>go wrong</em></li>
</ul></li>
<li>We distinguish <strong>bounded</strong> and <strong>unbounded</strong> indeterminacy:
<ul>
<li><strong>Unbounded</strong> if all possible preconditions &amp; effects cannot be listed.</li>
<li>Unbounded indeterminacy is related to <em>qualification problem</em>.</li>
</ul></li>
</ul>
<h4 id="methods-for-handling-indeterminacy">Methods for Handling Indeterminacy</h4>
<ul>
<li><p><strong>Sensorless/Conformant Planning</strong></p>
<p>Achieve goal in all possible circumstances, <em>relies on coercion</em>.</p></li>
<li><p><strong>Contingency Planning</strong></p>
<p>Used in <em>partially observable</em> and <em>non-deterministic</em> environments; includes <em>sensing actions</em> and describes <em>different paths for different circumstances</em>.</p></li>
<li><p><strong>Online Planning and Replanning</strong></p>
<p>Checks whether plan requires <em>revision during execution</em> and replan accordingly.</p></li>
<li><p>The difference between those three is not crystal-clear unfortunately.</p></li>
<li><p><strong>Conformant planning</strong> seems to rely on brute-force/coercion so it's easy to distinguish.</p>
<ul>
<li>An example would be to paint everything the same colour, to satisfy the goal that all objects have the same colour (as opposed to intelligently painting only those which needs to be painted).</li>
<li><strong>The idea is to come up with a plan that works in all possible cases.</strong></li>
</ul></li>
<li><p><strong>Contingent planning</strong> (also called conditional planning) requires <strong>planning ahead</strong> for <strong>different possible results of <em>each action</em></strong>.</p>
<ul>
<li>The chief difference between this and online planning is that here in contingent planning, we have considered all the possible paths <em>beforehand</em> so indeed there is still some kind of determinacy in the world.
<ul>
<li>It can be said that the world is deterministic/static up to our agent's actions.</li>
</ul></li>
<li>So by observing the indeterminate effects of our actions alone (which is <strong>bounded</strong>) we can choose which pre-computed plan to follow.
<ul>
<li>If the effects of our actions are <strong>unbounded</strong>, you'd use online planning unless the "side effects" benign or irrelevant to our goal.</li>
</ul></li>
</ul></li>
<li><p><strong>Online planning</strong> (which requires <em>execution monitoring</em>) handles domains where --not only the effects of an action is non-deterministic-- but the world is dynamic.</p>
<ul>
<li>Thus while executing the plan, <em>before performing each action</em>, monitor the environment.
<ul>
<li>If the environment is different than expected, <em>replan</em>.</li>
</ul></li>
<li>Perhaps it can also be said that here the agent <em>plans itself</em> rather than choosing a pre-computed plan.</li>
</ul></li>
<li><p>Source: <a href="http://vlm1.uta.edu/~athitsos/courses/cse4308_fall2015/lectures/04b_real_world_planning.pdf">http://vlm1.uta.edu/~athitsos/courses/cse4308_fall2015/lectures/04b_real_world_planning.pdf</a></p></li>
</ul>
<h3 id="sensing-with-percepts">Sensing with Percepts</h3>
<ul>
<li>A <strong>percept schema</strong> models the agent's sensors.</li>
<li>It tells the agent what it knows, given certain conditions about the state it's in.
<ul>
<li><img src="2D.assets/1554364420993.png" alt="1554364420993" /></li>
</ul></li>
<li>A fully observable environment has a percept axiom for each <strong>fluent</strong> <em>with no preconditions</em>!</li>
<li>A sensorless planner has no percept schemata at all.</li>
</ul>
<h3 id="belief-states">Belief States</h3>
<p>There are three approaches:</p>
<ol>
<li><p>A belief state is <strong>a set of state representations</strong> that the agent thinks might be the case.</p>
<p><img src="2D.assets/1554365037393.png" alt="1554365037393" /></p></li>
<li><p>Logical sentences can capture a belief state.</p>
<ul>
<li>Open-world assumption (the lack of a predicate shows ignorance about it) offers a more compact representation (rather than having to specify $A$ and $\lnot A$ in your belief state as in first approach.)</li>
<li>A problem is that there will be many equivalent sentences, and we need a <strong>canonical</strong> representation to avoid general theorem proving.
<ul>
<li><strong>Solution:</strong> All representations are ordered conjunctions of literals under the open-world assumption.
<ul>
<li>But this doesn't capture everything (for example, you cannot use disjunctions.)</li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>Knowledge propositions</strong> <em>e.g.</em> $K(\text{AtR}) \land K(\text{CleanR})$ (<em>i.e.</em> closed-world assumption).</p></li>
</ol>
<p>We'll use the second method in this course, but clearly there is some loss of expressiveness.</p>
<h3 id="sensorless-planning">Sensorless Planning</h3>
<ul>
<li><p>There are no sensory fluents (<em>e.g.</em> $\text{InView}$), because there are no sensors!</p></li>
<li><p>There will be unchanging facts (such as <em>Table</em> and <em>Chair</em> are objects, <em>C<sub>1</sub></em> and <em>C<sub>2</sub></em> are cans, etc.)</p></li>
<li><p>And we would know that objects and cans have colours.</p>
<p>$\forall x.\ \exists c.\ \text{Colour}(x, c)$</p></li>
<li><p>After skolemisation this gives an <strong>initial belief state</strong>:</p>
<p>$b_0 = \text{Colour}(x, C(x))$</p></li>
<li><p><strong>A belief state corresponds exactly to <em>the set of possible worlds</em> that satisfy the formula under open-world assumption.</strong></p></li>
</ul>
<p><strong>Rules:</strong></p>
<ul>
<li>We can apply actions only whose preconditions are satisfied by our current belief state $b$.</li>
<li>The <strong>update of a belief state $b$ given an action $a$</strong> is the set of all states that result (in the physical transition model) from doing $a$ <strong>in each possible state $s$ that satisfied belief state $b$</strong>.</li>
</ul>
<h3 id="extending-action-representations">Extending Action Representations</h3>
<h4 id="conditional-effects">Conditional Effects</h4>
<p><img src="2D.assets/1554366203459.png" alt="1554366203459" /></p>
<ul>
<li>So far, we have only considered actions that have the <em>same effects</em> on all states where the preconditions are satisfied.</li>
<li><strong>This means that any initial belief state that is a conjunction is updated by actions to a belief state that is also a conjunction.</strong>
<ul>
<li>Indeed that was the case with PDDL!</li>
</ul></li>
<li>But some actions are best expressed with conditional effects.</li>
<li>This is especially true if the effects are non-deterministic, but in a <em>bounded</em> way.</li>
</ul>
<h4 id="others">Others</h4>
<ul>
<li><p><strong>Disjunctive Effects</strong></p>
<p><img src="2D.assets/1554366238009.png" alt="1554366238009" /></p></li>
<li><p><strong>Combination of Disjunctions and Conditions</strong></p>
<p><img src="2D.assets/1554366278098.png" alt="1554366278098" /></p></li>
</ul>
<h3 id="contingent-planning">Contingent Planning</h3>
<ul>
<li><p>Conditions are generously used:</p>
<p><strong>if</strong> $\text{AtL} \land \text{CleanL}$ <strong>then</strong> $\text{Right}$ <strong>else</strong> $\text{Vacuum}$</p>
<ul>
<li>Think like Haskell, where conditions themselves are expression!</li>
<li>So don't try to write line by line <em>imperative code</em>.
<ul>
<li>Nesting conditional steps results in trees.</li>
</ul></li>
</ul></li>
<li><p>Variables are <strong>existentially quantified</strong>.</p></li>
<li><p><strong>Conditional plans shall succeed regardless of circumstances.</strong></p>
<ul>
<li>Similar to adversarial search, it's a game against nature!
<ul>
<li>Game tree has state nodes, and chance nodes where nature determines the outcome.</li>
</ul></li>
</ul></li>
<li><p><strong>Solution:</strong> A subtree with <strong>at least one goal node at sub-sub tree</strong>.</p>
<p><img src="2D.assets/1554366709856.png" alt="1554366709856" /></p>
<ul>
<li>Here, it's clear that (at the very beginning) going <em>Left</em> is a solution since after that, both sub-subtrees have at least one goal leaf.</li>
<li>Circles are <em>chance nodes</em> where we observe the environment (and since the environment is non-deterministic we call them chance nodes.)</li>
</ul></li>
</ul>
<h5 id="acyclic-vs-cyclic-solutions">Acyclic vs Cyclic Solutions</h5>
<ul>
<li>If an identical state is encountered (on the same path), terminate with failure (since an acyclic solution can be reached from the previous incarnation of the repeated state.)</li>
<li><strong>However, sometimes all solutions are cyclic!</strong>
<ul>
<li><em>E.g.</em> Trying to restart a car until the engine starts in winter. You might have to try again and again...</li>
</ul></li>
</ul>
<h5 id="non-determinism-and-partially-observable-environments">Non-determinism and Partially Observable Environments</h5>
<ul>
<li>Imagine a vacuum cleaner that can sense the cleanliness of the square it's in, but not of any other square
<ul>
<li>...and dirt can sometimes be left behind when leaving a clean square!</li>
</ul></li>
<li>We can keep moving back and forth between squares but now <strong>the goal test cannot be performed!</strong></li>
<li><strong>Fully observability is a special case of partial observability with singleton belief states.</strong></li>
</ul>
<h2 id="5-march-2019">5 March 2019</h2>
<h3 id="online-planning">Online Planning</h3>
<h4 id="execution-monitoring-and-replanning">Execution Monitoring and Replanning</h4>
<ul>
<li><p><strong>Execution monitoring</strong> is checking whether things are going according to the plan (necessitated by <strong>unbounded</strong> indeterminacy in realistic environments.)</p>
<ul>
<li><strong>Action monitoring</strong> is checking whether <em>next action</em> is feasible.</li>
<li><strong>Plan monitoring</strong> is checking whether the <em>remainder of the plan</em> is feasible.
<ul>
<li>Plan monitoring is thus more advanced &amp; complicated.</li>
</ul></li>
</ul></li>
<li><p><strong>Replanning</strong> is the ability to find a new plan when things go wrong (usually by repairing the old plan.)</p></li>
<li><p>Taken together, these methods yield powerful planning abilities.</p>
<p><img src="2D.assets/1554369485431.png" alt="1554369485431" /></p>
<ul>
<li>Realise that <em>repairing for continuation</em> is not always the best strategy, indeed there might be shorter/cheaper way from $O$ to $G$.</li>
</ul></li>
<li><p><strong>Action monitoring</strong> often results in <strong>suboptimal behaviour</strong>, as it executes everything until actual failure.</p>
<ul>
<li>Whereas <strong>plan monitoring</strong> checks preconditions for entire remaining plan.</li>
<li>Plan monitoring can also take advantage of <strong>serendipity</strong> (unexpected circumstances that might make remaining plan easier, as hinted above.)</li>
</ul></li>
<li><p>In partially observable environments, things are even more complex (<em>e.g.</em> sensing actions have to be planned for [you need to open a can of paint to see what colour is it], things can fail in turn, etc.)</p></li>
</ul>
<h3 id="hierarchical-decomposition-in-planning">Hierarchical Decomposition in Planning</h3>
<ul>
<li>At each level of the hierarchy, activity involves only small number of steps (which often have further sub-steps).</li>
<li><strong>Hierarchical Task Network (HTN)</strong> planning: initial plan provides only high-level description, and needs to be <em>refined</em> by <strong>action refinements</strong>.
<ul>
<li>Refinement process continued until plan consists only of <strong>primitive actions</strong>.</li>
</ul></li>
<li>Each <strong>High Level Action (HLA)</strong> has (at least) one refinement into a sequence of actions.
<ul>
<li>The actions in the sequence may be HLAs or primitive (or a combination of both).</li>
<li>If they are all primitive, than that's an <em>implementation</em> of the HLA.</li>
<li><strong>Refinements can be recursive</strong>, given that they terminate of course.</li>
</ul></li>
<li><strong>High-Level Plans (HLP)</strong> are a sequence of HLAs.</li>
<li>An <em>implementation</em> of a high-level plan is the concatenation of <em>an</em> implementation of each of its HLAs.</li>
<li><strong>An HLP achieves the goal from an initial state if <em>at least one</em> of its implementations does this.</strong>
<ul>
<li>Not all implementations of an HLP have to reach the goal state!</li>
<li>The agent get to decide which implementation of which HLAs to execute.</li>
</ul></li>
</ul>
<h4 id="searching-for-primitive-solutions">Searching for Primitive Solutions</h4>
<ul>
<li>The <strong>HLA plan library</strong> is a hierarchy (too of course)!
<ul>
<li>Ordered children to an high-level action are the sequences of (high-level or primitive) actions provided by one of its refinements.</li>
</ul></li>
<li>Because a given HLA can have more than one refinement, <strong>there can be more than one node for a given HLA in the same <em>action-plan</em> hierarchy!</strong></li>
<li>This hierarchy is essentially a <strong>search space of action sequences</strong> that conform to knowledge about how high-level actions can be broken down.
<ul>
<li>So we can search this space for a plan!</li>
</ul></li>
</ul>
<h5 id="breadth-first-search-1">Breadth First Search</h5>
<ul>
<li>Start your plan $P$ with the HLA $[\text{Act}]$.</li>
<li>Take the first high-level action $A$ in $P$ (recall that $P$ is an action <em>sequence</em> [<em>i.e.</em> it's ordered]).</li>
<li>Do a breadth-first search in your <strong>hierarchical plan library</strong>, to find <em>a</em> refinement of $A$ whose preconditions are satisfied by the outcome of the (sequence of) actions in $P$ prior to $A$.
<ul>
<li>I think (personally), that we consider all possible refinements of $A$, instead of picking one.</li>
</ul></li>
<li>Replace $A$ in $P$ with this refinement.</li>
<li>Keep repeating until your plan $P$ has no high-level actions and either:
<ul>
<li>Your plan $P$'s outcome is in the goal, in which case return $P$;</li>
<li>or your plan $P$'s outcome is not the goal, in which case return <em>failure</em>.</li>
</ul></li>
</ul>
<h6 id="problems">Problems</h6>
<ul>
<li>Like forward search, you consider lots of <strong>irrelevant actions</strong>.</li>
<li>The algorithm essentially refines HLAs right down to primitive actions so as to determine if a plan will succeed.
<ul>
<li>But this contradicts <em>common sense</em>!</li>
<li>Sometimes you know an HLA will work <em>regardless</em> of how it's broken down.
<ul>
<li>We don't need to know which route to take to SFOParking to know that we can go from home to the airport, and take a shuttle from parking to the airport.</li>
</ul></li>
<li><strong>We can capture this if we add to HLAs <em>themselves</em> a set of preconditions and effects.</strong></li>
</ul></li>
</ul>
<h6 id="adding-preconditions-and-effects-to-hlas">Adding Preconditions and Effects to HLAs</h6>
<ul>
<li>One challenge in specifying preconditions and effects of an HLA is that the HLA may have more than one refinement, <strong>each one with slightly different preconditions and effects</strong>!
<ul>
<li>If you decide to take a taxi to the parking area, you need $\text{Cash}$.</li>
<li>If you refine it with $\text{Drive}$, you don't need it!</li>
<li>The difference may restrict your <em>choice</em> on how you can refine an HLA.
<ul>
<li>Recall that an HLA achieves a goal if one of its refinements does this.</li>
<li>And you can choose the refinement.</li>
</ul></li>
</ul></li>
</ul>
<h5 id="formalisms">Formalisms</h5>
<ul>
<li><p>$\text{REACH}$ returns a set of states reachable from a given state and a high-level action.</p></li>
<li><p>$s' \in \text{REACH}(s, h)$ if and only if $s'$ is reachable from <em>at least one</em> of refinements of HLA $h$ given state $s$.</p>
<p>$$\begin{align*}\text{REACH}(s, [h_1, h_2]) = \bigcup_{s' \in \text{REACH}(s, h_1)} \text{REACH}(s', h_2)\end{align*}$$</p>
<ul>
<li>Add bigcup for each HLA before the last one...</li>
</ul></li>
<li><p>HLP $p$ achieves goal $g$ given state $s$ if and only if $\exists s'$ such that</p>
<p>$$\begin{align*}s\vDash g \text{ and } s' \in \text{REACH}(s, p)\end{align*}$$</p></li>
<li><p>Thus we should search on HLPs to find a $p$ with this relation to $g$, and then focus on refining it.</p>
<ul>
<li>But a pre-requisite to this algorithm is to define $\text{REACH}(s, h)$ for each $h$ and $s$.</li>
<li>In other words, we still need to determine how to represent the preconditions &amp; effects of HLAs...</li>
</ul></li>
</ul>
<h6 id="defining-reach">Defining REACH</h6>
<ul>
<li><p>A primitive action makes a fluent true, false, or leaves it unchanged.</p></li>
<li><p>But with HLAs, you sometimes get to <strong>choose</strong>, by choosing a particular refinement.</p></li>
<li><p>We introduce a new notation to reflect this:</p>
<p><img src="2D.assets/1554372414657.png" alt="1554372414657" /></p></li>
<li><p>We should now be able to <em>derive</em> the correct preconditions and effects for its refinements <strong>before committing ourselves to a specific refinement</strong>.</p></li>
<li><p>Of course these are approximate, thus we cannot combine them as we wish!</p>
<ul>
<li>Solution is to write <strong>approximate descriptions</strong> for $\text{REACH}$.</li>
</ul></li>
</ul>
<h6 id="approximate-descriptions-of-reach">Approximate Descriptions of REACH</h6>
<p><strong>Optimistic Description</strong> $\text{REACH}^+(s, h)$</p>
<ul>
<li>Take union of all possible outcomes from all refinements.
<ul>
<li><em>I.e.</em> Union of all.</li>
</ul></li>
<li>This <strong>over-generates</strong> reachable states.</li>
</ul>
<p><strong>Pessimistic Description</strong> $\text{REACH}^-(s, h)$</p>
<ul>
<li>Only states that satisfy the effects of <strong>all</strong> refinements survive.
<ul>
<li><em>I.e.</em> Intersection of all.</li>
</ul></li>
<li>This <strong>under-generates</strong> reachable states.</li>
</ul>
<p>$$\text{REACH}^-(s, h) \subseteq \text{REACH}(s, h) \subseteq \text{REACH}^+(s, h)$$</p>
<h5 id="a-better-algorithm">A Better Algorithm</h5>
<p>We know that</p>
<ol>
<li>If $\exists s' \in \text{REACH}^-(s, h)$ such that $s' \vDash g$, we know that $h$ <strong>can</strong> succeed.</li>
<li>If $\lnot\exists s' \in \text{REACH}^+(s, h)$ such that $s' \vDash g$, we know that $h$ <strong>will</strong> fail.</li>
</ol>
<p>Thus the algorithm is:</p>
<ul>
<li>Do the breadth first search as before.</li>
<li>But we can now <strong>stop searching</strong> and <strong>implement instead</strong> when we reach an $h$ where (1) is true.</li>
<li>And we can <strong>drop</strong> $h$ (and all its refinements) when (2) is true.</li>
<li>If (1) and (2) are both false for the current $h$, then we don't know if $h$ will succeed or fail, but we can find out by refining it (<em>i.e.</em> continuing breadth-first search).</li>
<li>N.B. We check for (1) and (2) at every iteration of breadth-first search.</li>
</ul>
<h2 id="7-march-2019">7 March 2019</h2>
<p><em>This lecture is a soft introduction.</em></p>
<h3 id="handling-uncertainty">Handling Uncertainty</h3>
<ul>
<li>Do NOT confuse <em>uncertainty</em> with <em>indeterminacy</em>.
<ul>
<li>Uncertainty <em>often</em> arises from indeterminacy, but unreliable sensors in a fully-deterministic environment can be the cause of uncertainty too!</li>
</ul></li>
<li>So far we have always assumed that propositions are true, false, or unknown.</li>
<li>Whereas in reality, we have <em>hunches</em> rather than complete ignorance or absolute knowledge.</li>
<li>The previous approaches we have seen such as conditional or online planning handle things that might go wrong, but they don't tell us <strong>how likely it is that something might go wrong</strong>.</li>
<li>And <strong>rational decisions</strong> (<em>i.e.</em> the right thing to do) depend on the relative importance of various goals and the <strong>likelihood</strong> that (and the degree to which) they will be achieved.</li>
</ul>
<h4 id="using-classical-logic-to-handle-uncertainty">Using Classical Logic to Handle Uncertainty</h4>
<ul>
<li><p>Classical logic is not very useful to capture uncertainty because of:</p>
<ul>
<li><p><strong>complexity</strong></p>
<p>can be impractical to include all antecedents and consequents in rules, and/or it might be too hard to use them</p></li>
<li><p><strong>theoretical ignorance</strong></p>
<p>we often don't know "rules" completely; <em>e.g.</em> $\forall p.\ \text{Disease}(p, \text{Cavity}) \implies \text{Symptom}(p, \text{Toothache})$ (how can we be sure of this?)</p></li>
<li><p><strong>practical ignorance</strong></p>
<p>we often don't know for certain the current state (<em>e.g.</em> we can only express our degree of belief in patient having a dental cavity.)</p></li>
<li><p><strong>lack of consideration for <em>unknown</em> factors</strong></p>
<p>classical logic completely ignores the possibility of some unknown factors that might be influencing the world; furthermore leaves little room for agency (<em>i.e.</em> a way for agents to choose one option over another [<em>e.g.</em> even though it's more likely, perhaps as a dentist you want to explore other options as they are cheaper/less painful for the patient.])</p></li>
</ul></li>
</ul>
<h4 id="degrees-of-belief-and-probabilities">Degrees of Belief and Probabilities</h4>
<ul>
<li>One possible approach is to express <strong>degrees of belief</strong> in propositions using <strong>probability theory</strong>.
<ul>
<li>Probabilities can <strong>summarise</strong> the <em>uncertainty</em> that arises from our "laziness" and/or ignorance.</li>
<li>Probabilities in $[0, 1]$ express the degree to which we believe a proposition to be true.</li>
</ul></li>
<li><em>Nota bene</em> that <strong>in probability theory, propositions <em>themselves</em> are <em>actually</em> either true or false!</strong>
<ul>
<li><strong>Degrees of truth</strong> (which is an entirely different concept) are subject to other methods (like <strong>fuzzy logic</strong>) which are not dealt with here.</li>
</ul></li>
<li><strong>Degrees of belief depend on <em>evidence</em></strong> and should change with new evidence (<em>e.g.</em> sensory percepts).
<ul>
<li>Do not confuse this with change in the world that might make the proposition itself true or false.
<ul>
<li>But of course the change in the world will affect our percepts too...</li>
</ul></li>
<li>Before any evidence is obtained we speak of <strong>prior/unconditional probability</strong>, whereas after evidence of <strong>posterior probability</strong>.</li>
</ul></li>
</ul>
<h4 id="rational-decisions">Rational Decisions</h4>
<ul>
<li>Agent must have <strong>preferences</strong> over <strong>outcomes</strong> of plans.
<ul>
<li><em>E.g.</em> even though a cavity more likely, perhaps as a dentist you want to explore other options as they are cheaper/less painful for the patient.</li>
</ul></li>
<li><strong>Utility theory</strong> can be used to reason about those preferences.
<ul>
<li>Based on the idea that every <strong>state</strong> has a degree of usefulness and agents prefer states with higher utility.</li>
</ul></li>
<li>Utility may vary from one agent to another.</li>
</ul>
<h4 id="decision-theory">Decision Theory</h4>
<ul>
<li><p>A general theory of <strong>rational decision making</strong>.</p></li>
<li><p><strong>Decision theory</strong> is the combination of <em>probability theory</em> and <em>utility theory</em>.</p></li>
<li><p><strong>Essentially:</strong></p>
<p>An agent is rational if and only if it chooses the action that has the <strong>highest expected utility averaged over all possible outcomes of the action</strong>.</p></li>
<li><p>Relies on the <strong>Principle of Maximum Expected Utility</strong>.</p></li>
<li><p>We will study decision theory in this course, but beware that it's not free of criticism:</p>
<ul>
<li>How do we know our preferences? Human behaviour is not a perfect example of rationality, so do we want our agents to imitate us or to act more rationally yet in less humane ways?
<ul>
<li>Humans are indeed <em>predictably irrational</em>.</li>
</ul></li>
<li>Are our preferences always consistent with each other, pairwise and/or mutually?</li>
<li>Are there any situations where <strong>risk-taking</strong> is desirable? If yes, how can we model that?</li>
</ul></li>
<li><p>For the time being, we will focus on probability and not utility.</p>
<ul>
<li><p>Yet to have an idea of general abstract design of an agent based on decision theory:</p>
<ol>
<li>Update the belief state based on the previous action and percepts.</li>
<li>Calculate the probabilities of the each possible outcome of each action given action descriptions and the updated belief state.</li>
<li>Select action with highest expected utility given probabilities and utility information.</li>
</ol>
<ul>
<li>It is possible to imagine an agent making plans that consists of a sequence of actions based on the same principals.</li>
<li>This is very simple but broadly accepted as a general principle for building agents that are able to cope with real-world environments.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="introduction-to-probability">Introduction to Probability</h3>
<ul>
<li><p>Degrees of Belief concern propositions.</p></li>
<li><p>A <strong>Random Variable</strong> is a part of the world whose status is unknown, and it has a <strong>domain</strong> (<em>e.g.</em> $\text{Weather}$ with domain ${\text{Sunny}, \text{Rainy}, \text{Snowy}}$).</p>
<ul>
<li>Domain can be <em>boolean</em>, <em>discrete</em>, or <em>continuous</em>.</li>
</ul></li>
<li><p>We can compose complex propositions from statements about random variables.</p>
<p><em>E.g.</em> $\text{Weather} = \text{Sunny} \land \text{Mood} = \text{Happy}$</p></li>
<li><p>An <strong>Atomic Event</strong> is a <strong>complete specification</strong> of the state of the world. Thus:</p>
<ul>
<li>atomic events are <strong>mutually exclusive</strong>.</li>
<li>their set is exhaustive.</li>
<li>every event entails truth or falsehood of any <strong>proposition</strong> (like <em>models</em> in logic).</li>
<li>every proposition is logically equivalent to the disjunction of all atomic events that entail it.
<ul>
<li>In other words, every propositions "stands for"/"represents" multiple states of the world (<em>e.g.</em> there are surely more than one different configurations of Scotland where the weather is sunny and we are happy...)</li>
</ul></li>
</ul></li>
<li><p>A <strong>Probability Distribution</strong> is the probabilities of all values of a random variable.</p>
<ul>
<li>It's a function that maps each value of a random variable to a non-negative real number, whose sum must be equal to 1.</li>
</ul></li>
<li><p>For a mixture of several variables, we obtain a <strong>Joint Probability Distribution (JPD)</strong> -- a cross-product of individual distributions.</p></li>
<li><p>For continuous variables we use <strong>Probability Density Functions (PDF)</strong> since we cannot enumerate values.</p></li>
<li><p><strong>Conditional Probability</strong> $\Pr(a \mid b)$ is the probability of $a$ given that <strong>all we know</strong> is $b$.</p>
<p>$$\begin{align*}\Pr(a \mid b) = \frac{\Pr(a \land b)}{\Pr(b)}\end{align*}​$$</p>
<ul>
<li><p>Often written as <strong>product rule</strong>: $\Pr(a \land b) = \Pr(a \mid b)\Pr(b)​$.</p></li>
<li><p>Conditional probability is good for describing joint probability distributions:</p>
<p>$$\Pr(a, b) = \Pr(a \land b) = \Pr(a \mid b)\Pr(b)$$</p>
<ul>
<li>Hence you can find them called as <strong>conditional probability distribution</strong> too. =)</li>
</ul></li>
<li><p><strong>Conditional probability does NOT mean logical implication.</strong></p></li>
</ul></li>
</ul>
<h4 id="the-axioms-of-probability">The Axioms of Probability</h4>
<ul>
<li><strong>Kolmogorov's Axioms</strong> define basic semantics for probabilities:
<ol>
<li>$0 \le \Pr(a) \le 1$ for any proposition $a$</li>
<li>$\Pr(\top) = 1$ and $\Pr(\bot) = 0$</li>
<li>$\Pr(a \lor b) = \Pr(a) + \Pr(b) - \Pr(a \land b)$</li>
</ol></li>
</ul>
<p><img src="2D.assets/1554380493244.png" alt="1554380493244" /></p>
<h2 id="12-march-2019">12 March 2019</h2>
<h3 id="inference-with-joint-probability-distributions">Inference with Joint Probability Distributions</h3>
<ul>
<li><p><strong>Problem</strong></p>
<p>Given some observed evidence and a query proposition, how can we compute the <strong>posterior probability</strong> of that proposition?</p></li>
<li><p>We'll first discuss JPD as a "knowledge base".</p></li>
</ul>
<h3 id="marginalisation-conditioning--normalisation">Marginalisation, Conditioning &amp; Normalisation</h3>
<p><img src="2D.assets/1554384555599.png" alt="1554384555599" /></p>
<ul>
<li><p><strong>Marginalisation</strong> is (a way of calculating the probability of something by) extracting the distribution of a subset of variables.</p>
<p>$$\Pr(Y) = \sum_z \Pr(Y, z)$$</p>
<ul>
<li><p>Example:</p>
<p><img src="2D.assets/1554384669363.png" alt="1554384669363" /></p></li>
</ul></li>
<li><p><strong>Conditioning</strong> is a variant (also a way of calculating the probability of something) using the product rule.</p>
<p>$$\Pr(Y) = \sum_z \Pr(Y \mid z) P(z)$$</p>
<ul>
<li><p>Example:</p>
<p>$$\Pr(\text{cavity}) = \Pr(\text{cavity} \mid \text{tootache}) + \Pr(\text{cavity} \mid \lnot\text{tootache})$$</p></li>
</ul></li>
<li><p><strong>Normalisation</strong> is to ensure that probabilities sum to 1; normalisation constants often denoted by $\alpha$.</p>
<ul>
<li><p>Example:</p>
<p>$$\begin{align*} &amp;\Pr(\text{Cavity} \mid \text{toothache})\\ =\ &amp;\alpha\Pr(\text{Cavity}, \text{toothache})\\ =\ &amp;\alpha[\Pr(\text{Cavity}, \text{toothache}, \text{catch}) + \Pr(\text{Cavity}, \text{toothache}, \lnot\text{catch})]\\ =\ &amp;\alpha[\langle 0.108, 0.016\rangle + \langle 0.012, 0.064\rangle]\\ =\ &amp;\alpha\langle 0.12, 0.08 \rangle\\ =\ &amp;\langle 0.6, 0.4 \rangle \end{align*}​$$</p>
<ul>
<li><p><strong>Beware the notation!</strong></p>
<p>If the first letter of a random variable is capital, it indicates the probability distribution of that random variable written between angle brackets. Often used for boolean variables like $\Pr(A) = \langle \Pr(a), \Pr(\lnot a) \rangle$ but can be used for any as long as the order of the values of the random variable is well defined.</p></li>
<li><p>Notice that the result is same as calculating the conditional probability below</p>
<p>$$\begin{align*} &amp;\Pr(\text{Cavity} \mid \text{toothache})\\ =\ &amp;\frac{\Pr(\text{Cavity} \land \text{toothache})}{\Pr(\text{toothache})}\\ =\ &amp;\frac{\langle 0.108 + 0.012,\ 0.016 + 0.064 \rangle}{0.108 + 0.016 + 0.012 + 0.064}\\ =\ &amp;\langle 0.6, 0.4 \rangle \end{align*}$$</p></li>
</ul></li>
</ul></li>
</ul>
<h3 id="a-general-inference-procedure">A General Inference Procedure</h3>
<ul>
<li>Let $X$ be a query variable, $E$ set of (observed) evidence variables and $e$ their <em>observed</em> values, $Y​$ remaining unobserved variables.
<ul>
<li>Note that $X$, $E$ and $Y$ constitute <em>complete set of variables</em>; <em>i.e.</em> $\Pr(x, e, y)$ is simply a single probability (entry) in the JPD.</li>
</ul></li>
<li><strong>Query evaluation:</strong> $\Pr(X \mid e) = \alpha\Pr(X, e) = \alpha\sum_y \Pr(X, e, y)$
<ul>
<li>We used, in order, normalisation and marginalisation.</li>
<li>This is only theoretically relevant since it requires $O(2^n)$ steps (and entries) for $n$ Boolean variables.</li>
<li>Basically all methods we will talk about deal with tackling this problem!</li>
</ul></li>
</ul>
<h3 id="independence">Independence</h3>
<ul>
<li>Two random variables are called <strong>independent</strong> when they are <strong>uncorrelated</strong>.
<ul>
<li><strong>But two uncorrelated variables can be dependent.</strong></li>
</ul></li>
<li>Precisely, two random variables $X$, $Y$ are called independent iff
<ul>
<li>$\Pr(X \mid Y) = \Pr(X)$, or</li>
<li>$\Pr(Y \mid X) = \Pr(Y)$, or</li>
<li>$\Pr(X, Y) = \Pr(X)\Pr(Y)$.</li>
</ul></li>
<li><strong>Independence assumptions</strong> (regardless of whether they are true or not) can help to dramatically reduce complexity.
<ul>
<li>Independence assumptions might indeed be <em>necessary</em> even when they are not entirely justified, so as to make probabilistic reasoning in the domain practical.</li>
</ul></li>
</ul>
<h3 id="bayes-rule">Bayes' Rule</h3>
<p><strong>Bayes' rule</strong> is derived by writing the product rule in two forms and equating them:</p>
<p>$$\begin{align*} \Pr(a \land b) &amp;= \Pr(a \mid b)\Pr(b)\\ \Pr(a \land b) &amp;= \Pr(b \mid a)\Pr(a)\\ \therefore \Pr(b \mid a) &amp;= \frac{\Pr(a \mid b)\Pr(b)}{\Pr(a)} \end{align*}​$$</p>
<p>General case for multivaried variables using <strong>background evidence</strong> $e$:</p>
<p>$$\Pr(Y \mid X, e) = \dfrac{\Pr(X \mid Y, e) \Pr(Y \mid e)}{\Pr(X \mid e)}$$</p>
<p>It is useful because often we have good estimates for three terms on the right and are interested in the fourth.</p>
<h4 id="applying-bayes-rule">Applying Bayes' Rule</h4>
<ul>
<li><p>We can avoid calculating the probability of evidence (<em>i.e.</em> the denominator) by using normalisation:</p>
<p>$\Pr(M \mid s) = \alpha \langle \Pr(s\mid m)\Pr(m),\ \Pr(s\mid \lnot m)\Pr(\lnot m) \rangle$</p>
<ul>
<li>Usefulness of this depends on whether $\Pr(s \mid \lnot m)$ is easier to calculate than $\Pr(s)​$ (the denominator).</li>
</ul></li>
<li><p>Diagnostic knowledge (from symptoms to causes) is often <strong>fragile</strong> (<em>e.g.</em> $m$ for meningitis and $s$ for stiff neck, $\Pr(m \mid s)$ will go up if $\Pr(m)$ goes up due to epidemic.)</p></li>
</ul>
<h3 id="combining-evidence">Combining Evidence</h3>
<ul>
<li><p>Attempting to use additional evidence is easy in the JPD model</p>
<p>$\Pr(\text{Cavity} \mid \text{toothache} \land \text{catch}) = \alpha \langle 0.108, 0.016 \rangle \approx \langle 0.871, 0.129 \rangle​$</p>
<p>but requires additional knowledge in Bayesian model:</p>
<p>$\Pr(\text{Cavity} \mid \text{toothache} \land \text{catch}) = \alpha\Pr(\text{toothache} \land \text{catch} \mid \text{Cavity})\Pr(\text{Cavity})​$</p>
<ul>
<li><p><strong>Problem:</strong> this is basically almost as hard as JPD calculation.</p></li>
<li><p><strong>Solution:</strong> refine the idea of independence; $\text{Tootache}$ and $\text{Catch}$ are independent given presence/absence of $\text{Cavity}$ (both caused by cavity, and have no effect on each other)</p>
<p>$\Pr(\text{toothache} \land \text{catch} \mid \text{Cavity}) = \Pr(\text{toothache} \mid \text{Cavity})\Pr(\text{catch} \mid \text{Cavity})$</p></li>
</ul></li>
<li><p><strong>What is "JPD model" and "Bayesian model"?</strong></p>
<ul>
<li>JPD model uses table look-ups straight away whereas Bayesian model decomposes the query into other probabilities, which then can be looked-up in <em>smaller</em> JPD tables. Keep reading for more details.
<ul>
<li>The advantage is that tables grow at a decreased rate and so our look-up times &amp; memory requirements.</li>
</ul></li>
</ul></li>
</ul>
<h5 id="conditional-independence">Conditional Independence</h5>
<ul>
<li><p>Two variables $X$ and $Y$ are conditionally independent given $Z$ iff</p>
<p>$\Pr(X, Y \mid Z) = \Pr(X \mid Z)\Pr(Y \mid Z)$</p>
<ul>
<li>Equivalently:
<ul>
<li>$\Pr(X \mid Y, Z) = \Pr(X \mid Z)$</li>
<li>$\Pr(Y \mid X, Z) = \Pr(Y \mid Z)$</li>
</ul></li>
</ul></li>
<li><p>As before, conditional independence allows us to <strong>decompose large JPD tables into smaller ones</strong>, and grows $O(n)$ instead of $O(2^n)$.</p>
<ul>
<li>This is what makes probabilistic reasoning methods scalable at all!</li>
</ul></li>
<li><p><strong>Conditional independence assumptions are much more often reasonable than <em>absolute independence</em> assumptions.</strong></p>
<ul>
<li><p><strong>Naive Bayes model:</strong></p>
<p>$$\Pr(\text{Cause}, \text{Effect}_1, \ldots, \text{Effect}_n) = \Pr(\text{Cause})\prod_i \Pr(\text{Effect}_i \mid \text{Cause}) $$</p>
<ul>
<li>Based on the idea that <strong>all effects are conditionally independent given the cause variable</strong>.</li>
<li>Also called <strong>Bayesian Classifier</strong> or (by some) even <strong>"Idiot" Bayes Model</strong>.</li>
<li>Works surprisingly well in many domains despite its simplicity!</li>
</ul></li>
</ul></li>
<li><p><strong>Bayes (when taken with some independence assumptions) allows us to decompose an otherwise-single JPD table into smaller ones.</strong></p>
<ul>
<li>I know, I repeated myself but it's important!</li>
</ul></li>
</ul>
<h2 id="14-march-2019">14 March 2019</h2>
<h3 id="bayesian-networks">Bayesian Networks</h3>
<ul>
<li><p>Full Joint Probability Distributions (JPD) can become <em>intractably</em> large very quickly.</p></li>
<li><p>Conditional independence helps to reduce the number of probabilities required to specify the JPD (<em>i.e.</em> what we call "The Bayesian Model").</p></li>
<li><p>We are introducing <strong>Bayesian Networks (BN)</strong> to <strong>systematically</strong> describe dependencies between random variables.</p></li>
<li><p>Roughly speaking, Bayesian Networks are graphs that connect <strong>nodes representing variables</strong> with each other <strong>whenever they depend on each other</strong>.</p></li>
<li><p>A BN is a <strong>Directed Acyclic Graph (DAG)</strong> with nodes <em>annotated with probability information (tables)</em></p></li>
<li><p>The nodes represent random variables (discrete or continuous).</p></li>
<li><p><strong>Directed edges</strong> connect nodes.</p>
<ul>
<li>If there is an arrow from $X$ to $Y$, we call $X$ a <strong>parent</strong> of $Y$.</li>
<li>Each node $X_i$ has a <strong>Conditional Probability Distribution (CPD)</strong> attached to it.</li>
<li><strong>The CPD describes how $X_i$ depends on its parents.</strong>
<ul>
<li>In other words, there is a table with rows for each $\Pr(X_i \mid \text{Parents}(X_i))$.
<ul>
<li>Each row in CPTs contains a <strong>conditioning case</strong> (<em>i.e.</em> a configuration of the parent values).</li>
<li>Thus there would be $2^n$ rows, for $n$ being the number of parents.</li>
<li><strong>Beware that the probabilities in the table do not have to sum to 1!</strong>
<ul>
<li>If you think so, it's probably due to a grave misunderstanding of the underlying concept.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>Topology of graphs describes conditional independence relationships.</strong></p>
<ul>
<li><p>Intuitively, <strong>links describe direct effects</strong> of variables on each other in the domain.</p></li>
<li><p><strong>Assumption: anything that is not directly connected does not directly depend on each other.</strong></p>
<ul>
<li><p>For example:</p>
<p><img src="./2D.assets/1554573068773.png" alt="1554573068773" /></p></li>
</ul></li>
</ul></li>
</ul>
<h4 id="an-example">An Example</h4>
<p><img src="./2D.assets/1554573308955.png" alt="1554573308955" /></p>
<p><strong>Note that:</strong></p>
<ul>
<li>We don't model the perception of earthquake by John or Mary (as it is largely irrelevant to our purposes.)</li>
<li><strong>No <em>explicit</em> modelling</strong> of other things (phone ring or loud music) confusing John or Mary (we instead <strong>summarise in uncertainty</strong> regarding their actions.)
<ul>
<li>The uncertainties are $\Pr(\text{JohnCalls} \mid \lnot\text{A})$ and $\Pr(\text{MaryCalls} \mid \lnot\text{A})$ (the following can be included too $\Pr(\lnot\text{JohnCalls} \mid \text{A})$ and $\Pr(\lnot\text{MaryCalls} \mid \text{A})$.)</li>
<li>Indeed, the uncertainty summarises <em>any</em> kind of failure.
<ul>
<li>It's often almost impossible to enumerate all possible causes.</li>
<li>...and we don't have to estimate their <em>individual</em> probabilities anyway!</li>
</ul></li>
</ul></li>
</ul>
<h4 id="semantics-of-bayesian-networks">Semantics of Bayesian Networks</h4>
<ul>
<li><p>Bayesian Networks can be viewed in two different ways:</p>
<ul>
<li><p><strong>BN as a representation of JPD</strong></p>
<p>Useful for constructing BNs.</p></li>
<li><p><strong>BNs as a collection of conditional independence statements</strong></p>
<p>Useful for designing <em>inference</em> procedures.</p></li>
</ul></li>
<li><p>Every entry $\Pr(X_1 = x_1 \land \ldots \land X_n = x_n)$ in the JPD can be calculated using a BN:</p>
<ul>
<li><p>$\Pr(x_1, \ldots, x_n) = \prod^n_{i=1} \Pr(x_i \mid \text{Parents}(X_i))$</p></li>
<li><p>As before, <strong>this can be used to answer any query.</strong></p></li>
<li><p>The proof relies on the repeated application of the <strong>product rule</strong></p>
<p>$\Pr(x_1, \ldots, x_n) = \Pr(x_n \mid x_{n-1}, \ldots, x_1) \Pr(x_{n-1}, \ldots, x_1)$</p>
<p>which is called the <strong>chain rule</strong></p>
<p>$\Pr(x_1, \ldots, x_n) = \Pr(x_n \mid x_{n-1}, \ldots, x_1) \Pr(x_{n-1} \mid x_{n-2}, \ldots, x_1) \cdots \Pr(x_2 \mid x_1) \Pr(x_1)$.</p>
<ul>
<li><p>And given the chain rule, all we need is an <strong>appropriate labelling</strong> of the nodes such that our calculations using Bayesian Network simply translates into applications of the chain rule.</p>
<ul>
<li><p>"Appropriate" being</p>
<p>$\Pr(X_i \mid X_{i-1}, \ldots, X_1) = \Pr(X_i \mid \text{Parents}(X_i))$ thus $\text{Parents}(X_i) \subseteq { X_{i-1}, \ldots, X_1 }$.</p></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Bayesian Networks are example of <strong>locally structured (sparse)</strong> systems: subcomponents only interact with small number of other components.</p>
<ul>
<li><em>E.g.</em> For 30 random variables where each depends on 5 other, its BN will have $30 \times 2^5 = 960$ probabilities stored in the CPDs, whilst a JPD would have $2^30 \approx 1000^3$ entries!</li>
<li>Remember that <strong>this is based on designer's independence assumptions</strong>.</li>
</ul></li>
<li><p>To determine a good BN structure is not a trivial task!</p>
<ul>
<li>When in doubt, add "root causes" first, and then the variables they influence and so on, until you reach "leaves" which have no influence on other variables (<em>i.e.</em> follow a breadth-first top-down strategy in building the graph.)</li>
</ul></li>
</ul>
<h5 id="topological-semantics-of-conditional-independence">Topological Semantics of Conditional Independence</h5>
<p><img src="./2D.assets/1554575139021.png" alt="1554575139021" /></p>
<ol>
<li><p>A node is conditionally independent of its <strong>non-descendants</strong>, given its parents.</p></li>
<li><p>A node is conditionally independent of <strong>all other nodes</strong>, given its <strong>Markov Blanket</strong>, <em>i.e.</em></p>
<ol>
<li>its <strong>parents</strong></li>
<li>its <strong>children</strong></li>
<li>its <strong>children's parents</strong></li>
</ol>
<ul>
<li>This is also a perfect example to explain the difference between independence and causation: Given a node (let's call it "our node")...
<ul>
<li>It's clear that the children and the children's parents do not "affect" our node (unless the children's parents are the parents of our node too).</li>
<li>Yet knowing their value, we can have a better estimate for the probability of our node.</li>
</ul></li>
</ul></li>
</ol>
<h4 id="even-more-efficient-representation-of-cpd">(Even More) Efficient Representation of CPD</h4>
<ul>
<li>Even $2^n$ ($n$ parents) conditioning cases require
<ul>
<li>a great deal of experience &amp; knowledge of the domain</li>
<li>lots of space</li>
</ul></li>
<li>Completely arbitrary relationships are unlikely, probabilities are often describable by <strong>canonical distributions</strong> that fit some standard pattern (Poisson, Gaussian/Normal, ...)</li>
<li>By <em>specifying the pattern</em> by a few parameters, we can save a lot of space!</li>
<li>The simplest case would be a <strong>deterministic node</strong> whose value can be directly inferred from the values of its parents.
<ul>
<li>For example logical or mathematical functions.</li>
</ul></li>
</ul>
<h5 id="noisy-or-relationships">Noisy-OR relationships</h5>
<ul>
<li>Any cause <strong>can</strong> make effect true, bot won't <strong>necessarily</strong> (which is then called <strong>effect inhibited</strong>)
<ul>
<li>$\Pr(\text{Effect} \mid \text{Cause}) &lt; 1$</li>
</ul></li>
<li>Assumes all causes are listed (a <strong>leak node</strong> can be used to cater for "miscellaneous" unlisted causes).</li>
<li>Also assumes <strong>inhibitions are mutually conditionally independent</strong>.
<ul>
<li>Whatever inhibits from $\text{Cause}_1$ making $\text{Effect}$ true is independent of what inhibits $\text{Cause}_2$ from making $\text{Effect}$ true.</li>
</ul></li>
<li>So $\text{Effect}$ is <em>false</em> only if each of its <em>true</em> parents are inhibited and we can compute this likelihood from product of probabilities for each individual cause inhibiting $\text{Effect}$.</li>
</ul>
<p>Example</p>
<ul>
<li><img src="./2D.assets/1554575758299.png" alt="1554575758299" /></li>
<li><img src="./2D.assets/1554575792669.png" alt="1554575792669" />
<ul>
<li>Note well that we calculate $\Pr(\lnot\text{Fever})$ directly (instead of $\Pr(\text{Fever})$).</li>
<li>Again, note well that we assume all causes are listed (thus $\Pr(\lnot\text{Fever} \mid \lnot\text{Cold}, \lnot\text{Flu}, \lnot\text{Malaria})$ is equal to 1).</li>
<li>Values in bold are <strong>inhibition probabilities</strong>.</li>
<li>Realise that inhibition probabilities alone are sufficient to encode the CPT table (<em>i.e.</em> requiring $k$ instead of $2^k$ values)!</li>
</ul></li>
</ul>
<h4 id="bns-with-continuous-variables">BNs with Continuous Variables</h4>
<ul>
<li>Often variables range over continuous domains.</li>
<li><strong>Discretisation</strong> is one possible solution, but often leads to inaccuracy or requires a lot of discrete values.
<ul>
<li>Due to our ignorance of the domain.</li>
<li>Many parameters: slice length (equal or logarithmic?), spacing (again; equal, logarithmic, or something else?), <em>etc.</em></li>
</ul></li>
<li>A better solution is to use <strong>canonical distributions</strong> specified in terms of a few parameters.</li>
<li><strong>Hybrid Bayesian Networks</strong> use a mixture of discrete and continuous variables (with special methods to deal with links between different types -- not discussed here).</li>
</ul>
<h2 id="15-march-2019">15 March 2019</h2>
<h3 id="exact-inference-in-bayesian-networks">Exact Inference in Bayesian Networks</h3>
<ul>
<li><p><strong>Basic Task</strong></p>
<p>Compute <em>posterior</em> distribution for set of <em>query variables</em> given some <em>observed event</em> (<em>i.e.</em> assignment of values to <em>evidence variables</em>.)</p></li>
<li><p><strong>Formally</strong></p>
<p>Compute $\Pr(X \mid e)$ given query variables $X$, evidence variables $E$ (and non-evidence or <em>hidden</em> variables $Y$.)</p></li>
<li><p>First we will discuss exact (but slow) algorithms for computing posterior probabilities, then approximate methods later.</p></li>
</ul>
<h4 id="exact-inference-by-enumeration">Exact Inference by Enumeration</h4>
<ul>
<li>We have seen that any conditional probability can be computed from a full JPD by summing terms
<ul>
<li>$\Pr(X \mid e) = \alpha\Pr(X, e) = \alpha\sum_y\Pr(X, e, y)$</li>
</ul></li>
<li>Since BN gives a complete representation of full JPD, we should be able to answer a query by computing sums of (logical) "products" (<em>i.e.</em> AND) of conditional probabilities from the BN.</li>
</ul>
<p><strong>Example</strong></p>
<p><img src="./2D.assets/1554573308955.png" alt="1554573308955" /></p>
<ul>
<li><p>Consider query</p>
<p>$\Pr(\text{Burglary} \mid \text{JohnCalls} = \top, \text{MaryCalls} = \top) = \Pr(B \mid j, m)$</p>
<ul>
<li>$\Pr(B \mid j, m) = \alpha\Pr(B, j, m) = \alpha\sum_e\sum_a\Pr(B, e, a, j, m)$
<ul>
<li><em>Marginalisation</em> &amp; <em>Normalisation</em></li>
</ul></li>
</ul></li>
<li><p>We can use CPTs to simplify this by exploiting the BN structure.</p></li>
<li><p>For example, for $\text{Burglary} = \top$:</p>
<p>$\Pr(b \mid j,m) = \alpha\sum_e\sum_a\Pr(b)\Pr(e)\Pr(a \mid b,e)\Pr(j\mid a)\Pr(m\mid a)$</p>
<ul>
<li><p>In this example, note...</p>
<ul>
<li>How we wrote marginalisation in a top-down manner. Of course the order would not matter as multiplication is commutative, but it will help you greatly to follow systematic approach in the exam.</li>
</ul></li>
<li><p>We can improve efficiency of this further by moving terms outside that don't depend on sums</p>
<p>$\Pr(b \mid j,m) = \alpha\Pr(b)\sum_e\Pr(e)\sum_a\Pr(a \mid b,e)\Pr(j\mid a)\Pr(m\mid a)$</p>
<ul>
<li>To compute this, we need to loop through random variables in order and multiply CPT entries (<em>i.e.</em> probabilities); for each summation we need to loop over the possible values of random variables.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="the-variable-elimination-algorithm">The Variable Elimination Algorithm</h4>
<ul>
<li><p>Enumeration method is computationally quite hard.</p></li>
<li><p>You often compute the same thing several times:</p>
<p><em>e.g.</em> $\Pr(j \mid a)\Pr(m \mid a)$ and $\Pr(j \mid \lnot a)\Pr(m \mid \lnot a)$ for each value of e.</p></li>
<li><p>Evaluation of expression shown in the following tree:</p>
<p><img src="2D.assets/1554625610650.png" alt="1554625610650" /></p>
<ul>
<li>Note how the last two probabilities are calculated twice for each top-most branch!</li>
</ul></li>
<li><p><strong>The Idea of <em>Variable Elimination</em>:</strong> Avoid repeated calculations!</p>
<ul>
<li>The trick is to <em>store results</em> after doing calculations once.</li>
<li>Works <strong>bottom-up</strong> by evaluating sub-expressions.</li>
</ul></li>
<li><p>Assume we want to evaluate</p>
<p><img src="2D.assets/1554625749279.png" alt="1554625749279" /></p>
<ul>
<li><p>We have annotated each part with a <strong>factor</strong>.</p></li>
<li><p><strong>A <em>factor</em> is a matrix</strong>, indexed with its argument variables. <em>E.g.</em></p>
<ul>
<li><p>Factor $f_5(A = \top)$ corresponds to $\Pr(m \mid a)$ and depends just on $A$ because $m$ is fixed (thus it's a $2 \times 1$ matrix).</p>
<p>$f_5(A) = \langle \Pr(m \mid a),\ \Pr(m \mid \lnot a) \rangle$</p></li>
<li><p>Factor $f_3(A, B, E)$ is a $2 \times 2 \times 2$ matrix for $\Pr(a \mid B, e)$.</p></li>
</ul></li>
<li><p><strong>Summing out</strong> $A$ produces a $2 \times 2$ matrix via <strong>pointwise product</strong></p>
<p>$$\begin{align*} f_6(B, E) &amp;= \sum_a f_3(A, B, E) \times f_4(A) \times f_5(A)\\ &amp;=f_3(a, B, E) \times f_4(a) \times f_5(a) + f_3(\lnot a, B, E) \times f_4(\lnot a) \times f_5(\lnot a) \end{align*}$$</p>
<ul>
<li><strong>Any factor that does not depend on the variable to be summed out can (and should) be moved outside the summation process!</strong></li>
</ul></li>
<li><p>So we had</p>
<p>$\Pr(B \mid j, m) = \alpha f_1(B) \times \sum_e f_2(E) \times f_6(B, E)​$</p></li>
<li><p>Summing out $E$ the same way</p>
<p>$f_7(B) = f_2(e) \times f_6(B, e) + f_2(\lnot e) \times f_6(B, \lnot e)$</p></li>
<li><p>Using $f_1(B) = \Pr(B)$, we can finally compute</p>
<p>$\Pr(B \mid j, m) = \alpha f_1(B) \times f_7(B)$</p></li>
<li><p>Remains to define pointwise product (and thus summing out [even though by now we have an intuitive understanding of it.])</p></li>
</ul></li>
</ul>
<h5 id="pointwise-product">Pointwise Product</h5>
<ul>
<li>Pointwise Product yields <strong>a product for union of variables</strong> in its arguments.</li>
<li>Matrices are multiplied only when we need to sum out a variable from the accumulated product.</li>
</ul>
<p><strong>Example:</strong></p>
<p><img src="2D.assets/1554626799089.png" alt="1554626799089" /></p>
<ul>
<li><p>$f(\top, \top, \bot) = f_1(\top, \top) \times f_2(\top, \bot)$</p></li>
<li><p>Beware that the second argument of $f_1$ and the first argument of $f_2$ are the same thus the following is illegal/undefined:</p>
<p>$f_1(\top, \top) \times f_2(\bot, \top)$</p></li>
<li><p>After variables are unified, the rest is a very straightforward multiplication!</p></li>
</ul>
<p><strong>Tips:</strong></p>
<ul>
<li>We keep saying "matrices" but indeed you should stick to tables whenever you can because it's simply much easier to follow.</li>
</ul>
<h5 id="an-example-1">An Example</h5>
<p><img src="./2D.assets/1554573308955.png" alt="1554573308955" /></p>
<p><img src="2D.assets/1554627268137.png" alt="1554627268137" /></p>
<ul>
<li><strong>Notice how we use $\alpha'$, instead of keep using $\alpha$, for factors.</strong></li>
</ul>
<h2 id="19-march-2019">19 March 2019</h2>
<h3 id="approximate-inference-in-bayesian-networks">Approximate Inference in Bayesian Networks</h3>
<ul>
<li>Exact inference is computationally very expensive.</li>
<li>Approximate methods are thus important, here we consider randomised <strong>sampling</strong> algorithms (also called <strong>Monte Carlo (MC)</strong> algorithms.)</li>
<li>We will talk about two types of Monte Carlo algorithms:
<ol>
<li><strong>Direct Sampling Methods</strong></li>
<li><strong>Markov Chain Sampling</strong></li>
</ol></li>
</ul>
<h4 id="direct-sampling-methods">Direct Sampling Methods</h4>
<ul>
<li><p><strong>Basic Idea</strong></p>
<p>Generate samples from a known probability distribution.</p></li>
<li><p>If we consider an unbiased coin as a random variable, sampling from the distribution is like flipping the coin.</p></li>
<li><p>It is possible to sample any distribution <em>on a single variable</em> given a set of random numbers from $[0, 1]$.</p></li>
<li><p><strong>Simplest method:</strong> generate events from network without requiring evidence</p>
<ul>
<li><strong>Sample each variable in "topological order"</strong> (<em>i.e.</em> top-down)</li>
<li>Probability distribution for sampled value is conditioned on values assigned to parents.</li>
</ul></li>
</ul>
<p><strong>Example</strong></p>
<p><img src="2D.assets/1554629666461.png" alt="1554629666461" /></p>
<ul>
<li>Direct Sampling process:
<ul>
<li>Sample from $\Pr(\text{Cloudy}) = \langle 0.5, 0.5 \rangle$, suppose this returns <em>true</em>.</li>
<li>Sample from $\Pr(\text{Sprinkler} \mid \text{Cloudy} = \top) = \langle 0.1, 0.9 \rangle$, suppose this returns <em>false</em>.
<ul>
<li>Beware that CPD tables do <em>not</em> include negation of the query, thus $\Pr(\text{Sprinkler} \mid \text{Cloudy} = \top) = \langle 0.1, 0.5 \rangle$ is a grave mistake!</li>
</ul></li>
<li>Sample from $\Pr(\text{Rain} \mid \text{Cloudy} = \top) = \langle 0.8, 0.2 \rangle$, suppose this returns <em>true</em>.</li>
<li>Sample from $\Pr(\text{WetGrass} \mid \text{Sprinkler} = \top, \text{Rain} = \top) = \langle 0.9, 0.1 \rangle$, suppose this returns true.</li>
</ul></li>
<li>Event returned $[\text{Cloudy}, \text{Sprinkler}, \text{Rain}, \text{WetGrass}] = [\top, \bot, \top, \top]$.</li>
</ul>
<p><strong>Direct Sampling Continued</strong></p>
<ul>
<li><p>Generate samples with probability $\text{S}(x_1, \ldots, x_n) = \Pr(x_1, \ldots, x_n)$, <em>i.e.</em> in accordance with the distribution.</p></li>
<li><p>Answers are computed by counting the number $\text{N}(x_1, \ldots, x_n)$ of the times event $x_1, \ldots, x_n$ was generated and dividing by total number $N$ of all samples.</p></li>
<li><p>In the limit, we should get</p>
<p>$$\lim_{s\rightarrow\infty} \dfrac{\text{N}(x_1, \ldots, x_n)}{N} = \text{S}(x_1, \ldots, x_n) = \Pr(x_1, \ldots, x_n)$$</p></li>
<li><p>If the estimated probability $\hat \Pr $ becomes exact in the limit, we call the estimate <strong>consistent</strong> and we write $\approx$ in this sense</p>
<ul>
<li><em>E.g.</em> $\Pr(x_1, \ldots, x_n) \approx \text{N}(x_1, \ldots, x_n) / N$</li>
</ul></li>
</ul>
<h4 id="rejection-sampling">Rejection Sampling</h4>
<ul>
<li><p>The purpose is to <strong>produce samples for hard-to-sample distribution from an easy-to-sample distribution</strong>.</p></li>
<li><p><strong>The method</strong></p>
<ul>
<li><p>To determine $\Pr(X \mid e)$, generate samples from the prior distribution specified by the BN first.</p></li>
<li><p><strong>Then reject those that do not match the evidence.</strong></p></li>
<li><p>The estimate $\hat \Pr(X=x\mid e)$ is obtained by counting how often $X = x$ occurs in the <em>remaining</em> samples.</p></li>
<li><p><strong>Rejection sampling is consistent</strong> because by definition:</p>
<p>$$\hat \Pr(X \mid e) = \dfrac{\text{N}(X, e)}{\text{N}(e)} \approx \dfrac{\Pr(X, e)}{\Pr(e)} = \Pr(X \mid e)$$</p></li>
</ul></li>
<li><p>Rejection sampling rejects too many samples that are not consistent with $e$.</p>
<ul>
<li>Rejections increase <em>exponential</em> in number of evidence variables!</li>
<li>Thus it can be quite wasteful and therefore computationally expensive (what we were trying to avoid in the first place.)</li>
<li>It gets also worse as evidence gets <em>more rare</em>.</li>
</ul></li>
<li><p>Not really usable (similar to naively estimating conditional probabilities from observation.)</p></li>
</ul>
<h4 id="likelihood-weighting">Likelihood Weighting</h4>
<ul>
<li>Avoids the inefficiency of rejecting sampling by generating only the samples which are <em>consistent</em> with evidence.</li>
<li>Fixes the values for evidence variables $E$ and samples only the remaining variables $X$ and $Y$.</li>
<li>Since not all events are equally probable, each event has to be weighted by its <strong>likelihood that accords to the evidence</strong>.</li>
<li><strong>Likelihood is measured by product of conditional probabilities for each evidence variable, given its parents.</strong></li>
</ul>
<p><strong>Example</strong></p>
<p><img src="2D.assets/1554629666461.png" alt="1554629666461" /></p>
<p><img src="2D.assets/1554631206112.png" alt="1554631206112" /></p>
<ul>
<li>Tally is the keyword here.
<ul>
<li>Usually the tally-weight is equal to 1, meaning each sample counts as 1, whereas here the weight is in range $[0, 1]$.</li>
</ul></li>
<li>The weight is <em>updated</em> with the probability of each evidence variable.</li>
<li><strong>You should still the topological order of the graph.</strong></li>
</ul>
<p><strong>Why It Works?</strong></p>
<p><img src="2D.assets/1554632224019.png" alt="1554632224019" /></p>
<ul>
<li><p>$\text{S}(z, e)$ is the ratio of the number of samples with desired query to the total number of samples.</p></li>
<li><p>$\text{w}(z, e)$ is the tally-weight.</p></li>
<li><p>$\text{S}(z, e) = \prod^l_{i=1}\Pr(z_i \mid \text{Parents}(Z_i))​$</p>
<p>$\text{S}$'s sample values for $Z_i$ is influenced by the evidence among $Z_i$'s parents.</p></li>
<li><p>But $\text{S}$ pays no attention when sampling $Z_i$'s value to evidence from $Z_i$'s non-parents; so it's not sampling from the true posterior probability distribution.</p></li>
<li><p>But the <strong>likelihood weight</strong> $w$ makes up for the difference between the actual and desired sampling distributions.</p></li>
<li><p>Beware that $z$ is (the set of) all random variables other than the evidence.</p></li>
</ul>
<p><strong>Problems</strong></p>
<ul>
<li>Most samples will have very small weights as the number of evidence variables increase.</li>
<li>These will be dominated by tiny fraction of samples that accord more than infinitesimal likelihood to the evidence.
<ul>
<li>Which would require us to sample <em>a lot</em> to a get an accurate representation of rare cases.</li>
<li>Indeed, this is a generic problem with most sampling methods.</li>
</ul></li>
</ul>
<h3 id="the-markov-chain-monte-carlo-mcmc-algorithm">The Markov Chain Monte Carlo (MCMC) Algorithm</h3>
<ul>
<li><p>The algorithm: Create an event from a <em>previous event</em>, rather than generating all events from scratch.</p>
<ul>
<li>Helpful to think of the BN as having a <strong>current state</strong> specifying a value for each variable.</li>
</ul></li>
<li><p><strong>Consecutive state is generated by sampling a value for one of the non-evidence variables $X_i$ conditioned on the current values of variables in the <em>Markov Blanket</em> of $X_i$</strong>.</p>
<p><img src="2D.assets/1554632679966.png" alt="1554632679966" /></p></li>
<li><p><strong>Algorithm randomly wanders around state space, flipping one variable at a time and keeping evidence variables fixed.</strong></p></li>
<li><p>MCMC is a <em>very powerful</em> method used for all kinds of things involving probabilities.</p></li>
</ul>
<p><strong>Example</strong></p>
<p><img src="2D.assets/1554629666461.png" alt="1554629666461" /></p>
<p><img src="2D.assets/1554633012953.png" alt="1554633012953" /></p>
<p>Simply:</p>
<ul>
<li>Fix evidence variables to their observed values.</li>
<li>Initialise other (also called <em>hidden</em>) variables randomly.</li>
<li>Wander around (<em>i.e.</em> iterate through) the hidden variables again and again, "flipping" them based on their <em>Markov Blanket</em>.</li>
</ul>
<p><strong>Why It Works?</strong></p>
<p>Basic idea of proof that MCMC is consistent is as follows:</p>
<blockquote>
<p>The sampling process settles into a "dynamic equilibrium" in which the long-term fraction of time spent in each state is exactly proportional to its posterior probability.</p>
</blockquote>
<h2 id="21--22-march-2019">21 &amp; 22 March 2019</h2>
<h3 id="time-and-dynamic-environments">Time and Dynamic Environments</h3>
<ul>
<li>So far we have only seen methods for describing uncertainty in static environments.</li>
<li>Every variable had a fixed value, and <strong>we assumed that nothing changes during evidence collection or diagnosis</strong>.</li>
<li>Basic idea: Imagine <strong>one BN model</strong> (of the problem) <strong>for every time step</strong> and reason about <strong>changes between them</strong>.</li>
</ul>
<h4 id="states-and-observations">States and Observations</h4>
<ul>
<li>Series of <em>snapshots</em> (<strong>time slices</strong>) will be used to describe process of change.</li>
<li>Snapshots consist of <strong>observable</strong> random variables $E_t$ and <strong>non-observable</strong> ones $X_t$ where $t$ is the time.</li>
<li>For simplicity, <strong>we assume sets of observable &amp; non-observable variables remain constant over time</strong>, but this is not necessary.
<ul>
<li><em>I.e.</em> There won't be new random variables introduced over time.</li>
</ul></li>
<li>Observation at $t$ will be $E_t = e_t$ for some set of values $e_t$.</li>
<li>Assume that states start at $t = 0$ and evidence starts arriving at $t = 1$.</li>
</ul>
<h4 id="stationary-processes-and-the-markov-assumption">Stationary Processes and The Markov Assumption</h4>
<ul>
<li><p>"How do we specify dependencies among variables?"</p></li>
<li><p>It's natural to arrange them in <strong>temporal order</strong> (<em>i.e.</em> causes usually precede effects.)</p>
<ul>
<li>Don't causes always precede effects, <em>a priori</em>? Anyway...</li>
</ul></li>
<li><p>A problem is that the set of random variables is literally unbounded as there is (a different) one <em>for each time slice</em>!</p>
<ul>
<li><p>So we would have to</p>
<ol>
<li><p>specify unbounded number of conditional probability tables</p></li>
<li><p>specify an unbounded number of parents for each of these</p></li>
</ol></li>
<li><p>Solution to first problem:</p>
<p>We assume that changes are caused by a <strong>stationary process</strong> -- the laws that govern the process do not change themselves over time</p>
<ul>
<li>In other words, the laws of nature do not change over time.</li>
<li>This is not to be confused with static.</li>
<li><strong>Practically</strong>, this means that $\Pr(X_t \mid \text{Parents}(X_t))$ does not depend on $t$.</li>
</ul></li>
<li><p>Solution to second problem:</p>
<p><strong>Markov Assumption</strong> -- the current state depends only on a finite history of previous states.</p>
<ul>
<li><p>Such processes are called <strong>Markov Processes</strong> or <strong>Markov Chains</strong>.</p></li>
<li><p>The simplest form of which is called <strong>First-Order Markov Process</strong>, where every state depends only on predecessor state.</p>
<ul>
<li>We can write this as $\Pr(X_t \mid X_{0:t-1}) = \Pr(X_t \mid X_{t-1})$</li>
<li>This conditional distribution is called <strong>Transition Model</strong>.</li>
</ul></li>
<li><p>Difference between first-order and second-order Markov processes:</p>
<p><img src="2D.assets/1554637781589.png" alt="1554637781589" /></p></li>
</ul></li>
</ul></li>
<li><p>Additionally, we will assume that <strong>evidence variables depend only on current state</strong>.</p>
<p>$\Pr(E_t \mid X_{0:t}, E_{0:t-1}) = \Pr(E_t \mid X_t)$</p>
<ul>
<li>This is called the <strong>Sensor Model</strong> (or <strong>Observation Model</strong>) of the system.</li>
</ul></li>
<li><p>Notice the direction of dependence: <strong>state causes evidence but inference goes in other direction</strong>!</p>
<ul>
<li>In umbrella world, it's the rain that causes umbrella to appear (not the umbrella that causes the rainfall.)</li>
</ul></li>
<li><p>Finally, we need a prior <strong>distribution over initial states</strong> $\Pr(X_0)$.</p></li>
<li><p>These three distributions</p>
<ul>
<li>Transition Model</li>
<li>Sensor/Observation Model</li>
<li>Initial Prior Distribution</li>
</ul>
<p>give a complete specification of the JPD.</p>
<p>$\Pr(X_0, X_1, \ldots, X_t, E_1, \ldots,E_t) = \Pr(X_0) \prod^t_{i=1}\Pr(X_i \mid X_{i-1})\Pr(E_i \mid X_i)$</p></li>
<li><p>Umbrella World Example</p>
<p><img src="2D.assets/1554638275988.png" alt="1554638275988" /></p></li>
<li><p>If Markov assumptions seem to simplistic for some domains (and hence, inaccurate), two measures can be taken</p>
<ul>
<li>We can increase the order of the Markov process model
<ul>
<li><em>I.e.</em> increase the number of previous state the current state depends on.</li>
</ul></li>
<li>We can increase the set of state variables
<ul>
<li>For example, add information about season, pressure, or humidity.</li>
<li>Beware that this might also increase <strong>prediction requirements</strong> (if we have to predict them, problem alleviated if we add new sensors)</li>
</ul></li>
</ul></li>
</ul>
<h3 id="inference-tasks-in-temporal-models">Inference Tasks in Temporal Models</h3>
<ul>
<li><p>Now that we have described a general model, we need (to understand) inference methods for a number of tasks.</p></li>
<li><p><strong>Filtering/Monitoring</strong></p>
<p>Compute <strong>belief state</strong> given evidence to date, <em>i.e.</em> $\Pr(X_t \mid e_{1:t})$.</p></li>
<li><p><strong>Likelihood of Evidence Sequence</strong></p>
<p>Interestingly, an almost identical calculation yields the likelihood of the evidence sequence $\Pr(e_{1:t})$.</p></li>
<li><p><strong>Prediction</strong></p>
<p>Compute the posterior distribution over a future state given evidence to date, <em>i.e.</em> $\Pr(X_{t+k} \mid e_{1:t})$.</p></li>
<li><p><strong>Smoothing/Hindsight</strong></p>
<p>Compute posterior distribution of past state given evidence to date, <em>i.e.</em> $\Pr(X_k \mid e_{1:t})$ for $0 \le k &lt; t$.</p></li>
<li><p><strong>Most Likely Explanation</strong></p>
<p>Compute $$\begin{align*}\arg\max_{x_{1:t}} \Pr(x_{1:t} \mid e_{1:t})\end{align*}$$, <em>i.e.</em> the most likely <em>sequence</em> of states given evidence to date.</p></li>
</ul>
<h4 id="filteringmonitoring">Filtering/Monitoring</h4>
<ul>
<li><p>Done by <strong>recursive estimation</strong>: compute result for $t+1$ by doing it for $t$ and then updating with new evidence $e_{t+1}$. That is, for some function $f$:</p>
<p>$\Pr(X_{t+1} \mid e_{1:t+1}) = f(e_{t+1}, \Pr(X_t \mid e_{1:t}))$</p></li>
<li><p><img src="2D.assets/1554639449932.png" alt="1554639449932" /></p>
<ul>
<li><p><strong>Follow the steps carefully and make sure you understand.</strong></p>
<p>The probability of current state $X_t$ given $e_{1:t}$ evidence to date is</p>
<ul>
<li>the product of
<ul>
<li>the probability of observing current evidence given current state, and</li>
<li>the probability of transitioning from previous state to current state <strong>multiplied by</strong> the probability of previous state $X_{t-1}$ given $e_{1:t-1}$ evidence to its date
<ul>
<li><strong>summed over</strong> all possible values of a state.</li>
</ul></li>
</ul></li>
<li>and normalised.
<ul>
<li>Because the denominator is omitted whilst applying Bayes' Rule.</li>
<li>Also, note that $e_{1:t}​$ is <em>background evidence</em>.</li>
</ul></li>
</ul></li>
<li><p>As you can see, we reduced problem into recursive steps that now only require:</p>
<ul>
<li>$\Pr(e_{t+1} \mid X_{t+1})$ <em>i.e.</em> sensor model.</li>
<li>$\Pr(X_{t+1} \mid x_t)$, <em>i.e.</em> transition model.</li>
<li>$\Pr(X_0)$, <em>i.e.</em> initial state probabilities (for the base case of the recursion).</li>
<li>where $\Pr(x_t \mid e_{1:t})$ is the recursive bit.</li>
</ul></li>
</ul></li>
<li><p>We can view estimate $\Pr(X_t \mid e_{1:t})$ as a <strong>forward message</strong> $f_{1:t}$ propagated and updated through sequence.</p>
<ul>
<li>We write this process as $f_{1:t+1} = \alpha,\text{FORWARD}(f_{1:t}, e_{t+1})$.</li>
<li>Time and space requirements for this are constant regardless of length of sequence.
<ul>
<li>Since we "cache" only the last prediction (to be used for the next step).</li>
</ul></li>
<li>This is extremely important for agent design.</li>
</ul></li>
</ul>
<h4 id="prediction">Prediction</h4>
<ul>
<li><p>Prediction works like filtering but without new evidence.</p></li>
<li><p>Computation involves transition model only and not sensor model (<strong>until we start filtering</strong>).</p>
<p><img src="2D.assets/1554641080782.png" alt="1554641080782" /></p>
<ul>
<li><p><strong>Follow the steps carefully to make sure you understand.</strong></p>
<p>The probability of state $X_{t+k}$ given $e_{1:t}$ evidence to date is</p>
<ul>
<li>the sum (over all possible values of a state) of
<ul>
<li>the probability of transitioning from the previous state to current state <strong>multiplied by</strong> the probability of state $X_{t+k-1}$ given $e_{1:t}$ evidence to date.</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>As we predict further and further into the future, distribution converges to $\langle 0.5, 0.5 \rangle$.</p>
<ul>
<li>This is called <strong>Stationary Distribution</strong> of the Markov Process (the more uncertainty, the quicker it will converge.)</li>
</ul></li>
</ul>
<h4 id="likelihood-of-evidence-sequence">Likelihood of Evidence Sequence</h4>
<ul>
<li><p>We can use the above method to compute <strong><em>likelihood</em> of evidence sequence</strong> $\Pr(e_{1:t})$.</p></li>
<li><p>Useful to compare different temporal models (I have observed these evidences, thus the temporal model that assigns the highest probability to my observation is probably the best [<em>i.e.</em> highest likelihood theorem]).</p></li>
<li><p>Use a likelihood <strong>forward message</strong> $I_{1:t} = \Pr(X_t, e_{1:t})$ and compute</p>
<p>$I_{1:t+1} = \alpha,\text{FORWARD}(I_{1:t}, e_{t+1})$</p></li>
<li><p>Once we compute $I_{1:t}$, summing out yields likelihood</p>
<p>$$\begin{align*}L_{1:t} = \Pr(e_{1:t}) = \sum_{x_t} I_{1:t}(x_t, e_{1:t})\end{align*}$$</p></li>
<li><p>TODO: how to calculate this.</p></li>
<li><p>I struggle with FORWARD Messages too, kind of got it but have difficulty with the notation used.</p></li>
</ul>
<h4 id="smoothinghindsight">Smoothing/Hindsight</h4>
<p><img src="2D.assets/1554712017030.png" alt="1554712017030" /></p>
<ul>
<li><p>Smoothing is computation of distribution of past states given evidence to date, <em>i.e.</em> $\Pr(X_k \mid e_{1:t})$, for $1 \le k &lt; t$.</p></li>
<li><p>Easiest to view as a 2-step process (up to $k$, then $k+1$ to $t$)</p>
<p><img src="2D.assets/1554711771187.png" alt="1554711771187" /></p>
<ul>
<li><p><strong>Follow the steps carefully to make sure you understand.</strong></p>
<p>The probability of state $X_k$ given $e_{1:t}$ evidence to date is</p>
<ul>
<li>the product of
<ul>
<li>filtering on $X_k$ (<em>i.e.</em> $\Pr(X_k \mid e_{1:k})$, and</li>
<li>$\Pr(e_{k+1:t} \mid X_k)​$
<ul>
<li><em>I.e.</em> the probability of evidence sequence from $k+1$ to date given $X_k$.</li>
<li><img src="2D.assets/1554712597588.png" alt="1554712597588" />
<ul>
<li>The product of
<ul>
<li>the probability of transitioning from given state $X_k$ to next $x_{k+1}$</li>
<li>the probability of emitting $e_{k+1}$ in state $x_k+1$</li>
<li>recursive probability for $e_{k+2:t}$ and $x_{k+1}$</li>
<li><strong>summed over</strong> all possible values of $x_{k+1}$.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>and normalised.
<ul>
<li>Because the denominator is omitted whilst applying Bayes’ Rule.</li>
<li>Also, note that $e_{1:k}$ is <em>background evidence</em>.</li>
</ul></li>
</ul></li>
<li><p>Here <strong>backward message</strong> $b_{k+1:t} = \Pr(e_{k+1:t} \mid X_k)$ is analogous to forward message.</p>
<ul>
<li><p>Formula for backward message:</p>
<p><img src="2D.assets/1554712597588.png" alt="1554712597588" /></p>
<ul>
<li>First term $\Pr(e_{k+1} \mid x_{k+1})$ is <em>sensor model</em>.</li>
<li>Third term $\Pr(x_{k+1} \mid X_k)$ is <em>transition model</em>.</li>
<li>Second term $\Pr(e_{k+2}:t \mid x_{k+1})$ is the <em>recursive bit</em>.</li>
</ul></li>
<li><p>Define $b_{k+1:t} = \text{BACKWARD}(b_{k+2:t}, e_{k+1:t})$</p>
<ul>
<li><p>The backward phase has to be initialised with</p>
<p>$b_{t+1:t} = \Pr(e_{t+1:t} \mid X_t) = 1$ (a vector of 1s)</p>
<p>because the probability of observing empty sequence is 1.</p></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h5 id="example-4">Example</h5>
<p><img src="2D.assets/1554638275988.png" alt="1554638275988" /></p>
<p><img src="2D.assets/1554713827657.png" alt="1554713827657" /></p>
<p><img src="2D.assets/1554713851549.png" alt="1554713851549" /></p>
<ul>
<li><strong>So our confidence that it rained on Day 1 increases when we see the umbrella on the second day as well as the first.</strong></li>
<li>A simple, improved version of this that stores results, runs in liner time (called <strong>forward-backward algorithm</strong>.)</li>
</ul>
<h4 id="most-likely-explanationsequence">Most Likely Explanation/Sequence</h4>
<ul>
<li><p>Suppose $[\top, \top, \bot, \top, \top]$ is the umbrella sequence for first five days, what is the most likely weather sequence that caused it?</p>
<ul>
<li><p>We could use <em>smoothing</em> procedure to find posterior distribution for weather at each step and then use the most likely weather at each step to construct a sequence.</p>
<ul>
<li>But NO! <strong>Smoothing considers distributions over individual time steps</strong>, whereas we must consider <em>joint</em> probabilities over <em>all</em> time steps.</li>
</ul></li>
<li><p>Actual algorithm is based on viewing each sequence as a path through a graph (where nodes are states at each time step.)</p>
<ul>
<li><p>In umbrella example:</p>
<p><img src="2D.assets/1554714271697.png" alt="1554714271697" /></p>
<ul>
<li>Progress is shown in part (b) of the diagram.</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>The most important realisation is that <strong>most likely path to state $X_k = x_k $ consists of most likely path to state $X_{k-1} = x_{k -1}$ followed by transition from $X_{k-1} = x_{k-1}$ to $X_k = x_k$</strong>.</p></li>
<li><p>I refuse to copy the actual formula here, because it's unnecessarily complicated.</p>
<ul>
<li>From "2A - Processing Formal and Natural Languages", remember <strong>Viterbi Algorithm</strong>.
<ul>
<li>For each time step, product $\prod$ of :
<ul>
<li>the probability of transitioning to that state $\times$ the probability of emission at that time given the state
<ul>
<li>Bear in mind that emissions are fixed, thus we are trying to maximise the probability over different values of states!</li>
</ul></li>
</ul></li>
</ul></li>
<li>The algorithm has to keep pointers from each state back the best state that leads to it (<em>i.e.</em> for <strong>backtracking</strong>.)</li>
</ul></li>
</ul>
<h3 id="hidden-markov-models-hmm">Hidden Markov Models (HMM)</h3>
<ul>
<li>So far, we have seen a general model for temporal probabilistic reasoning (independent of transition/sensor models).</li>
<li>We are going to look at more concrete models and applications.</li>
<li><strong>Hidden Markov Models</strong> are temporal probabilistic models in which state of the process is described by <strong>a single variable.</strong>
<ul>
<li>Like $\text{Rain}$ in our umbrella example.</li>
<li>More than one variable can be accommodated, but only by combining them into a single "mega-variable".</li>
<li>Structure of HMMs allows for a very simple and elegant matrix implementation of basic algorithms.</li>
</ul></li>
</ul>
<h2 id="26-march-2019">26 March 2019</h2>
<p><strong>Personal Notice</strong></p>
<p>I think this section is quite sloppy, at least the concepts themselves. It seems to rely a lot on <em>gut feelings</em> of their inventors than actual theories so don't sweat over it too much. Read it casually and try having a conceptual understanding instead.</p>
<h3 id="dynamic-bayesian-networks-dbn">Dynamic Bayesian Networks (DBN)</h3>
<ul>
<li>A <strong>Dynamic Bayesian Network</strong> is a Bayesian Network describing temporal probability model that can have <strong>any number of</strong> (finite) <em>state variables</em> $X_t$ and <em>evidence variables</em> $E_t$.
<ul>
<li><strong>HMMs are DBNs with a single state and a single evidence variable.</strong>
<ul>
<li><strong>READ IT AGAIN.</strong></li>
</ul></li>
<li>But recall that one can "combine" a set of discrete (evidence or state) variables into a single variable (whose values are tuples).</li>
<li>So every discrete-variable DBN can be described as a HMM.</li>
<li>So why bother with DBNs?</li>
<li>Because <strong>decomposing a complex system into constituent variables, as a DBN does, <em>ameliorates</em> sparseness in the temporal probability model</strong>.
<ul>
<li><em>ameliorate:</em> "make (something bad or unsatisfactory) better"</li>
<li>How so? I don't know either. [TODO]</li>
</ul></li>
</ul></li>
</ul>
<h4 id="constructing-dynamic-bayesian-networks">Constructing Dynamic Bayesian Networks</h4>
<ul>
<li><p>We have to specify distribution of <em>prior state variables</em> $\Pr(X_0 )$, transition model $\Pr(X_{t+1} \mid X_t)$ and sensor model $\Pr(E_t \mid X_t)$.</p></li>
<li><p>Also, we have to fix the topology of the graph.</p></li>
<li><p>Due to <em>stationary assumption</em> (<em>i.e.</em> laws of the nature don't change over time), <strong>it's most convenient to specify the topology for the first (time) slice</strong>.</p>
<ul>
<li><p>For umbrella world example:</p>
<p><img src="2D.assets/1554719273031.png" alt="1554719273031" /></p>
<ul>
<li>This is actually a bad example --I think-- as it does not feature more than one state (random) variable, thus making it unclear that it's a DBN.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="modelling-failure">Modelling Failure</h4>
<ul>
<li>"An error model is just a particular kind of emission model: one where the observation or reading on one device is ostensibly telling you about the value of something, but could get it wrong."</li>
<li>One way to model error is through <strong>Gaussian Error Model</strong>, <em>i.e.</em> a small Gaussian error is added to the meter reading.
<ul>
<li>We can approximate this also for the discrete case through an appropriate distribution.</li>
</ul></li>
<li>But the problem is usually much worse: <em>sensor failure</em> rather than <em>inaccurate measurements</em>.</li>
</ul>
<h5 id="transient-failure">Transient Failure</h5>
<ul>
<li><p>Transient failure is when a sensor <em>occasionally</em> sends inaccurate data.</p></li>
<li><p><em>E.g.</em> After 20 consecutive readings of 100%, battery level suddenly drops to 0%.</p></li>
<li><p>In Gaussian error model, belief about current battery level $\text{Battery}_t$ depeds on:</p>
<ul>
<li>Sensor model: $\Pr(\text{BatteryMeter}_t = 0 \mid \text{Battery}_t)$, and</li>
<li>Prediction model: $$\Pr(\text{Battery}_t \mid \text{BatteryMeter}_{1:t})$$</li>
</ul></li>
<li><p>If probability of sensor error is <strong>smaller</strong> than sudden transition to 0, then the battery will be considered empty with a higher probability.</p></li>
<li><p>After a transient failure is gone, the probability will be corrected accordingly.</p>
<ul>
<li><p><strong>But in the meanwhile, the robot has made completely wrong judgement...</strong></p>
<p><img src="2D.assets/1554720281882.png" alt="1554720281882" /></p>
<ul>
<li>See how the robot thinks after reading two consecutive 0% that the battery is empty (whereas it's actually not.)
<ul>
<li>This can be disastrous if the robot decides to halt for instance (imagine a rover on Mars...)</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Thus to handle transient failures properly, <strong>sensor model</strong> must include possibility of failure.</p>
<ul>
<li><p>Simplest failure model: assign small probability to incorrect values,</p>
<p><em>e.g.</em> $\Pr(\text{BatteryMeter}_t = 0 \mid \text{Battery}_t = 5) = 0.03$</p></li>
<li><p>When faced with 0% reading, provided that predicted probability of empty battery is much less than 0.03, best explanation is failure.</p>
<ul>
<li><p>Thus you consider both options:</p>
<ol>
<li><p>$\Pr(\text{BatteryMeter}_t = 0 \mid \text{Battery}_t = 5) \Pr(\text{Battery}_t = 5 \mid \text{Battery}_{t-1} = 5)​$</p></li>
<li><p>$\Pr(\text{BatteryMeter}_t = 0 \mid \text{Battery}_t = 0) \Pr(\text{Battery}_t = 0 \mid \text{Battery}_{t-1} = 5)​$</p></li>
</ol>
<ul>
<li>and if (1) is greater, then conclude that it's a transient failure, else (if (2) is greater) then conclude that the battery is actually empty.
<ul>
<li>"You have to look at the chances of the battery going from 5 to 0 in one time step, and compare that with it going from 5 to 5 in one time step."</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>This model is much less susceptible to failure, because now an explanation can be given!</p></li>
<li><p>However, as it currently is, it cannot cope with persistent failure either.</p></li>
</ul></li>
</ul>
<h4 id="persistent-failure">Persistent Failure</h4>
<p><img src="2D.assets/1554721445915.png" alt="1554721445915" /></p>
<ul>
<li>In case of a permanent failure, the robot will (wrongly) believe that the battery is empty.</li>
<li><strong>Persistent Failure Models</strong> describe how sensor behaves under normal conditions and <em>after failure</em>.</li>
<li>A simple way is to add an additional variable $\text{XBroken}$ for each meter/sensor, and CPT to next $\text{XBroken}$ state has a very small probability if not broken, but $1.0$ if broken before (called <strong>Persistence Arc</strong>).
<ul>
<li>When $\text{XBroken}$ is true, $X$ will be considered faulty (and ignored).</li>
</ul></li>
</ul>
<p><img src="2D.assets/1554721881906.png" alt="1554721881906" /></p>
<ul>
<li>In case of a temporary blip, the probability of a broken sensor rises quickly but goes back to zero if expected values are observed.</li>
<li>In case of persistent failure, robot assumes discharge of battery at "normal" rate.</li>
</ul>
<h4 id="exact-inference-in-dbns">Exact Inference in DBNs</h4>
<ul>
<li><p>Since DBNs are BNs, we already have inference algorithms like variable eliminiation.</p></li>
<li><p>Essentially, DBN is equivalent to infinite "<strong>unfolded</strong>" BN, but slices beyond required inference period are irrelevant.</p>
<ul>
<li><p><strong>Unrolling</strong>/<strong>Unfolding</strong></p>
<p>Reproducing basic time slice to accommodate observation sequence.</p>
<p><img src="2D.assets/1554722395796.png" alt="1554722395796" /></p></li>
</ul></li>
<li><p>Exact inference in DBNs is intractable, and this is a major problem.</p>
<ul>
<li>But there are approximate inference methods that work well in practice.</li>
<li>This issue is currently a hot topic in AI... ;)</li>
</ul></li>
</ul>
<h3 id="bns-hmms-and-dbns">BNs, HMMs, and DBNs</h3>
<p><img src="2D.assets/1554723143576.png" alt="1554723143576" /></p>
<ul>
<li>Every Bayesian Network is also a Dynamic Bayesian Network.
<ul>
<li>Every Dynamic Bayesian Network can be unfolded/unrolled (for a finite amount of time-steps) into a Bayesian Network.</li>
</ul></li>
<li>Every Hidden Markov Model is a Dynamic Bayesian Network with a single state and evidence variable.
<ul>
<li>Thus Hidden Markov Models too can be unfolded/unrolled into a Bayesian Network.</li>
</ul></li>
</ul>
<h2 id="28-march-2019">28 March 2019</h2>
<h3 id="beliefs-and-desires">Beliefs and Desires</h3>
<blockquote>
<p>"So I say, walk by the Spirit, and you will not gratify the desires of the flesh but of robots."</p>
<p>--- Galatians 5:16</p>
</blockquote>
<ul>
<li>I am getting bored at this point sorry.</li>
<li><em>*Ehm*</em> Rational agents do things that are an <em>optimal</em> trade-off between:
<ul>
<li>The likelihood of reaching a particular state (given one's actions)</li>
<li>The desirability of that state</li>
</ul></li>
<li>So far we have done the <em>likelihood</em> bit: we know how to evaluate the probability of being in a particular state at a particular time.
<ul>
<li>But we've not looked at an agent's preferences or desires.
<ul>
<li>Do androids dream of electric sheep?</li>
</ul></li>
</ul></li>
<li>Now we will discuss <strong>utility theory</strong> in more detail to obtain a full picture of <strong>decision-theoretic agent design</strong>.</li>
</ul>
<h3 id="utility-theory--utility-functions">Utility Theory &amp; Utility Functions</h3>
<ul>
<li><p>Agent's preferences between world states are described using a <strong>Utility Function</strong>.</p></li>
<li><p>UF assigns some numerical value $\text{U}(S)$ to each state $S$ to express its desirability for the agent.</p></li>
<li><p>Non-deterministic action $a$ has results $\text{Result}(a)$ and probabilities $\Pr(\text{Result}(a)=s' \mid a, e)$ summarise agent's knowledge about its effects given evidence observations $e$.</p></li>
<li><p>Can be combined with probabilities for outcomes to obtain <strong>Expected Utility</strong> of action.</p>
<p><img src="2D.assets/1554731485464.png" alt="1554731485464" /></p></li>
<li><p>Principle of <strong>Maximum Expected Utility (MEU)</strong> says agent should use action that maximises expected utility.</p></li>
<li><p>In a sense, this summarises the whole endeavour of AI:</p>
<blockquote>
<p>"If agent maximises utility function that correctly reflects the performance measure applied to it, then optimal performance will be achieved by averaging over all environments in which agent could be placed."</p>
</blockquote>
<ul>
<li>Of course, this doesn't tell us how to define utility function or how to determine probabilities for any sequence of actions in a complex environment.</li>
</ul></li>
<li><p>For now we will only look at <strong>one-shot decisions</strong>, not <strong>sequential decisions</strong> (tomorrow).</p></li>
</ul>
<h3 id="constraints-on-rational-preferences">Constraints on Rational Preferences</h3>
<ul>
<li><p>Questions can be answered by looking at constraints on preferences.</p></li>
<li><p><strong>Notation</strong></p>
<ul>
<li><p>$A \prec B$</p>
<p>$A$ is preferred to $B$</p></li>
<li><p>$A \sim B​$</p>
<p>The agent is indifferent between $A$ and $B$</p></li>
<li><p>$A \preceq B​$</p>
<p>The agent prefers $A$ to $B$ but also is indifferent between them.</p></li>
</ul></li>
</ul>
<h4 id="lotteries">Lotteries</h4>
<ul>
<li><p>What are $A$ and $B$?</p>
<ul>
<li><p>Introduce <strong>lotteries</strong> with outcomes $C_1, \ldots C_n$ and accompanying probabilities.</p>
<p>$L = [p_1, C_1;\ p_2, C_2;\ \ldots;\ p_n, C_n]$</p></li>
</ul></li>
<li><p><strong>Outcome</strong> of a lottery can be a state or another lottery.</p></li>
<li><p>Is used to understand how preferences between complex lotteries are defined in terms of preferences among their outcome states.</p>
<ul>
<li>Such a complicated sentence for such a simple concept...</li>
</ul></li>
</ul>
<h4 id="axioms-on-preference">Axioms on Preference</h4>
<ul>
<li><strong>Orderability</strong>
<ul>
<li>Either $A \prec B$</li>
<li>or $B \prec A$</li>
<li>or $A \sim B$</li>
</ul></li>
<li><strong>Transitivity</strong>
<ul>
<li>$(A \prec B) \land (B \prec C) \implies (A \prec C)$</li>
</ul></li>
<li><strong>Continuity</strong>
<ul>
<li>If $B$ is between $A$ and $C$ in preference, then with <strong>some</strong> probability agent will be indifferent between getting $B$ for sure and a lottery over $A$ and $C$</li>
<li>$A \prec B \prec C \implies \exists p.\ [p, A;\ 1-p, C] \sim B​$</li>
<li>This is counter-intuitive at first, but realise that it says "for some probability" and not for all! It is then easy to see that the axiom is reasonable.</li>
</ul></li>
<li><strong>Substitutability</strong>
<ul>
<li>Indifference between lotteries leads to indifference between complex lotteries built from them.</li>
<li>$A \sim B \implies [p, A;\ 1-p, C] \sim [p, B;\ 1-p, C]$</li>
</ul></li>
<li><strong>Monotonicity</strong>
<ul>
<li>Preferring $A$ to $B$ implies preference for any lottery that assigns higher probability to A.</li>
<li>$A \prec B \implies (p \ge q \iff [p, A;\ 1-p, B] \preceq [q, A;\ 1-q, B])$</li>
</ul></li>
<li><strong>Decomposability</strong>
<ul>
<li>Compound lotteries can be reduced to simpler ones.</li>
<li><img src="2D.assets/1554733205938.png" alt="1554733205938" /></li>
<li><img src="2D.assets/1554733224270.png" alt="1554733224270" /></li>
</ul></li>
</ul>
<h4 id="axioms-of-utility">Axioms of Utility</h4>
<ul>
<li><p>The following <strong>axioms of utility</strong> ensure that <em>utility functions</em> follow the above axioms on preference.</p>
<ul>
<li><p><strong>Utility Principle</strong></p>
<p>There exists a function such that</p>
<p>$\text{U}(A) &gt; \text{U}(B) \iff A \prec B \qquad \text{U}(A) = \text{U}(B) \iff A \sim B$</p></li>
<li><p><strong>Maximum Expected Utility (MEU) Principle</strong></p>
<p>Utility of a lottery is the sum of probability of outcomes times their utilities.</p>
<p>$\text{U}([p_1, S_1;\ \ldots; p_n, S_n]) = \sum_i p_i\text{U}(S_i)$</p></li>
</ul></li>
<li><p>But an agent might now know even his own utilities!</p>
<ul>
<li>We can observe someone's (or even our own) utilities by observing his (or our own) behaviour (and by assuming that the agent chooses to MEU).</li>
</ul></li>
<li><p>According to the above axioms, arbitrary preferences can be expressed by utility functions.</p></li>
<li><p>Usually preferences are more systematic than "random", a typical example being money (roughly, we like to maximise our money.)</p>
<ul>
<li>Agents exhibit <strong>monotonic preference</strong> toward money, but how about lotteries involving money?
<ul>
<li>"Who wants to be a millionaire"-type problem: <strong>is pocketing a smaller amount irrational?</strong></li>
<li><strong>Expected Monetary Value (EMV)</strong> is actual expectation of outcome.</li>
<li>Calculations depend on utility values assigned to levels of monetary wealth (is first million more valuable than second?)
<ul>
<li><img src="2D.assets/1554734792215.png" alt="1554734792215" /></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="utility-scales">Utility Scales</h3>
<ul>
<li>Axioms don't say anything about scales.</li>
<li>For example, transformation of $\text{U}(S)$ into $\text{U}'(S) = k_1 + k_2\text{U}(S)$ ($k_2$ positive) doesn't effect behaviour.</li>
<li><strong>In deterministic contexts, behaviour is unchanged by any monotonic transformation</strong> (utility function is <strong>value/ordinal function</strong>)</li>
<li>One procedure for assessing utilities is to use <strong>Normalised Utility</strong> between "best possible prize" $(u^\top = 1)$ and "worst possible catastrophe" $(u^\bot = 0)$.</li>
<li>Do
<ul>
<li>Ask agent to indicate preference between a given $S$ and the <strong>standard lottery</strong> $[p, u^\top;\ (1-p), u^\bot]$</li>
<li>Adjust $p$ until agent is indifferent between $S$ and standard lottery.
<ul>
<li>See <strong>Continuity Axiom</strong>.</li>
</ul></li>
<li>Set $\text{U}(S) = p​$.
<ul>
<li>Or we see that $\text{U}(S) = p$.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="decision-networks-dn-influence-diagrams">Decision Networks (DN) (Influence Diagrams)</h3>
<ul>
<li><p>We need a way of integrating utilities into our view of probabilistic reasoning.</p></li>
<li><p><strong>Decision Networks (DN) (Influence Diagrams)</strong> combine BNs with additional node types for actions and utilities.</p></li>
<li><p>"Airport Sitting" Example:</p>
<p><img src="2D.assets/1554736655525.png" alt="1554736655525" /></p>
<ul>
<li><p><strong>Chance Nodes</strong> (ovals)</p>
<p>represent random variables with CPTs, parents can be decision nodes.</p></li>
<li><p><strong>Decision Nodes</strong> (rectangles)</p>
<p>represent decision-making points at which actions are available.</p></li>
<li><p><strong>Utility Nodes</strong> (rhombus)</p>
<p>represent utility function; connected to all nodes that affect utility directly.</p></li>
<li><p>Often nodes describing outcome states (<em>i.e.</em> states that are used to describe outcome of actions) are omitted and expected utility associated with actions is expressed (rather than represented by states) -- <strong>action-utility tables</strong></p></li>
</ul></li>
</ul>
<h4 id="action-utility-tables--graphs">Action-Utility Tables &amp; Graphs</h4>
<ul>
<li>Less flexible but simpler.</li>
<li><img src="2D.assets/1554736989578.png" alt="1554736989578" /></li>
<li>Why "Deaths", "Noise", and "Cost" is omitted?
<ul>
<li>Because those are affected by <em>root chance nodes</em> "Air Traffic", "Litigation", and "Construction", and the <em>decision node</em> "Airport Site".</li>
<li>Thus it can be said that by expressing "Deaths", "Noise", and "Cost" in terms of other nodes, we can simplify our graph.</li>
<li>It's quite Spartan, I know.</li>
</ul></li>
</ul>
<h4 id="evaluating-decision-networks">Evaluating Decision Networks</h4>
<ul>
<li>Evaluation of a DN, given the values of all <em>root chance nodes</em>, works by setting decision node to every possible value.
<ul>
<li>Return action with highest expected utility.</li>
<li>It's literally brute-forcing...</li>
</ul></li>
<li>Using any algorithm for BN inference, this yields a simple framework for building agents that make <em>single-shot</em> decisions.</li>
</ul>
<h2 id="29-march-2019">29 March 2019</h2>
<h3 id="sequential-decision-problems">Sequential Decision Problems</h3>
<ul>
<li>We have previously looked at one-shot decisions, but decisions are often sequential.</li>
<li>Actions are somewhat unreliable too.</li>
</ul>
<h3 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h3>
<ul>
<li>To describe such worlds, we can use a <strong>Transition Model</strong> $\text{T}(s, a, s')$ denoting the probability that action $a$ in $s$ will lead to state $s'$.
<ul>
<li>The model is clearly Markovian: the probability of reaching $s'$ depends only on $s$ and not on history of earlier states.</li>
<li>Think of $\text{T}$ as a big three-dimensional table.</li>
</ul></li>
<li>Utility function now depends on <strong>Environment History</strong>.
<ul>
<li><del>What an awful, <em>awful</em> name...</del>
<ul>
<li><del>Sincerely, <em>action history</em> would be a far better choice.</del></li>
<li>No no, it's actually about the <em>states</em> you have been.</li>
</ul></li>
<li>As you can guess from my rant, this is about evaluating our sequence of <del>actions</del> states we have been whilst reaching our destination.
<ul>
<li>Because practically, you'd like to achieve your goals as soon as possible.</li>
</ul></li>
<li>Therefore, for every move, agent receives a <strong>State Reward</strong> $\text{R}(s)$ (for moving) in each state $s$.
<ul>
<li><em>E.g.</em> $-0.04$ apart from the goal state for instance.</li>
<li>Thus the <strong>Utility of Environment History</strong> is the sum of <em>state rewards</em>, <strong>for now</strong>.</li>
</ul></li>
<li>In a sense, this is a stochastic generalisation of search algorithms!</li>
</ul></li>
<li><strong>Definition</strong>
<ul>
<li>Initial State: $S_0$</li>
<li>Transition Model: $\text{T}(s, a, s')$</li>
<li>Utility Function: $\text{R}(s)$</li>
</ul></li>
<li><strong>Policy</strong> is a description of what agent does (should do) in every state, written as $\pi$.
<ul>
<li>$\pi(s)$ for individual state describes which action should be taken in $s$.</li>
<li><strong>Optimal Policy</strong> is one that yields the highest possible expected utility, denoted by $\pi^*$.</li>
</ul></li>
<li><strong>Example</strong>
<ul>
<li><img src="2D.assets/1554738666412.png" alt="1554738666412" /></li>
</ul></li>
</ul>
<h3 id="optimality-in-sequential-decision-problems">Optimality in Sequential Decision Problems</h3>
<ul>
<li><p>We have used <em>sum of rewards</em> as <em>utility of environment history</em>, but what are the other alternatives?</p>
<ul>
<li><p>The first questions we should ask ourselves is <strong>finite horizon</strong> or <strong>infinite horizon</strong>.</p></li>
<li><p><strong>Finite Horizon</strong> means there is a fixed time $N$ after which nothing matters:</p>
<p>$\forall k.\ \text{U}_h([s_0, s_1, \ldots, s_N+k]) = \text{U}_h(s_0, s_1, \ldots, s_N])$</p></li>
</ul></li>
<li><p><strong>Stationary Optimal Policies</strong> (w/ <em>infinite horizon</em>) are those where the time at state does <em>NOT</em> matter.</p>
<ul>
<li>whereas in <strong>Non-Stationary Optimal Policies</strong> (w/ <em>finite horizon</em>) time at state <em>does</em> matter.</li>
</ul></li>
<li><p><strong>Stationarity Assumption</strong></p>
<p><img src="2D.assets/1554739428134.png" alt="1554739428134" /></p></li>
<li><p>Stationarity may look harmless, but allows only two ways to assign utilities to sequences under stationarity assumptions:</p>
<ul>
<li><p><strong>Additive Rewards</strong></p>
<p><img src="2D.assets/1554739487758.png" alt="1554739487758" /></p></li>
<li><p><strong>Discounted Rewards</strong> (for <strong>discount factor</strong> $0 \le \gamma \le 1$)</p>
<p><img src="2D.assets/1554739522966.png" alt="1554739522966" /></p>
<ul>
<li>Discount Factor makes more distant future rewards less significant.</li>
<li>We will mostly use discounted rewards in what follows.</li>
</ul></li>
</ul></li>
<li><p><strong>Choosing <em>infinite horizon</em> creates a loop-hole.</strong></p>
<ul>
<li><p>Some sequences will be infinite with infinite (additive) reward, how do we compare them?</p>
<ul>
<li>Realise that things get really hairy if you use positive rewards; ask yourself what keeps your agent agent from looping around non-terminal states to gather rewards.</li>
</ul></li>
<li><p><strong>Solution 1:</strong> with discounted rewards the utility is bounded</p>
<p><img src="2D.assets/1554740327532.png" alt="1554740327532" /></p>
<ul>
<li>For instance, we can stop summing after a certain cut-off point and take the maximum possible value as laid out above?</li>
</ul></li>
<li><p><strong>Solution 2:</strong> under <strong>proper policies</strong> (?) (<em>i.e.</em> if agent will eventually visit a terminal state) additive rewards are finite</p></li>
<li><p><strong>Solution 3:</strong> compare <strong>average reward</strong> per time step.</p></li>
</ul></li>
</ul>
<h3 id="value-iteration">Value Iteration</h3>
<ul>
<li><p><strong>Value Iteration</strong> is an algorithm for calculating optimal policy in Markov Decision Processes:</p>
<blockquote>
<p>"Calculate the utility of each state and then select optimal action based on these utilities."</p>
</blockquote></li>
<li><p>Since <em>discounted rewards</em> seemed to create no problems, we will use</p>
<p>$$\begin{align*} \pi^* &amp;= \arg\max_\pi R_0 + \sum^\infty_{t=1} \sum_{s_t} \text{T}(s_{t-1},\pi(s_{t-1}),s_t) \left(\gamma^t \text{R}(s_t)\right) \end{align*}$$</p>
<p>as a criterion for optimal policy.</p>
<ul>
<li>Remember that $\pi​$ is a function, not a single action!</li>
<li>"In other words, at each time step $t​$, you marginalise over the possible outcome states $s_t​$, so that you get the weighted average of the reward you will get at time step $t​$, and you sum over all of those rewards from $t=0​$ to infinity."</li>
</ul></li>
</ul>
<p><strong>Explanation</strong></p>
<ul>
<li><p>Each policy $\pi​$ yields a tree, with root node $s_0​$, and daughters to a node $s​$ are the <strong>possible successor states</strong> given the action $\pi(s)​$. [Remember that we work in non-deterministic environments now.]</p>
<ul>
<li><p><img src="2D.assets/1554743195700.png" alt="1554743195700" /></p>
<ul>
<li>The subscripts stand to distinguish possible successor states.</li>
</ul></li>
<li><p>$\text{T}(s, a, s')$ gives the probability of traversing an arc from $s$ to daughter $s'$.</p></li>
</ul></li>
</ul>
<h4 id="utilities-of-states">Utilities of States</h4>
<ul>
<li><p>$\text{R}(s)$ is the reward for being in $s$ <strong>now</strong>.</p></li>
<li><p>By making $\text{U}(s)$ the utility of the states that might follow it, $\text{U}(s)$ captures <em>long-term</em> advantages of starting from $s$.</p>
<ul>
<li>$\text{U}(s)$ reflects what you can do <strong>from</strong> $s$.</li>
<li>$\text{R}(s)$ does not.</li>
</ul></li>
<li><p>States that follow depend on $\pi$. So utility of $s$ given $\pi$ is:</p>
<p>$$\begin{align*}\text{U}^\pi(s_k) = \sum^\infty_{t=k} \sum_{s_t} T(s_{t-1},\pi(s_{t-1}),s_t) \left(\gamma^t R(s_t)\right) \end{align*}$$</p>
<ul>
<li>"In other words, at each time step $t$, you marginalise over the possible outcome states $s_t$, so that you get the weighted average of the reward you will get at time step $t$, and you sum over all of those rewards from $t=0$ to infinity."</li>
</ul></li>
<li><p>With this definition, <em>true</em> utility $\text{U}(s)​$ is $\text{U}^{\pi^*}​$.</p></li>
<li><p>Given $\text{U}(s)​$, we can easily determine optimal policy:</p>
<p>$$\begin{align*}\pi^*(s) = \arg\max_a \sum_{s'} \text{T}(s, a, s') \text{U}(s')\end{align*}​$$</p></li>
<li><p><strong>"Utility of a state is immediate reward plus expected utility of subsequent states if agent chooses optimal actions."</strong></p>
<ul>
<li><p>N.B. Direct relationship between utility of a state and that of its neighbours.</p></li>
<li><p>This can be written as famous <strong>Bellman Equations</strong>:</p>
<p>$$\begin{align*}\text{U}(s) = \text{R}(s) + \gamma \arg\max_a \sum_{s'} \text{T}(s, a, s') \text{U}(s')\end{align*}​$$</p></li>
<li><p><strong>For $n$ states we have $n$ Bellman equations with $n$ unknowns</strong> (utilities of states).</p></li>
<li><p><strong>Value iteration is an iterative approach to solving the $n$ equations.</strong></p></li>
<li><p>Start with arbitrary values and update them as follows:</p>
<p><img src="2D.assets/1554748248823.png" alt="1554748248823" /></p></li>
<li><p><strong>The algorithm converges to right and unique solution.</strong></p>
<ul>
<li><p>Example:</p>
<p><img src="2D.assets/1554748305698.png" alt="1554748305698" /></p></li>
</ul></li>
</ul></li>
</ul>
<h3 id="decision-theoretic-agents">Decision-Theoretic Agents</h3>
<ul>
<li><p>We now have (tediously) gathered all the ingredients to build decision-theoretic agents.</p>
<ul>
<li><p>Transition and observation models will be described by a <em>Dynamic Bayesian Network</em>.</p></li>
<li><p>They will be augmented by decision and utility nodes to obtain a <em>Dynamic Decision Network</em>.</p>
<ul>
<li><p>Looks something like this:</p>
<p><img src="2D.assets/1554748523882.png" alt="1554748523882" /></p></li>
</ul></li>
<li><p>Decisions will be made by projecting forward possible action sequences and choosing the best one.</p></li>
<li><p>Practical design for a <strong>utility-based agent</strong>.</p></li>
</ul></li>
</ul>
<hr />
<p><img src="2D.assets/Thats_All_Folks.jpg" alt="https://allthingsd.com/files/2013/03/Thats_All_Folks.jpg" /></p>
</body>
</html>
